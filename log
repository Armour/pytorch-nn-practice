==> Init variables..
==> Init seed..
==> Download data..
Files already downloaded and verified
==> Calculate mean and std..
==> Prepare training transform..
==> Prepare testing transform..
==> Init dataloader..
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
==> Set learning rate: 0.010000
==> Training Epoch: 1
0.000000/1000.000000 ==> Training loss: 5.099441    Training error rate: 100.000000
50.000000/1000.000000 ==> Training loss: 5.162900    Training error rate: 100.000000
100.000000/1000.000000 ==> Training loss: 4.416446    Training error rate: 92.000000
150.000000/1000.000000 ==> Training loss: 4.146763    Training error rate: 92.000000
200.000000/1000.000000 ==> Training loss: 4.196798    Training error rate: 94.000000
250.000000/1000.000000 ==> Training loss: 4.275916    Training error rate: 98.000000
300.000000/1000.000000 ==> Training loss: 4.085358    Training error rate: 96.000000
350.000000/1000.000000 ==> Training loss: 3.955751    Training error rate: 92.000000
400.000000/1000.000000 ==> Training loss: 4.038115    Training error rate: 90.000000
450.000000/1000.000000 ==> Training loss: 3.933294    Training error rate: 90.000000
500.000000/1000.000000 ==> Training loss: 3.824020    Training error rate: 84.000000
550.000000/1000.000000 ==> Training loss: 4.291460    Training error rate: 98.000000
600.000000/1000.000000 ==> Training loss: 3.678003    Training error rate: 86.000000
650.000000/1000.000000 ==> Training loss: 3.824186    Training error rate: 90.000000
700.000000/1000.000000 ==> Training loss: 4.028839    Training error rate: 94.000000
750.000000/1000.000000 ==> Training loss: 3.705155    Training error rate: 82.000000
800.000000/1000.000000 ==> Training loss: 3.787447    Training error rate: 88.000000
850.000000/1000.000000 ==> Training loss: 3.356160    Training error rate: 88.000000
900.000000/1000.000000 ==> Training loss: 3.684165    Training error rate: 86.000000
950.000000/1000.000000 ==> Training loss: 3.438225    Training error rate: 82.000000
==> Total training loss: 4026.317283    Total training error rate: 92.072000
==> Testing Epoch: 1
0.000000/100.000000 ==> Testing loss: 3.758418    Testing error rate: 86.000000
50.000000/100.000000 ==> Testing loss: 3.650816    Testing error rate: 81.000000
==> Total testing loss: 365.586010    Total testing error rate: 86.580000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 2
0.000000/1000.000000 ==> Training loss: 3.549226    Training error rate: 86.000000
50.000000/1000.000000 ==> Training loss: 3.510678    Training error rate: 88.000000
100.000000/1000.000000 ==> Training loss: 3.433121    Training error rate: 84.000000
150.000000/1000.000000 ==> Training loss: 3.509465    Training error rate: 90.000000
200.000000/1000.000000 ==> Training loss: 3.349941    Training error rate: 82.000000
250.000000/1000.000000 ==> Training loss: 3.458431    Training error rate: 92.000000
300.000000/1000.000000 ==> Training loss: 3.431476    Training error rate: 82.000000
350.000000/1000.000000 ==> Training loss: 3.303687    Training error rate: 78.000000
400.000000/1000.000000 ==> Training loss: 3.742269    Training error rate: 94.000000
450.000000/1000.000000 ==> Training loss: 3.477706    Training error rate: 78.000000
500.000000/1000.000000 ==> Training loss: 3.338876    Training error rate: 86.000000
550.000000/1000.000000 ==> Training loss: 3.078589    Training error rate: 72.000000
600.000000/1000.000000 ==> Training loss: 3.103625    Training error rate: 82.000000
650.000000/1000.000000 ==> Training loss: 3.437057    Training error rate: 82.000000
700.000000/1000.000000 ==> Training loss: 3.287236    Training error rate: 76.000000
750.000000/1000.000000 ==> Training loss: 2.865732    Training error rate: 78.000000
800.000000/1000.000000 ==> Training loss: 3.272567    Training error rate: 82.000000
850.000000/1000.000000 ==> Training loss: 3.279181    Training error rate: 80.000000
900.000000/1000.000000 ==> Training loss: 2.880152    Training error rate: 74.000000
950.000000/1000.000000 ==> Training loss: 3.383698    Training error rate: 72.000000
==> Total training loss: 3359.914930    Total training error rate: 81.912000
==> Testing Epoch: 2
0.000000/100.000000 ==> Testing loss: 3.374336    Testing error rate: 80.000000
50.000000/100.000000 ==> Testing loss: 3.117177    Testing error rate: 74.000000
==> Total testing loss: 315.195562    Total testing error rate: 77.330000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 3
0.000000/1000.000000 ==> Training loss: 3.145865    Training error rate: 74.000000
50.000000/1000.000000 ==> Training loss: 2.870741    Training error rate: 62.000000
100.000000/1000.000000 ==> Training loss: 3.297317    Training error rate: 82.000000
150.000000/1000.000000 ==> Training loss: 3.799235    Training error rate: 84.000000
200.000000/1000.000000 ==> Training loss: 2.678745    Training error rate: 82.000000
250.000000/1000.000000 ==> Training loss: 2.843819    Training error rate: 74.000000
300.000000/1000.000000 ==> Training loss: 3.275985    Training error rate: 78.000000
350.000000/1000.000000 ==> Training loss: 2.756059    Training error rate: 66.000000
400.000000/1000.000000 ==> Training loss: 2.847559    Training error rate: 78.000000
450.000000/1000.000000 ==> Training loss: 2.795875    Training error rate: 72.000000
500.000000/1000.000000 ==> Training loss: 2.741857    Training error rate: 68.000000
550.000000/1000.000000 ==> Training loss: 2.882214    Training error rate: 70.000000
600.000000/1000.000000 ==> Training loss: 2.903807    Training error rate: 74.000000
650.000000/1000.000000 ==> Training loss: 2.969321    Training error rate: 72.000000
700.000000/1000.000000 ==> Training loss: 2.800308    Training error rate: 70.000000
750.000000/1000.000000 ==> Training loss: 2.433994    Training error rate: 68.000000
800.000000/1000.000000 ==> Training loss: 2.547932    Training error rate: 66.000000
850.000000/1000.000000 ==> Training loss: 2.979413    Training error rate: 68.000000
900.000000/1000.000000 ==> Training loss: 2.698562    Training error rate: 64.000000
950.000000/1000.000000 ==> Training loss: 2.946457    Training error rate: 70.000000
==> Total training loss: 2873.056162    Total training error rate: 73.292000
==> Testing Epoch: 3
0.000000/100.000000 ==> Testing loss: 2.909447    Testing error rate: 70.000000
50.000000/100.000000 ==> Testing loss: 2.580161    Testing error rate: 69.000000
==> Total testing loss: 278.947637    Total testing error rate: 70.880000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 4
0.000000/1000.000000 ==> Training loss: 2.840415    Training error rate: 72.000000
50.000000/1000.000000 ==> Training loss: 2.582368    Training error rate: 70.000000
100.000000/1000.000000 ==> Training loss: 2.590998    Training error rate: 66.000000
150.000000/1000.000000 ==> Training loss: 2.331321    Training error rate: 66.000000
200.000000/1000.000000 ==> Training loss: 2.334332    Training error rate: 64.000000
250.000000/1000.000000 ==> Training loss: 2.528769    Training error rate: 68.000000
300.000000/1000.000000 ==> Training loss: 2.625975    Training error rate: 68.000000
350.000000/1000.000000 ==> Training loss: 2.551847    Training error rate: 64.000000
400.000000/1000.000000 ==> Training loss: 2.384376    Training error rate: 62.000000
450.000000/1000.000000 ==> Training loss: 2.799548    Training error rate: 76.000000
500.000000/1000.000000 ==> Training loss: 2.656728    Training error rate: 84.000000
550.000000/1000.000000 ==> Training loss: 2.758385    Training error rate: 68.000000
600.000000/1000.000000 ==> Training loss: 2.019773    Training error rate: 50.000000
650.000000/1000.000000 ==> Training loss: 2.257355    Training error rate: 64.000000
700.000000/1000.000000 ==> Training loss: 2.132658    Training error rate: 52.000000
750.000000/1000.000000 ==> Training loss: 2.271585    Training error rate: 52.000000
800.000000/1000.000000 ==> Training loss: 2.052769    Training error rate: 60.000000
850.000000/1000.000000 ==> Training loss: 2.125876    Training error rate: 54.000000
900.000000/1000.000000 ==> Training loss: 2.733238    Training error rate: 72.000000
950.000000/1000.000000 ==> Training loss: 2.218821    Training error rate: 58.000000
==> Total training loss: 2457.501722    Total training error rate: 64.844000
==> Testing Epoch: 4
0.000000/100.000000 ==> Testing loss: 2.381531    Testing error rate: 62.000000
50.000000/100.000000 ==> Testing loss: 2.129648    Testing error rate: 52.000000
==> Total testing loss: 228.611561    Total testing error rate: 60.640000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 5
0.000000/1000.000000 ==> Training loss: 2.190807    Training error rate: 58.000000
50.000000/1000.000000 ==> Training loss: 1.896788    Training error rate: 56.000000
100.000000/1000.000000 ==> Training loss: 2.148231    Training error rate: 58.000000
150.000000/1000.000000 ==> Training loss: 2.085775    Training error rate: 62.000000
200.000000/1000.000000 ==> Training loss: 2.178115    Training error rate: 58.000000
250.000000/1000.000000 ==> Training loss: 2.192413    Training error rate: 60.000000
300.000000/1000.000000 ==> Training loss: 2.155362    Training error rate: 54.000000
350.000000/1000.000000 ==> Training loss: 2.115980    Training error rate: 56.000000
400.000000/1000.000000 ==> Training loss: 2.430694    Training error rate: 66.000000
450.000000/1000.000000 ==> Training loss: 2.306495    Training error rate: 64.000000
500.000000/1000.000000 ==> Training loss: 2.201328    Training error rate: 62.000000
550.000000/1000.000000 ==> Training loss: 1.843375    Training error rate: 52.000000
600.000000/1000.000000 ==> Training loss: 2.112009    Training error rate: 58.000000
650.000000/1000.000000 ==> Training loss: 1.933476    Training error rate: 60.000000
700.000000/1000.000000 ==> Training loss: 2.056706    Training error rate: 44.000000
750.000000/1000.000000 ==> Training loss: 1.873052    Training error rate: 58.000000
800.000000/1000.000000 ==> Training loss: 2.080417    Training error rate: 64.000000
850.000000/1000.000000 ==> Training loss: 2.575918    Training error rate: 72.000000
900.000000/1000.000000 ==> Training loss: 2.106843    Training error rate: 58.000000
950.000000/1000.000000 ==> Training loss: 1.903697    Training error rate: 56.000000
==> Total training loss: 2151.165540    Total training error rate: 58.320000
==> Testing Epoch: 5
0.000000/100.000000 ==> Testing loss: 2.217825    Testing error rate: 56.000000
50.000000/100.000000 ==> Testing loss: 2.022952    Testing error rate: 55.000000
==> Total testing loss: 209.899406    Total testing error rate: 55.800000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 6
0.000000/1000.000000 ==> Training loss: 2.059193    Training error rate: 54.000000
50.000000/1000.000000 ==> Training loss: 2.071478    Training error rate: 54.000000
100.000000/1000.000000 ==> Training loss: 2.033415    Training error rate: 60.000000
150.000000/1000.000000 ==> Training loss: 1.547388    Training error rate: 44.000000
200.000000/1000.000000 ==> Training loss: 1.880664    Training error rate: 46.000000
250.000000/1000.000000 ==> Training loss: 2.022408    Training error rate: 50.000000
300.000000/1000.000000 ==> Training loss: 2.078007    Training error rate: 56.000000
350.000000/1000.000000 ==> Training loss: 1.965082    Training error rate: 54.000000
400.000000/1000.000000 ==> Training loss: 2.158001    Training error rate: 58.000000
450.000000/1000.000000 ==> Training loss: 1.924524    Training error rate: 44.000000
500.000000/1000.000000 ==> Training loss: 1.738860    Training error rate: 52.000000
550.000000/1000.000000 ==> Training loss: 1.860035    Training error rate: 54.000000
600.000000/1000.000000 ==> Training loss: 2.002609    Training error rate: 54.000000
650.000000/1000.000000 ==> Training loss: 1.760525    Training error rate: 50.000000
700.000000/1000.000000 ==> Training loss: 1.627587    Training error rate: 56.000000
750.000000/1000.000000 ==> Training loss: 2.063627    Training error rate: 60.000000
800.000000/1000.000000 ==> Training loss: 1.956282    Training error rate: 60.000000
850.000000/1000.000000 ==> Training loss: 1.778941    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.968885    Training error rate: 62.000000
950.000000/1000.000000 ==> Training loss: 1.872229    Training error rate: 58.000000
==> Total training loss: 1933.528420    Total training error rate: 53.426000
==> Testing Epoch: 6
0.000000/100.000000 ==> Testing loss: 1.913968    Testing error rate: 47.000000
50.000000/100.000000 ==> Testing loss: 1.744333    Testing error rate: 45.000000
==> Total testing loss: 188.599868    Total testing error rate: 50.630000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 7
0.000000/1000.000000 ==> Training loss: 1.655617    Training error rate: 46.000000
50.000000/1000.000000 ==> Training loss: 2.058225    Training error rate: 58.000000
100.000000/1000.000000 ==> Training loss: 1.641984    Training error rate: 44.000000
150.000000/1000.000000 ==> Training loss: 1.519437    Training error rate: 48.000000
200.000000/1000.000000 ==> Training loss: 1.335865    Training error rate: 36.000000
250.000000/1000.000000 ==> Training loss: 1.371627    Training error rate: 44.000000
300.000000/1000.000000 ==> Training loss: 1.876528    Training error rate: 54.000000
350.000000/1000.000000 ==> Training loss: 1.786777    Training error rate: 46.000000
400.000000/1000.000000 ==> Training loss: 1.516261    Training error rate: 48.000000
450.000000/1000.000000 ==> Training loss: 1.786289    Training error rate: 50.000000
500.000000/1000.000000 ==> Training loss: 1.638345    Training error rate: 40.000000
550.000000/1000.000000 ==> Training loss: 1.813026    Training error rate: 48.000000
600.000000/1000.000000 ==> Training loss: 1.816995    Training error rate: 62.000000
650.000000/1000.000000 ==> Training loss: 1.644472    Training error rate: 54.000000
700.000000/1000.000000 ==> Training loss: 1.601784    Training error rate: 42.000000
750.000000/1000.000000 ==> Training loss: 1.614946    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.718328    Training error rate: 54.000000
850.000000/1000.000000 ==> Training loss: 1.503470    Training error rate: 42.000000
900.000000/1000.000000 ==> Training loss: 1.878428    Training error rate: 54.000000
950.000000/1000.000000 ==> Training loss: 1.960265    Training error rate: 50.000000
==> Total training loss: 1747.794708    Total training error rate: 48.896000
==> Testing Epoch: 7
0.000000/100.000000 ==> Testing loss: 1.839888    Testing error rate: 52.000000
50.000000/100.000000 ==> Testing loss: 1.753475    Testing error rate: 45.000000
==> Total testing loss: 182.689208    Total testing error rate: 49.470000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 8
0.000000/1000.000000 ==> Training loss: 2.101798    Training error rate: 50.000000
50.000000/1000.000000 ==> Training loss: 1.694197    Training error rate: 52.000000
100.000000/1000.000000 ==> Training loss: 1.381680    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 1.221705    Training error rate: 40.000000
200.000000/1000.000000 ==> Training loss: 1.771448    Training error rate: 56.000000
250.000000/1000.000000 ==> Training loss: 1.331762    Training error rate: 44.000000
300.000000/1000.000000 ==> Training loss: 1.735250    Training error rate: 44.000000
350.000000/1000.000000 ==> Training loss: 1.564412    Training error rate: 48.000000
400.000000/1000.000000 ==> Training loss: 1.583546    Training error rate: 48.000000
450.000000/1000.000000 ==> Training loss: 1.592473    Training error rate: 50.000000
500.000000/1000.000000 ==> Training loss: 1.602143    Training error rate: 38.000000
550.000000/1000.000000 ==> Training loss: 1.400313    Training error rate: 38.000000
600.000000/1000.000000 ==> Training loss: 1.742413    Training error rate: 48.000000
650.000000/1000.000000 ==> Training loss: 1.781681    Training error rate: 56.000000
700.000000/1000.000000 ==> Training loss: 1.934592    Training error rate: 56.000000
750.000000/1000.000000 ==> Training loss: 1.280964    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 2.093398    Training error rate: 48.000000
850.000000/1000.000000 ==> Training loss: 1.373587    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.557337    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.478017    Training error rate: 42.000000
==> Total training loss: 1607.199916    Total training error rate: 45.666000
==> Testing Epoch: 8
0.000000/100.000000 ==> Testing loss: 1.741315    Testing error rate: 43.000000
50.000000/100.000000 ==> Testing loss: 1.704606    Testing error rate: 47.000000
==> Total testing loss: 176.907435    Total testing error rate: 47.950000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 9
0.000000/1000.000000 ==> Training loss: 1.712361    Training error rate: 48.000000
50.000000/1000.000000 ==> Training loss: 1.524778    Training error rate: 38.000000
100.000000/1000.000000 ==> Training loss: 1.393590    Training error rate: 38.000000
150.000000/1000.000000 ==> Training loss: 1.836840    Training error rate: 44.000000
200.000000/1000.000000 ==> Training loss: 1.573191    Training error rate: 44.000000
250.000000/1000.000000 ==> Training loss: 1.445380    Training error rate: 40.000000
300.000000/1000.000000 ==> Training loss: 1.783751    Training error rate: 52.000000
350.000000/1000.000000 ==> Training loss: 1.687281    Training error rate: 52.000000
400.000000/1000.000000 ==> Training loss: 1.481537    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.447264    Training error rate: 38.000000
500.000000/1000.000000 ==> Training loss: 1.425301    Training error rate: 42.000000
550.000000/1000.000000 ==> Training loss: 1.473886    Training error rate: 44.000000
600.000000/1000.000000 ==> Training loss: 1.815346    Training error rate: 46.000000
650.000000/1000.000000 ==> Training loss: 1.521102    Training error rate: 38.000000
700.000000/1000.000000 ==> Training loss: 1.798823    Training error rate: 46.000000
750.000000/1000.000000 ==> Training loss: 1.337845    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.619673    Training error rate: 42.000000
850.000000/1000.000000 ==> Training loss: 1.812687    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.627762    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.633768    Training error rate: 54.000000
==> Total training loss: 1496.333617    Total training error rate: 42.558000
==> Testing Epoch: 9
0.000000/100.000000 ==> Testing loss: 1.805897    Testing error rate: 46.000000
50.000000/100.000000 ==> Testing loss: 1.769310    Testing error rate: 45.000000
==> Total testing loss: 181.217760    Total testing error rate: 48.140000
==> Set learning rate: 0.010000
==> Training Epoch: 10
0.000000/1000.000000 ==> Training loss: 1.308221    Training error rate: 42.000000
50.000000/1000.000000 ==> Training loss: 1.410322    Training error rate: 42.000000
100.000000/1000.000000 ==> Training loss: 1.523581    Training error rate: 40.000000
150.000000/1000.000000 ==> Training loss: 1.351217    Training error rate: 46.000000
200.000000/1000.000000 ==> Training loss: 1.427944    Training error rate: 42.000000
250.000000/1000.000000 ==> Training loss: 1.510982    Training error rate: 46.000000
300.000000/1000.000000 ==> Training loss: 1.980562    Training error rate: 48.000000
350.000000/1000.000000 ==> Training loss: 1.344460    Training error rate: 34.000000
400.000000/1000.000000 ==> Training loss: 1.590798    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.264358    Training error rate: 42.000000
500.000000/1000.000000 ==> Training loss: 1.392556    Training error rate: 38.000000
550.000000/1000.000000 ==> Training loss: 1.337679    Training error rate: 44.000000
600.000000/1000.000000 ==> Training loss: 1.452666    Training error rate: 44.000000
650.000000/1000.000000 ==> Training loss: 1.705950    Training error rate: 48.000000
700.000000/1000.000000 ==> Training loss: 1.303423    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 1.479489    Training error rate: 44.000000
800.000000/1000.000000 ==> Training loss: 1.039966    Training error rate: 28.000000
850.000000/1000.000000 ==> Training loss: 1.525991    Training error rate: 44.000000
900.000000/1000.000000 ==> Training loss: 1.504688    Training error rate: 48.000000
950.000000/1000.000000 ==> Training loss: 1.546789    Training error rate: 38.000000
==> Total training loss: 1395.739790    Total training error rate: 40.480000
==> Testing Epoch: 10
0.000000/100.000000 ==> Testing loss: 1.877012    Testing error rate: 49.000000
50.000000/100.000000 ==> Testing loss: 1.507011    Testing error rate: 44.000000
==> Total testing loss: 173.279470    Total testing error rate: 45.880000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 11
0.000000/1000.000000 ==> Training loss: 1.311020    Training error rate: 36.000000
50.000000/1000.000000 ==> Training loss: 1.559515    Training error rate: 48.000000
100.000000/1000.000000 ==> Training loss: 1.627589    Training error rate: 48.000000
150.000000/1000.000000 ==> Training loss: 1.203147    Training error rate: 36.000000
200.000000/1000.000000 ==> Training loss: 1.323186    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 1.649683    Training error rate: 52.000000
300.000000/1000.000000 ==> Training loss: 1.096161    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.966470    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 1.298168    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.596766    Training error rate: 44.000000
500.000000/1000.000000 ==> Training loss: 1.203436    Training error rate: 28.000000
550.000000/1000.000000 ==> Training loss: 1.087285    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 1.256607    Training error rate: 40.000000
650.000000/1000.000000 ==> Training loss: 1.551264    Training error rate: 50.000000
700.000000/1000.000000 ==> Training loss: 1.185382    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 1.462989    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.505102    Training error rate: 48.000000
850.000000/1000.000000 ==> Training loss: 1.326134    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 1.340112    Training error rate: 46.000000
950.000000/1000.000000 ==> Training loss: 1.878640    Training error rate: 60.000000
==> Total training loss: 1301.646671    Total training error rate: 37.654000
==> Testing Epoch: 11
0.000000/100.000000 ==> Testing loss: 1.623534    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.388697    Testing error rate: 37.000000
==> Total testing loss: 160.190827    Total testing error rate: 43.100000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 12
0.000000/1000.000000 ==> Training loss: 1.038240    Training error rate: 30.000000
50.000000/1000.000000 ==> Training loss: 0.933917    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 1.024173    Training error rate: 30.000000
150.000000/1000.000000 ==> Training loss: 1.391266    Training error rate: 38.000000
200.000000/1000.000000 ==> Training loss: 1.417287    Training error rate: 36.000000
250.000000/1000.000000 ==> Training loss: 1.024571    Training error rate: 34.000000
300.000000/1000.000000 ==> Training loss: 1.357576    Training error rate: 44.000000
350.000000/1000.000000 ==> Training loss: 0.931764    Training error rate: 32.000000
400.000000/1000.000000 ==> Training loss: 1.281607    Training error rate: 42.000000
450.000000/1000.000000 ==> Training loss: 1.186679    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 1.235408    Training error rate: 32.000000
550.000000/1000.000000 ==> Training loss: 1.012309    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 1.035232    Training error rate: 26.000000
650.000000/1000.000000 ==> Training loss: 1.675502    Training error rate: 48.000000
700.000000/1000.000000 ==> Training loss: 1.349720    Training error rate: 40.000000
750.000000/1000.000000 ==> Training loss: 0.894750    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 1.016231    Training error rate: 32.000000
850.000000/1000.000000 ==> Training loss: 1.287959    Training error rate: 38.000000
900.000000/1000.000000 ==> Training loss: 1.356178    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.030539    Training error rate: 34.000000
==> Total training loss: 1227.321164    Total training error rate: 35.586000
==> Testing Epoch: 12
0.000000/100.000000 ==> Testing loss: 1.326007    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.435211    Testing error rate: 40.000000
==> Total testing loss: 160.022756    Total testing error rate: 43.350000
==> Set learning rate: 0.010000
==> Training Epoch: 13
0.000000/1000.000000 ==> Training loss: 1.018544    Training error rate: 32.000000
50.000000/1000.000000 ==> Training loss: 1.258538    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 1.665256    Training error rate: 46.000000
150.000000/1000.000000 ==> Training loss: 1.054802    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.118211    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 1.119600    Training error rate: 34.000000
300.000000/1000.000000 ==> Training loss: 1.448340    Training error rate: 42.000000
350.000000/1000.000000 ==> Training loss: 1.036237    Training error rate: 40.000000
400.000000/1000.000000 ==> Training loss: 1.305989    Training error rate: 44.000000
450.000000/1000.000000 ==> Training loss: 1.126205    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 1.507743    Training error rate: 36.000000
550.000000/1000.000000 ==> Training loss: 1.176796    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 1.171439    Training error rate: 32.000000
650.000000/1000.000000 ==> Training loss: 1.072597    Training error rate: 42.000000
700.000000/1000.000000 ==> Training loss: 1.142354    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 1.275478    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.319710    Training error rate: 40.000000
850.000000/1000.000000 ==> Training loss: 1.223216    Training error rate: 40.000000
900.000000/1000.000000 ==> Training loss: 0.938066    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.087273    Training error rate: 40.000000
==> Total training loss: 1159.671479    Total training error rate: 34.186000
==> Testing Epoch: 13
0.000000/100.000000 ==> Testing loss: 1.461160    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.298147    Testing error rate: 35.000000
==> Total testing loss: 145.261071    Total testing error rate: 39.920000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 14
0.000000/1000.000000 ==> Training loss: 1.118261    Training error rate: 40.000000
50.000000/1000.000000 ==> Training loss: 1.124757    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 1.239783    Training error rate: 32.000000
150.000000/1000.000000 ==> Training loss: 1.187918    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 1.061010    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 0.839640    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 1.011726    Training error rate: 38.000000
350.000000/1000.000000 ==> Training loss: 1.097311    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 1.190576    Training error rate: 30.000000
450.000000/1000.000000 ==> Training loss: 0.762178    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.708490    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 1.111733    Training error rate: 34.000000
600.000000/1000.000000 ==> Training loss: 1.046133    Training error rate: 36.000000
650.000000/1000.000000 ==> Training loss: 1.255941    Training error rate: 40.000000
700.000000/1000.000000 ==> Training loss: 0.938072    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 1.017438    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 1.218119    Training error rate: 34.000000
850.000000/1000.000000 ==> Training loss: 0.997944    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 1.173154    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.205728    Training error rate: 38.000000
==> Total training loss: 1099.191433    Total training error rate: 32.438000
==> Testing Epoch: 14
0.000000/100.000000 ==> Testing loss: 1.375886    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.271874    Testing error rate: 37.000000
==> Total testing loss: 139.792277    Total testing error rate: 39.170000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 15
0.000000/1000.000000 ==> Training loss: 1.221613    Training error rate: 38.000000
50.000000/1000.000000 ==> Training loss: 0.560222    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.772762    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.710524    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.721321    Training error rate: 28.000000
250.000000/1000.000000 ==> Training loss: 1.056720    Training error rate: 30.000000
300.000000/1000.000000 ==> Training loss: 0.965970    Training error rate: 30.000000
350.000000/1000.000000 ==> Training loss: 1.331462    Training error rate: 36.000000
400.000000/1000.000000 ==> Training loss: 1.356863    Training error rate: 38.000000
450.000000/1000.000000 ==> Training loss: 1.071047    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 1.132167    Training error rate: 34.000000
550.000000/1000.000000 ==> Training loss: 1.021435    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 1.139271    Training error rate: 36.000000
650.000000/1000.000000 ==> Training loss: 0.996715    Training error rate: 32.000000
700.000000/1000.000000 ==> Training loss: 1.033458    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.971920    Training error rate: 34.000000
800.000000/1000.000000 ==> Training loss: 1.060618    Training error rate: 38.000000
850.000000/1000.000000 ==> Training loss: 1.182297    Training error rate: 40.000000
900.000000/1000.000000 ==> Training loss: 1.509563    Training error rate: 52.000000
950.000000/1000.000000 ==> Training loss: 0.985599    Training error rate: 30.000000
==> Total training loss: 1047.713929    Total training error rate: 31.170000
==> Testing Epoch: 15
0.000000/100.000000 ==> Testing loss: 1.496948    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.263095    Testing error rate: 36.000000
==> Total testing loss: 144.440971    Total testing error rate: 40.150000
==> Set learning rate: 0.010000
==> Training Epoch: 16
0.000000/1000.000000 ==> Training loss: 1.254057    Training error rate: 34.000000
50.000000/1000.000000 ==> Training loss: 0.903395    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.769630    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.850839    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.040989    Training error rate: 28.000000
250.000000/1000.000000 ==> Training loss: 0.819836    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.714233    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.939802    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.864940    Training error rate: 26.000000
450.000000/1000.000000 ==> Training loss: 1.144696    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.938208    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 1.097088    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 1.327735    Training error rate: 38.000000
650.000000/1000.000000 ==> Training loss: 1.081001    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 1.002182    Training error rate: 24.000000
750.000000/1000.000000 ==> Training loss: 0.763017    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 1.112584    Training error rate: 36.000000
850.000000/1000.000000 ==> Training loss: 1.136391    Training error rate: 36.000000
900.000000/1000.000000 ==> Training loss: 1.290845    Training error rate: 34.000000
950.000000/1000.000000 ==> Training loss: 0.902988    Training error rate: 26.000000
==> Total training loss: 990.005387    Total training error rate: 29.318000
==> Testing Epoch: 16
0.000000/100.000000 ==> Testing loss: 1.367854    Testing error rate: 40.000000
50.000000/100.000000 ==> Testing loss: 1.418253    Testing error rate: 33.000000
==> Total testing loss: 140.218684    Total testing error rate: 38.570000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 17
0.000000/1000.000000 ==> Training loss: 1.055174    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.886549    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 1.048889    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.783463    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.717315    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.899912    Training error rate: 28.000000
300.000000/1000.000000 ==> Training loss: 1.212397    Training error rate: 40.000000
350.000000/1000.000000 ==> Training loss: 0.936780    Training error rate: 24.000000
400.000000/1000.000000 ==> Training loss: 1.002606    Training error rate: 30.000000
450.000000/1000.000000 ==> Training loss: 0.864311    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 0.774292    Training error rate: 18.000000
550.000000/1000.000000 ==> Training loss: 1.071334    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.527526    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.781021    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 1.149153    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.701995    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 1.130402    Training error rate: 40.000000
850.000000/1000.000000 ==> Training loss: 1.261313    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 0.936168    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.922013    Training error rate: 30.000000
==> Total training loss: 948.588511    Total training error rate: 28.398000
==> Testing Epoch: 17
0.000000/100.000000 ==> Testing loss: 1.285158    Testing error rate: 41.000000
50.000000/100.000000 ==> Testing loss: 1.460227    Testing error rate: 37.000000
==> Total testing loss: 134.745520    Total testing error rate: 37.230000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 18
0.000000/1000.000000 ==> Training loss: 0.926093    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.796084    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.979270    Training error rate: 28.000000
150.000000/1000.000000 ==> Training loss: 1.031440    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 0.853587    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 0.909536    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.769329    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.512024    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 1.016944    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.944859    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.733042    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.744720    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 1.094880    Training error rate: 28.000000
650.000000/1000.000000 ==> Training loss: 1.038406    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 1.081145    Training error rate: 28.000000
750.000000/1000.000000 ==> Training loss: 0.731375    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 1.187885    Training error rate: 36.000000
850.000000/1000.000000 ==> Training loss: 0.683541    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.874115    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.047111    Training error rate: 38.000000
==> Total training loss: 904.076424    Total training error rate: 27.182000
==> Testing Epoch: 18
0.000000/100.000000 ==> Testing loss: 1.441281    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.389001    Testing error rate: 38.000000
==> Total testing loss: 137.738122    Total testing error rate: 37.410000
==> Set learning rate: 0.010000
==> Training Epoch: 19
0.000000/1000.000000 ==> Training loss: 0.773415    Training error rate: 28.000000
50.000000/1000.000000 ==> Training loss: 0.827497    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 0.632235    Training error rate: 24.000000
150.000000/1000.000000 ==> Training loss: 0.971510    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 0.541616    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.711499    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 1.100118    Training error rate: 46.000000
350.000000/1000.000000 ==> Training loss: 0.869232    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 0.708827    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.699850    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 1.015908    Training error rate: 28.000000
550.000000/1000.000000 ==> Training loss: 0.772177    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.835308    Training error rate: 22.000000
650.000000/1000.000000 ==> Training loss: 1.300890    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 0.876947    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 0.829903    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 0.835801    Training error rate: 32.000000
850.000000/1000.000000 ==> Training loss: 1.057488    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 0.781430    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.949653    Training error rate: 24.000000
==> Total training loss: 876.082728    Total training error rate: 26.468000
==> Testing Epoch: 19
0.000000/100.000000 ==> Testing loss: 1.321077    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.412123    Testing error rate: 34.000000
==> Total testing loss: 136.702347    Total testing error rate: 37.090000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 20
0.000000/1000.000000 ==> Training loss: 0.767655    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.823184    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 1.076606    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.674958    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.990246    Training error rate: 30.000000
250.000000/1000.000000 ==> Training loss: 0.617291    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.708325    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.889147    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.828724    Training error rate: 28.000000
450.000000/1000.000000 ==> Training loss: 0.751438    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.649178    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.726906    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.753827    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.808747    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 1.222787    Training error rate: 38.000000
750.000000/1000.000000 ==> Training loss: 0.877584    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.855438    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.967661    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.755784    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.551310    Training error rate: 8.000000
==> Total training loss: 833.038798    Total training error rate: 25.192000
==> Testing Epoch: 20
0.000000/100.000000 ==> Testing loss: 1.455191    Testing error rate: 41.000000
50.000000/100.000000 ==> Testing loss: 1.462273    Testing error rate: 39.000000
==> Total testing loss: 140.230237    Total testing error rate: 37.290000
==> Set learning rate: 0.010000
==> Training Epoch: 21
0.000000/1000.000000 ==> Training loss: 0.811763    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.613706    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.926985    Training error rate: 30.000000
150.000000/1000.000000 ==> Training loss: 0.687967    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.678507    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.755387    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.803285    Training error rate: 32.000000
350.000000/1000.000000 ==> Training loss: 0.670693    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 1.011418    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.591748    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.780168    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 0.664683    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 1.012379    Training error rate: 30.000000
650.000000/1000.000000 ==> Training loss: 0.767051    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.710810    Training error rate: 24.000000
750.000000/1000.000000 ==> Training loss: 0.566991    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.751992    Training error rate: 20.000000
850.000000/1000.000000 ==> Training loss: 0.836663    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 1.178255    Training error rate: 30.000000
950.000000/1000.000000 ==> Training loss: 0.693405    Training error rate: 24.000000
==> Total training loss: 805.103362    Total training error rate: 24.656000
==> Testing Epoch: 21
0.000000/100.000000 ==> Testing loss: 1.480474    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.286268    Testing error rate: 31.000000
==> Total testing loss: 138.010314    Total testing error rate: 36.940000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 22
0.000000/1000.000000 ==> Training loss: 0.385826    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.302237    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.778906    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.641563    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.672638    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.530859    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.903176    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.642352    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.865655    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.853225    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.812954    Training error rate: 32.000000
550.000000/1000.000000 ==> Training loss: 1.030781    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.781286    Training error rate: 24.000000
650.000000/1000.000000 ==> Training loss: 0.531625    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.764217    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.714585    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 1.226999    Training error rate: 42.000000
850.000000/1000.000000 ==> Training loss: 0.694048    Training error rate: 26.000000
900.000000/1000.000000 ==> Training loss: 0.824467    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.659504    Training error rate: 18.000000
==> Total training loss: 773.233685    Total training error rate: 23.638000
==> Testing Epoch: 22
0.000000/100.000000 ==> Testing loss: 1.505965    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.510814    Testing error rate: 33.000000
==> Total testing loss: 135.249002    Total testing error rate: 35.820000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 23
0.000000/1000.000000 ==> Training loss: 0.941841    Training error rate: 28.000000
50.000000/1000.000000 ==> Training loss: 0.799043    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.523504    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.562729    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.922137    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.944953    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.811191    Training error rate: 20.000000
350.000000/1000.000000 ==> Training loss: 0.694753    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.621766    Training error rate: 22.000000
450.000000/1000.000000 ==> Training loss: 0.710219    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.408593    Training error rate: 12.000000
550.000000/1000.000000 ==> Training loss: 0.609686    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.504516    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.899537    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 0.951560    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.618123    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.662444    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.966857    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.793802    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 1.061447    Training error rate: 30.000000
==> Total training loss: 747.542327    Total training error rate: 22.684000
==> Testing Epoch: 23
0.000000/100.000000 ==> Testing loss: 1.350777    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.434296    Testing error rate: 32.000000
==> Total testing loss: 131.407366    Total testing error rate: 34.600000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 24
0.000000/1000.000000 ==> Training loss: 0.604323    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.856048    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.563504    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.588071    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.483137    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.747936    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.987395    Training error rate: 32.000000
350.000000/1000.000000 ==> Training loss: 0.644239    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.584988    Training error rate: 22.000000
450.000000/1000.000000 ==> Training loss: 0.668496    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.608880    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.682369    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.630751    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.753236    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.949480    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.907636    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.943212    Training error rate: 30.000000
850.000000/1000.000000 ==> Training loss: 1.020969    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.845161    Training error rate: 28.000000
950.000000/1000.000000 ==> Training loss: 0.587399    Training error rate: 18.000000
==> Total training loss: 727.030024    Total training error rate: 22.458000
==> Testing Epoch: 24
0.000000/100.000000 ==> Testing loss: 1.381206    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.042953    Testing error rate: 30.000000
==> Total testing loss: 130.848066    Total testing error rate: 34.620000
==> Set learning rate: 0.010000
==> Training Epoch: 25
0.000000/1000.000000 ==> Training loss: 0.655942    Training error rate: 32.000000
50.000000/1000.000000 ==> Training loss: 0.641364    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.539858    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.518427    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.552606    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.680367    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.731961    Training error rate: 22.000000
350.000000/1000.000000 ==> Training loss: 0.944749    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.571494    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.505482    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.504386    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.896310    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.705431    Training error rate: 26.000000
650.000000/1000.000000 ==> Training loss: 0.792656    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.789050    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 0.597255    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.866821    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.579395    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.794953    Training error rate: 28.000000
950.000000/1000.000000 ==> Training loss: 0.731680    Training error rate: 24.000000
==> Total training loss: 698.406479    Total training error rate: 21.556000
==> Testing Epoch: 25
0.000000/100.000000 ==> Testing loss: 1.321187    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.090315    Testing error rate: 29.000000
==> Total testing loss: 127.379310    Total testing error rate: 34.670000
==> Set learning rate: 0.010000
==> Training Epoch: 26
0.000000/1000.000000 ==> Training loss: 0.697295    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.644665    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.567076    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.405677    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.496837    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.559085    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.541182    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.547433    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.728428    Training error rate: 20.000000
450.000000/1000.000000 ==> Training loss: 0.808685    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.722217    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.726590    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.594902    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.914522    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 0.881229    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.794620    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.932929    Training error rate: 34.000000
850.000000/1000.000000 ==> Training loss: 0.725854    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.634211    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.810605    Training error rate: 30.000000
==> Total training loss: 676.413966    Total training error rate: 20.674000
==> Testing Epoch: 26
0.000000/100.000000 ==> Testing loss: 1.136712    Testing error rate: 30.000000
50.000000/100.000000 ==> Testing loss: 1.198634    Testing error rate: 29.000000
==> Total testing loss: 127.802508    Total testing error rate: 33.790000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 27
0.000000/1000.000000 ==> Training loss: 0.637743    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.551723    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 0.610010    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.607348    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.570131    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.583517    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.595628    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.651267    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.467685    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.707869    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.679716    Training error rate: 26.000000
550.000000/1000.000000 ==> Training loss: 0.610795    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.791844    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.653148    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.767559    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.812972    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.642997    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.416368    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.686387    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.812223    Training error rate: 24.000000
==> Total training loss: 650.128888    Total training error rate: 20.248000
==> Testing Epoch: 27
0.000000/100.000000 ==> Testing loss: 1.193094    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.400613    Testing error rate: 34.000000
==> Total testing loss: 135.267944    Total testing error rate: 35.520000
==> Set learning rate: 0.010000
==> Training Epoch: 28
0.000000/1000.000000 ==> Training loss: 0.640792    Training error rate: 20.000000
50.000000/1000.000000 ==> Training loss: 0.577937    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.480967    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.599682    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.601168    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.877462    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 0.402524    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.676769    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.633084    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.767712    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.781782    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.754575    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.848044    Training error rate: 28.000000
650.000000/1000.000000 ==> Training loss: 0.378646    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.576601    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.640841    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.409048    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.735445    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.834494    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.691417    Training error rate: 22.000000
==> Total training loss: 641.710395    Total training error rate: 19.792000
==> Testing Epoch: 28
0.000000/100.000000 ==> Testing loss: 1.379735    Testing error rate: 33.000000
50.000000/100.000000 ==> Testing loss: 1.126057    Testing error rate: 28.000000
==> Total testing loss: 129.511713    Total testing error rate: 34.020000
==> Set learning rate: 0.010000
==> Training Epoch: 29
0.000000/1000.000000 ==> Training loss: 0.575846    Training error rate: 20.000000
50.000000/1000.000000 ==> Training loss: 0.522687    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.612530    Training error rate: 20.000000
150.000000/1000.000000 ==> Training loss: 0.484939    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.615149    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.590311    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.375446    Training error rate: 8.000000
350.000000/1000.000000 ==> Training loss: 0.530687    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.564189    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.462727    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.557025    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.858578    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.750054    Training error rate: 22.000000
650.000000/1000.000000 ==> Training loss: 0.747648    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.704276    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.467301    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.748628    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.761602    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.534158    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.657249    Training error rate: 24.000000
==> Total training loss: 616.169580    Total training error rate: 19.128000
==> Testing Epoch: 29
0.000000/100.000000 ==> Testing loss: 1.219901    Testing error rate: 31.000000
50.000000/100.000000 ==> Testing loss: 1.380697    Testing error rate: 40.000000
==> Total testing loss: 127.717714    Total testing error rate: 33.740000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 30
0.000000/1000.000000 ==> Training loss: 0.300561    Training error rate: 4.000000
50.000000/1000.000000 ==> Training loss: 0.698130    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.506481    Training error rate: 22.000000
150.000000/1000.000000 ==> Training loss: 0.419953    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.843976    Training error rate: 22.000000
250.000000/1000.000000 ==> Training loss: 0.270207    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.737075    Training error rate: 18.000000
350.000000/1000.000000 ==> Training loss: 0.652305    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.602375    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.645290    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.829702    Training error rate: 30.000000
550.000000/1000.000000 ==> Training loss: 0.546342    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.506395    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.622368    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.788734    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.638477    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.632868    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.650862    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.548122    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.687617    Training error rate: 22.000000
==> Total training loss: 610.352457    Total training error rate: 18.886000
==> Testing Epoch: 30
0.000000/100.000000 ==> Testing loss: 1.182697    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.471372    Testing error rate: 40.000000
==> Total testing loss: 133.991861    Total testing error rate: 34.240000
==> Set learning rate: 0.010000
==> Training Epoch: 31
0.000000/1000.000000 ==> Training loss: 0.667576    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.514208    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.617630    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.480765    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.610526    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.468969    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.945008    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.581384    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.363959    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.758432    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.634695    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.553093    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.556681    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.646187    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.553672    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.533663    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.602935    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.576443    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.706883    Training error rate: 30.000000
950.000000/1000.000000 ==> Training loss: 0.795737    Training error rate: 24.000000
==> Total training loss: 592.895187    Total training error rate: 18.432000
==> Testing Epoch: 31
0.000000/100.000000 ==> Testing loss: 1.279816    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.536136    Testing error rate: 35.000000
==> Total testing loss: 135.527912    Total testing error rate: 34.780000
==> Set learning rate: 0.010000
==> Training Epoch: 32
0.000000/1000.000000 ==> Training loss: 0.615069    Training error rate: 22.000000
50.000000/1000.000000 ==> Training loss: 0.669840    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.425765    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.628283    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.446802    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.461664    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.659031    Training error rate: 18.000000
350.000000/1000.000000 ==> Training loss: 0.471534    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.554291    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.821254    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 0.500932    Training error rate: 10.000000
550.000000/1000.000000 ==> Training loss: 0.622824    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.479912    Training error rate: 18.000000
650.000000/1000.000000 ==> Training loss: 0.642036    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.644528    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.835607    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.779741    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.658465    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.549923    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.571006    Training error rate: 14.000000
==> Total training loss: 578.608277    Total training error rate: 17.968000
==> Testing Epoch: 32
0.000000/100.000000 ==> Testing loss: 1.331419    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.472029    Testing error rate: 38.000000
==> Total testing loss: 133.881347    Total testing error rate: 34.920000
==> Set learning rate: 0.010000
==> Training Epoch: 33
0.000000/1000.000000 ==> Training loss: 0.376245    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.474759    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.368760    Training error rate: 8.000000
150.000000/1000.000000 ==> Training loss: 0.609937    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.443883    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.509917    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.451320    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.758068    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.477627    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.830653    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.529440    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 0.831604    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 0.704390    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.571816    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.589993    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.776993    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 0.440612    Training error rate: 10.000000
850.000000/1000.000000 ==> Training loss: 0.709979    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.767564    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.486793    Training error rate: 12.000000
==> Total training loss: 568.747226    Total training error rate: 17.668000
==> Testing Epoch: 33
0.000000/100.000000 ==> Testing loss: 1.496711    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.248182    Testing error rate: 35.000000
==> Total testing loss: 137.825184    Total testing error rate: 34.550000
==> Set learning rate: 0.010000
==> Training Epoch: 34
0.000000/1000.000000 ==> Training loss: 0.599273    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.453589    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.559413    Training error rate: 24.000000
150.000000/1000.000000 ==> Training loss: 0.362409    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.635144    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.385949    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.546547    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.433191    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.601622    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.451471    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.572335    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.392155    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.567442    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.609029    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.528576    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.440651    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.465503    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.741505    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.808153    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.722620    Training error rate: 24.000000
==> Total training loss: 544.274256    Total training error rate: 16.796000
==> Testing Epoch: 34
0.000000/100.000000 ==> Testing loss: 1.091064    Testing error rate: 30.000000
50.000000/100.000000 ==> Testing loss: 1.441337    Testing error rate: 34.000000
==> Total testing loss: 130.364300    Total testing error rate: 33.100000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 35
0.000000/1000.000000 ==> Training loss: 0.493398    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.517093    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.353866    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.349815    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.522168    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.778699    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.500315    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.376967    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.460305    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.368589    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.364994    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.426591    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.520659    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.488706    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.514119    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.321539    Training error rate: 8.000000
800.000000/1000.000000 ==> Training loss: 0.352800    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.747712    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.543496    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.385076    Training error rate: 18.000000
==> Total training loss: 537.894123    Total training error rate: 16.742000
==> Testing Epoch: 35
0.000000/100.000000 ==> Testing loss: 1.416957    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.298468    Testing error rate: 35.000000
==> Total testing loss: 128.735879    Total testing error rate: 33.180000
==> Set learning rate: 0.010000
==> Training Epoch: 36
0.000000/1000.000000 ==> Training loss: 0.473164    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.433537    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.467526    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.443184    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.617312    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.696765    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.382543    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.443756    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.459088    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.480450    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.516917    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.404938    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.250042    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.868545    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.427695    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.760578    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.595996    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.389359    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.581635    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.332481    Training error rate: 12.000000
==> Total training loss: 530.333477    Total training error rate: 16.518000
==> Testing Epoch: 36
0.000000/100.000000 ==> Testing loss: 1.491593    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.439892    Testing error rate: 33.000000
==> Total testing loss: 132.689520    Total testing error rate: 33.590000
==> Set learning rate: 0.010000
==> Training Epoch: 37
0.000000/1000.000000 ==> Training loss: 0.522319    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.337910    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.548351    Training error rate: 20.000000
150.000000/1000.000000 ==> Training loss: 0.572943    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.291482    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.392987    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.857531    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.416996    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.345856    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.720703    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.667278    Training error rate: 18.000000
550.000000/1000.000000 ==> Training loss: 0.518329    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.513659    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.516430    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.886918    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.589640    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.722260    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.764057    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.438010    Training error rate: 10.000000
950.000000/1000.000000 ==> Training loss: 0.519963    Training error rate: 14.000000
==> Total training loss: 514.767130    Total training error rate: 16.014000
==> Testing Epoch: 37
0.000000/100.000000 ==> Testing loss: 1.362627    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.494166    Testing error rate: 37.000000
==> Total testing loss: 135.344767    Total testing error rate: 34.270000
==> Set learning rate: 0.010000
==> Training Epoch: 38
0.000000/1000.000000 ==> Training loss: 0.390156    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.210681    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.212854    Training error rate: 6.000000
150.000000/1000.000000 ==> Training loss: 0.453237    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.567344    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.397695    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.659693    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.455969    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.388112    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.499256    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.281768    Training error rate: 10.000000
550.000000/1000.000000 ==> Training loss: 0.940469    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.354611    Training error rate: 12.000000
650.000000/1000.000000 ==> Training loss: 0.586523    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.491159    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.653063    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.430160    Training error rate: 12.000000
850.000000/1000.000000 ==> Training loss: 0.585682    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.761453    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.488902    Training error rate: 14.000000
==> Total training loss: 508.658543    Total training error rate: 15.914000
==> Testing Epoch: 38
0.000000/100.000000 ==> Testing loss: 1.669469    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.403951    Testing error rate: 35.000000
==> Total testing loss: 144.760002    Total testing error rate: 35.390000
==> Set learning rate: 0.010000
==> Training Epoch: 39
0.000000/1000.000000 ==> Training loss: 0.713053    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.360587    Training error rate: 8.000000
100.000000/1000.000000 ==> Training loss: 0.333455    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.469013    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.373242    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.424465    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.788312    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.402207    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.570696    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.537472    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.705001    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.544030    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.502664    Training error rate: 18.000000
650.000000/1000.000000 ==> Training loss: 0.349189    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.606753    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.480229    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.416004    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.641803    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.490630    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.656467    Training error rate: 22.000000
==> Total training loss: 511.452728    Total training error rate: 16.054000
==> Testing Epoch: 39
0.000000/100.000000 ==> Testing loss: 1.325889    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.391128    Testing error rate: 33.000000
==> Total testing loss: 133.601643    Total testing error rate: 33.750000
==> Set learning rate: 0.010000
==> Training Epoch: 40
0.000000/1000.000000 ==> Training loss: 0.716802    Training error rate: 30.000000
50.000000/1000.000000 ==> Training loss: 0.375767    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.348400    Training error rate: 10.000000
150.000000/1000.000000 ==> Training loss: 0.386557    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.490948    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.408495    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.239014    Training error rate: 6.000000
350.000000/1000.000000 ==> Training loss: 0.360666    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.521634    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.311205    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.692995    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.503899    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.571210    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.730871    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.454530    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.437251    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.784645    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.749598    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.507238    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.742778    Training error rate: 22.000000
==> Total training loss: 490.604032    Total training error rate: 15.276000
==> Testing Epoch: 40
0.000000/100.000000 ==> Testing loss: 1.388763    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.460940    Testing error rate: 34.000000
==> Total testing loss: 133.309104    Total testing error rate: 33.510000
==> Set learning rate: 0.010000
==> Training Epoch: 41
0.000000/1000.000000 ==> Training loss: 0.400794    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.552497    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.690024    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.691139    Training error rate: 26.000000
200.000000/1000.000000 ==> Training loss: 0.403494    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.454619    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.611817    Training error rate: 22.000000
350.000000/1000.000000 ==> Training loss: 0.284257    Training error rate: 8.000000
400.000000/1000.000000 ==> Training loss: 0.304677    Training error rate: 8.000000
450.000000/1000.000000 ==> Training loss: 0.488424    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.425047    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.485073    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.492987    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.392459    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.420543    Training error rate: 20.000000
750.000000/1000.000000 ==> Training loss: 0.349525    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.531872    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.396246    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.469083    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.736115    Training error rate: 22.000000
==> Total training loss: 482.457945    Total training error rate: 14.984000
==> Testing Epoch: 41
0.000000/100.000000 ==> Testing loss: 1.286527    Testing error rate: 31.000000
50.000000/100.000000 ==> Testing loss: 1.535186    Testing error rate: 41.000000
==> Total testing loss: 139.527525    Total testing error rate: 33.960000
==> Set learning rate: 0.010000
==> Training Epoch: 42
0.000000/1000.000000 ==> Training loss: 0.588413    Training error rate: 22.000000
50.000000/1000.000000 ==> Training loss: 0.583433    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.254410    Training error rate: 8.000000
150.000000/1000.000000 ==> Training loss: 0.348387    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.198521    Training error rate: 4.000000
250.000000/1000.000000 ==> Training loss: 0.424969    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.421748    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.449636    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.564013    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.470953    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.439111    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.566951    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.869370    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.212606    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.795025    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.538101    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.508715    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.432299    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.575001    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.418013    Training error rate: 14.000000
==> Total training loss: 485.551058    Total training error rate: 15.158000
==> Testing Epoch: 42
0.000000/100.000000 ==> Testing loss: 1.276446    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.411205    Testing error rate: 36.000000
==> Total testing loss: 133.642553    Total testing error rate: 33.830000
==> Set learning rate: 0.010000
==> Training Epoch: 43
0.000000/1000.000000 ==> Training loss: 0.268995    Training error rate: 6.000000
50.000000/1000.000000 ==> Training loss: 0.483741    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.404081    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.323366    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.669584    Training error rate: 22.000000
250.000000/1000.000000 ==> Training loss: 0.517139    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.528442    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.401222    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.408127    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.422027    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.469808    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.435880    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.676620    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.382778    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.430895    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.272249    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.807660    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.329221    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.588858    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.484126    Training error rate: 12.000000
==> Total training loss: 461.249712    Total training error rate: 14.458000
==> Testing Epoch: 43
0.000000/100.000000 ==> Testing loss: 1.401996    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.405297    Testing error rate: 31.000000
==> Total testing loss: 142.204903    Total testing error rate: 34.570000
==> Set learning rate: 0.010000
==> Training Epoch: 44
0.000000/1000.000000 ==> Training loss: 0.521284    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.326952    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.353533    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.498530    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.427460    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.349093    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.311323    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.337610    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.929363    Training error rate: 28.000000
450.000000/1000.000000 ==> Training loss: 0.502359    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.474299    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.644784    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.544656    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.418992    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.544744    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.677972    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.468367    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.603698    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.460167    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.414193    Training error rate: 12.000000
==> Total training loss: 460.236724    Total training error rate: 14.322000
==> Testing Epoch: 44
0.000000/100.000000 ==> Testing loss: 1.217281    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.287425    Testing error rate: 30.000000
==> Total testing loss: 136.454186    Total testing error rate: 33.370000
==> Set learning rate: 0.010000
==> Training Epoch: 45
0.000000/1000.000000 ==> Training loss: 0.430786    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.305765    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.326511    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.445151    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.351924    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.487034    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.348738    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.280281    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.374279    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.526731    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.709168    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.641037    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.673947    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.493031    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.863913    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 0.446785    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.953513    Training error rate: 28.000000
850.000000/1000.000000 ==> Training loss: 0.608976    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.604473    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.494909    Training error rate: 14.000000
==> Total training loss: 454.403468    Total training error rate: 14.204000
==> Testing Epoch: 45
0.000000/100.000000 ==> Testing loss: 1.304280    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.244558    Testing error rate: 33.000000
==> Total testing loss: 137.683011    Total testing error rate: 33.340000
==> Set learning rate: 0.010000
==> Training Epoch: 46
0.000000/1000.000000 ==> Training loss: 0.410263    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.319745    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.592297    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.366854    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.469393    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.320772    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.202899    Training error rate: 6.000000
350.000000/1000.000000 ==> Training loss: 0.471216    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.471814    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.523177    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.368291    Training error rate: 12.000000
550.000000/1000.000000 ==> Training loss: 0.488721    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.388250    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.337835    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.474120    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.304200    Training error rate: 8.000000
800.000000/1000.000000 ==> Training loss: 0.445077    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.430269    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.644756    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.239597    Training error rate: 8.000000
==> Total training loss: 450.199352    Total training error rate: 14.184000
==> Testing Epoch: 46
0.000000/100.000000 ==> Testing loss: 1.836052    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.678423    Testing error rate: 36.000000
==> Total testing loss: 145.251171    Total testing error rate: 34.780000
==> Set learning rate: 0.010000
==> Training Epoch: 47
0.000000/1000.000000 ==> Training loss: 0.490157    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.312375    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.246098    Training error rate: 6.000000
150.000000/1000.000000 ==> Training loss: 0.720378    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.359708    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.395107    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.394884    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.266999    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.276269    Training error rate: 8.000000
450.000000/1000.000000 ==> Training loss: 0.691242    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.646556    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.457709    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.435227    Training error rate: 12.000000
650.000000/1000.000000 ==> Training loss: 0.313825    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.354125    Training error rate: 10.000000
750.000000/1000.000000 ==> Training loss: 0.339714    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.515155    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.802391    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.374395    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.638388    Training error rate: 18.000000
==> Total training loss: 453.612544    Total training error rate: 14.124000
==> Testing Epoch: 47
0.000000/100.000000 ==> Testing loss: 1.587674    Testing error rate: 33.000000
50.000000/100.000000 ==> Testing loss: 1.214726    Testing error rate: 35.000000
==> Total testing loss: 136.184986    Total testing error rate: 33.200000
==> Set learning rate: 0.010000
==> Training Epoch: 48
0.000000/1000.000000 ==> Training loss: 0.291185    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.547348    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.419140    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.248258    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.602171    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.250053    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.498472    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.398649    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.379074    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.513914    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.417094    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.660054    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.266843    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.336535    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.667075    Training error rate: 20.000000
750.000000/1000.000000 ==> Training loss: 0.758635    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.339449    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.261778    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.472816    Training error rate: 12.000000
950.000000/1000.000000 ==> Training loss: 0.292143    Training error rate: 4.000000
==> Total training loss: 432.183565    Total training error rate: 13.526000
==> Testing Epoch: 48
0.000000/100.000000 ==> Testing loss: 1.483737    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.292743    Testing error rate: 30.000000
==> Total testing loss: 133.245294    Total testing error rate: 33.460000
==> Set learning rate: 0.010000
==> Training Epoch: 49
0.000000/1000.000000 ==> Training loss: 0.274462    Training error rate: 8.000000
50.000000/1000.000000 ==> Training loss: 0.320562    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.192347    Training error rate: 4.000000
150.000000/1000.000000 ==> Training loss: 0.296792    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.370274    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.291290    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.395462    Training error rate: 14.000000
350.000000/1000.000000 ==> Training loss: 0.447141    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.694239    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.391202    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.314588    Training error rate: 8.000000
550.000000/1000.000000 ==> Training loss: 0.522208    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.350614    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.629715    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.748747    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.634868    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.484708    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.231160    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.678211    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.405243    Training error rate: 12.000000
==> Total training loss: 434.063528    Total training error rate: 13.640000
==> Testing Epoch: 49
0.000000/100.000000 ==> Testing loss: 1.341824    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.651895    Testing error rate: 37.000000
==> Total testing loss: 132.950175    Total testing error rate: 32.860000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 50
0.000000/1000.000000 ==> Training loss: 0.271199    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.368749    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.334075    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.458919    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.540437    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.333041    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.357528    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.355388    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.452689    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.391300    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.344761    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.308382    Training error rate: 6.000000
600.000000/1000.000000 ==> Training loss: 0.434797    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.527235    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.473409    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.541115    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.221035    Training error rate: 8.000000
850.000000/1000.000000 ==> Training loss: 0.317308    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.460594    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.379441    Training error rate: 12.000000
==> Total training loss: 414.474264    Total training error rate: 13.054000
==> Testing Epoch: 50
0.000000/100.000000 ==> Testing loss: 1.411966    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.248434    Testing error rate: 29.000000
==> Total testing loss: 130.023694    Total testing error rate: 32.080000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 51
0.000000/1000.000000 ==> Training loss: 0.548065    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.530744    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.343395    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.170699    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.308693    Training error rate: 8.000000
250.000000/1000.000000 ==> Training loss: 0.087190    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.324754    Training error rate: 14.000000
350.000000/1000.000000 ==> Training loss: 0.187241    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.168844    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.186813    Training error rate: 8.000000
500.000000/1000.000000 ==> Training loss: 0.231997    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.105977    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.128095    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.162911    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.187772    Training error rate: 6.000000
750.000000/1000.000000 ==> Training loss: 0.241723    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.275930    Training error rate: 8.000000
850.000000/1000.000000 ==> Training loss: 0.246420    Training error rate: 6.000000
900.000000/1000.000000 ==> Training loss: 0.086957    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.111015    Training error rate: 2.000000
==> Total training loss: 188.944883    Total training error rate: 5.376000
==> Testing Epoch: 51
0.000000/100.000000 ==> Testing loss: 1.104177    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.076446    Testing error rate: 26.000000
==> Total testing loss: 100.681273    Total testing error rate: 25.650000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 52
0.000000/1000.000000 ==> Training loss: 0.041489    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.087955    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.106856    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.262815    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.054615    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.087994    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.076306    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.082756    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.065131    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.164704    Training error rate: 6.000000
500.000000/1000.000000 ==> Training loss: 0.093612    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.063397    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.161989    Training error rate: 6.000000
650.000000/1000.000000 ==> Training loss: 0.206366    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.144986    Training error rate: 4.000000
750.000000/1000.000000 ==> Training loss: 0.074154    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.091502    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.078383    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.158312    Training error rate: 4.000000
950.000000/1000.000000 ==> Training loss: 0.094958    Training error rate: 4.000000
==> Total training loss: 114.190603    Total training error rate: 2.926000
==> Testing Epoch: 52
0.000000/100.000000 ==> Testing loss: 1.114215    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.081510    Testing error rate: 26.000000
==> Total testing loss: 99.513879    Total testing error rate: 25.300000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 53
0.000000/1000.000000 ==> Training loss: 0.056279    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.093921    Training error rate: 4.000000
100.000000/1000.000000 ==> Training loss: 0.105373    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.177438    Training error rate: 4.000000
200.000000/1000.000000 ==> Training loss: 0.115392    Training error rate: 6.000000
250.000000/1000.000000 ==> Training loss: 0.023996    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.073734    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.022851    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.107368    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.045182    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.087432    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.057751    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.350080    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.031174    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.052472    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.086697    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.082900    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.120121    Training error rate: 6.000000
900.000000/1000.000000 ==> Training loss: 0.126391    Training error rate: 4.000000
950.000000/1000.000000 ==> Training loss: 0.106905    Training error rate: 6.000000
==> Total training loss: 90.951876    Total training error rate: 2.206000
==> Testing Epoch: 53
0.000000/100.000000 ==> Testing loss: 1.136514    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.078532    Testing error rate: 26.000000
==> Total testing loss: 98.590839    Total testing error rate: 25.240000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 54
0.000000/1000.000000 ==> Training loss: 0.113570    Training error rate: 4.000000
50.000000/1000.000000 ==> Training loss: 0.077907    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.085364    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.093794    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.048368    Training error rate: 2.000000
250.000000/1000.000000 ==> Training loss: 0.096128    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.031369    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.106214    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.044870    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.076299    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.108279    Training error rate: 2.000000
550.000000/1000.000000 ==> Training loss: 0.040064    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.039673    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.194496    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.042963    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.054935    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.045241    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.061993    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.062955    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.082277    Training error rate: 2.000000
==> Total training loss: 75.623383    Total training error rate: 1.646000
==> Testing Epoch: 54
0.000000/100.000000 ==> Testing loss: 1.154672    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.126218    Testing error rate: 25.000000
==> Total testing loss: 98.961594    Total testing error rate: 25.030000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 55
0.000000/1000.000000 ==> Training loss: 0.025742    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.078817    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.066502    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.054367    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.063125    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.050767    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.110262    Training error rate: 4.000000
350.000000/1000.000000 ==> Training loss: 0.044629    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.098108    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.044445    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.106241    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.048584    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.034466    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.038486    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.067968    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.080149    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.065024    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.097756    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.033506    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.102009    Training error rate: 2.000000
==> Total training loss: 68.041148    Total training error rate: 1.518000
==> Testing Epoch: 55
0.000000/100.000000 ==> Testing loss: 1.144079    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.141368    Testing error rate: 24.000000
==> Total testing loss: 99.233717    Total testing error rate: 24.840000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 56
0.000000/1000.000000 ==> Training loss: 0.031017    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.023896    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.062789    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.029212    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.045258    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.030282    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.014496    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.026994    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.074402    Training error rate: 4.000000
450.000000/1000.000000 ==> Training loss: 0.040946    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.048940    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.068985    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.059540    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.040985    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.037657    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.043446    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.066146    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.036403    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.074663    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.058206    Training error rate: 0.000000
==> Total training loss: 61.289695    Total training error rate: 1.348000
==> Testing Epoch: 56
0.000000/100.000000 ==> Testing loss: 1.155417    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.100886    Testing error rate: 25.000000
==> Total testing loss: 99.680632    Total testing error rate: 25.210000
==> Set learning rate: 0.001000
==> Training Epoch: 57
0.000000/1000.000000 ==> Training loss: 0.045787    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.044946    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.030281    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.097151    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.074337    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.007269    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.067873    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.095476    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.054860    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.037578    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.056823    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.036744    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.051314    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.032107    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.065067    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.042080    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.030135    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.036866    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.034153    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.032587    Training error rate: 0.000000
==> Total training loss: 55.976676    Total training error rate: 1.154000
==> Testing Epoch: 57
0.000000/100.000000 ==> Testing loss: 1.182112    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.179492    Testing error rate: 26.000000
==> Total testing loss: 99.916698    Total testing error rate: 24.790000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 58
0.000000/1000.000000 ==> Training loss: 0.061238    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.038715    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.047444    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.043027    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.011927    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.034696    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.030611    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.040962    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.068296    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.073488    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.032725    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.081250    Training error rate: 4.000000
600.000000/1000.000000 ==> Training loss: 0.036159    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.026027    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.040845    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.063127    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.048407    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.085359    Training error rate: 4.000000
900.000000/1000.000000 ==> Training loss: 0.025505    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.084730    Training error rate: 4.000000
==> Total training loss: 49.025808    Total training error rate: 0.916000
==> Testing Epoch: 58
0.000000/100.000000 ==> Testing loss: 1.220926    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.160979    Testing error rate: 26.000000
==> Total testing loss: 100.286230    Total testing error rate: 24.660000
==> Saving checkpoint..==> Init variables..
==> Init seed..
==> Download data..
Files already downloaded and verified
==> Calculate mean and std..
==> Prepare training transform..
==> Prepare testing transform..
==> Init dataloader..
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
==> Set learning rate: 0.010000
==> Training Epoch: 1
0.000000/1000.000000 ==> Training loss: 5.099441    Training error rate: 100.000000
50.000000/1000.000000 ==> Training loss: 5.162900    Training error rate: 100.000000
100.000000/1000.000000 ==> Training loss: 4.416446    Training error rate: 92.000000
150.000000/1000.000000 ==> Training loss: 4.146763    Training error rate: 92.000000
200.000000/1000.000000 ==> Training loss: 4.196798    Training error rate: 94.000000
250.000000/1000.000000 ==> Training loss: 4.275916    Training error rate: 98.000000
300.000000/1000.000000 ==> Training loss: 4.085358    Training error rate: 96.000000
350.000000/1000.000000 ==> Training loss: 3.955751    Training error rate: 92.000000
400.000000/1000.000000 ==> Training loss: 4.038115    Training error rate: 90.000000
450.000000/1000.000000 ==> Training loss: 3.933294    Training error rate: 90.000000
500.000000/1000.000000 ==> Training loss: 3.824020    Training error rate: 84.000000
550.000000/1000.000000 ==> Training loss: 4.291460    Training error rate: 98.000000
600.000000/1000.000000 ==> Training loss: 3.678003    Training error rate: 86.000000
650.000000/1000.000000 ==> Training loss: 3.824186    Training error rate: 90.000000
700.000000/1000.000000 ==> Training loss: 4.028839    Training error rate: 94.000000
750.000000/1000.000000 ==> Training loss: 3.705155    Training error rate: 82.000000
800.000000/1000.000000 ==> Training loss: 3.787447    Training error rate: 88.000000
850.000000/1000.000000 ==> Training loss: 3.356160    Training error rate: 88.000000
900.000000/1000.000000 ==> Training loss: 3.684165    Training error rate: 86.000000
950.000000/1000.000000 ==> Training loss: 3.438225    Training error rate: 82.000000
==> Total training loss: 4026.317283    Total training error rate: 92.072000
==> Testing Epoch: 1
0.000000/100.000000 ==> Testing loss: 3.758418    Testing error rate: 86.000000
50.000000/100.000000 ==> Testing loss: 3.650816    Testing error rate: 81.000000
==> Total testing loss: 365.586010    Total testing error rate: 86.580000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 2
0.000000/1000.000000 ==> Training loss: 3.549226    Training error rate: 86.000000
50.000000/1000.000000 ==> Training loss: 3.510678    Training error rate: 88.000000
100.000000/1000.000000 ==> Training loss: 3.433121    Training error rate: 84.000000
150.000000/1000.000000 ==> Training loss: 3.509465    Training error rate: 90.000000
200.000000/1000.000000 ==> Training loss: 3.349941    Training error rate: 82.000000
250.000000/1000.000000 ==> Training loss: 3.458431    Training error rate: 92.000000
300.000000/1000.000000 ==> Training loss: 3.431476    Training error rate: 82.000000
350.000000/1000.000000 ==> Training loss: 3.303687    Training error rate: 78.000000
400.000000/1000.000000 ==> Training loss: 3.742269    Training error rate: 94.000000
450.000000/1000.000000 ==> Training loss: 3.477706    Training error rate: 78.000000
500.000000/1000.000000 ==> Training loss: 3.338876    Training error rate: 86.000000
550.000000/1000.000000 ==> Training loss: 3.078589    Training error rate: 72.000000
600.000000/1000.000000 ==> Training loss: 3.103625    Training error rate: 82.000000
650.000000/1000.000000 ==> Training loss: 3.437057    Training error rate: 82.000000
700.000000/1000.000000 ==> Training loss: 3.287236    Training error rate: 76.000000
750.000000/1000.000000 ==> Training loss: 2.865732    Training error rate: 78.000000
800.000000/1000.000000 ==> Training loss: 3.272567    Training error rate: 82.000000
850.000000/1000.000000 ==> Training loss: 3.279181    Training error rate: 80.000000
900.000000/1000.000000 ==> Training loss: 2.880152    Training error rate: 74.000000
950.000000/1000.000000 ==> Training loss: 3.383698    Training error rate: 72.000000
==> Total training loss: 3359.914930    Total training error rate: 81.912000
==> Testing Epoch: 2
0.000000/100.000000 ==> Testing loss: 3.374336    Testing error rate: 80.000000
50.000000/100.000000 ==> Testing loss: 3.117177    Testing error rate: 74.000000
==> Total testing loss: 315.195562    Total testing error rate: 77.330000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 3
0.000000/1000.000000 ==> Training loss: 3.145865    Training error rate: 74.000000
50.000000/1000.000000 ==> Training loss: 2.870741    Training error rate: 62.000000
100.000000/1000.000000 ==> Training loss: 3.297317    Training error rate: 82.000000
150.000000/1000.000000 ==> Training loss: 3.799235    Training error rate: 84.000000
200.000000/1000.000000 ==> Training loss: 2.678745    Training error rate: 82.000000
250.000000/1000.000000 ==> Training loss: 2.843819    Training error rate: 74.000000
300.000000/1000.000000 ==> Training loss: 3.275985    Training error rate: 78.000000
350.000000/1000.000000 ==> Training loss: 2.756059    Training error rate: 66.000000
400.000000/1000.000000 ==> Training loss: 2.847559    Training error rate: 78.000000
450.000000/1000.000000 ==> Training loss: 2.795875    Training error rate: 72.000000
500.000000/1000.000000 ==> Training loss: 2.741857    Training error rate: 68.000000
550.000000/1000.000000 ==> Training loss: 2.882214    Training error rate: 70.000000
600.000000/1000.000000 ==> Training loss: 2.903807    Training error rate: 74.000000
650.000000/1000.000000 ==> Training loss: 2.969321    Training error rate: 72.000000
700.000000/1000.000000 ==> Training loss: 2.800308    Training error rate: 70.000000
750.000000/1000.000000 ==> Training loss: 2.433994    Training error rate: 68.000000
800.000000/1000.000000 ==> Training loss: 2.547932    Training error rate: 66.000000
850.000000/1000.000000 ==> Training loss: 2.979413    Training error rate: 68.000000
900.000000/1000.000000 ==> Training loss: 2.698562    Training error rate: 64.000000
950.000000/1000.000000 ==> Training loss: 2.946457    Training error rate: 70.000000
==> Total training loss: 2873.056162    Total training error rate: 73.292000
==> Testing Epoch: 3
0.000000/100.000000 ==> Testing loss: 2.909447    Testing error rate: 70.000000
50.000000/100.000000 ==> Testing loss: 2.580161    Testing error rate: 69.000000
==> Total testing loss: 278.947637    Total testing error rate: 70.880000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 4
0.000000/1000.000000 ==> Training loss: 2.840415    Training error rate: 72.000000
50.000000/1000.000000 ==> Training loss: 2.582368    Training error rate: 70.000000
100.000000/1000.000000 ==> Training loss: 2.590998    Training error rate: 66.000000
150.000000/1000.000000 ==> Training loss: 2.331321    Training error rate: 66.000000
200.000000/1000.000000 ==> Training loss: 2.334332    Training error rate: 64.000000
250.000000/1000.000000 ==> Training loss: 2.528769    Training error rate: 68.000000
300.000000/1000.000000 ==> Training loss: 2.625975    Training error rate: 68.000000
350.000000/1000.000000 ==> Training loss: 2.551847    Training error rate: 64.000000
400.000000/1000.000000 ==> Training loss: 2.384376    Training error rate: 62.000000
450.000000/1000.000000 ==> Training loss: 2.799548    Training error rate: 76.000000
500.000000/1000.000000 ==> Training loss: 2.656728    Training error rate: 84.000000
550.000000/1000.000000 ==> Training loss: 2.758385    Training error rate: 68.000000
600.000000/1000.000000 ==> Training loss: 2.019773    Training error rate: 50.000000
650.000000/1000.000000 ==> Training loss: 2.257355    Training error rate: 64.000000
700.000000/1000.000000 ==> Training loss: 2.132658    Training error rate: 52.000000
750.000000/1000.000000 ==> Training loss: 2.271585    Training error rate: 52.000000
800.000000/1000.000000 ==> Training loss: 2.052769    Training error rate: 60.000000
850.000000/1000.000000 ==> Training loss: 2.125876    Training error rate: 54.000000
900.000000/1000.000000 ==> Training loss: 2.733238    Training error rate: 72.000000
950.000000/1000.000000 ==> Training loss: 2.218821    Training error rate: 58.000000
==> Total training loss: 2457.501722    Total training error rate: 64.844000
==> Testing Epoch: 4
0.000000/100.000000 ==> Testing loss: 2.381531    Testing error rate: 62.000000
50.000000/100.000000 ==> Testing loss: 2.129648    Testing error rate: 52.000000
==> Total testing loss: 228.611561    Total testing error rate: 60.640000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 5
0.000000/1000.000000 ==> Training loss: 2.190807    Training error rate: 58.000000
50.000000/1000.000000 ==> Training loss: 1.896788    Training error rate: 56.000000
100.000000/1000.000000 ==> Training loss: 2.148231    Training error rate: 58.000000
150.000000/1000.000000 ==> Training loss: 2.085775    Training error rate: 62.000000
200.000000/1000.000000 ==> Training loss: 2.178115    Training error rate: 58.000000
250.000000/1000.000000 ==> Training loss: 2.192413    Training error rate: 60.000000
300.000000/1000.000000 ==> Training loss: 2.155362    Training error rate: 54.000000
350.000000/1000.000000 ==> Training loss: 2.115980    Training error rate: 56.000000
400.000000/1000.000000 ==> Training loss: 2.430694    Training error rate: 66.000000
450.000000/1000.000000 ==> Training loss: 2.306495    Training error rate: 64.000000
500.000000/1000.000000 ==> Training loss: 2.201328    Training error rate: 62.000000
550.000000/1000.000000 ==> Training loss: 1.843375    Training error rate: 52.000000
600.000000/1000.000000 ==> Training loss: 2.112009    Training error rate: 58.000000
650.000000/1000.000000 ==> Training loss: 1.933476    Training error rate: 60.000000
700.000000/1000.000000 ==> Training loss: 2.056706    Training error rate: 44.000000
750.000000/1000.000000 ==> Training loss: 1.873052    Training error rate: 58.000000
800.000000/1000.000000 ==> Training loss: 2.080417    Training error rate: 64.000000
850.000000/1000.000000 ==> Training loss: 2.575918    Training error rate: 72.000000
900.000000/1000.000000 ==> Training loss: 2.106843    Training error rate: 58.000000
950.000000/1000.000000 ==> Training loss: 1.903697    Training error rate: 56.000000
==> Total training loss: 2151.165540    Total training error rate: 58.320000
==> Testing Epoch: 5
0.000000/100.000000 ==> Testing loss: 2.217825    Testing error rate: 56.000000
50.000000/100.000000 ==> Testing loss: 2.022952    Testing error rate: 55.000000
==> Total testing loss: 209.899406    Total testing error rate: 55.800000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 6
0.000000/1000.000000 ==> Training loss: 2.059193    Training error rate: 54.000000
50.000000/1000.000000 ==> Training loss: 2.071478    Training error rate: 54.000000
100.000000/1000.000000 ==> Training loss: 2.033415    Training error rate: 60.000000
150.000000/1000.000000 ==> Training loss: 1.547388    Training error rate: 44.000000
200.000000/1000.000000 ==> Training loss: 1.880664    Training error rate: 46.000000
250.000000/1000.000000 ==> Training loss: 2.022408    Training error rate: 50.000000
300.000000/1000.000000 ==> Training loss: 2.078007    Training error rate: 56.000000
350.000000/1000.000000 ==> Training loss: 1.965082    Training error rate: 54.000000
400.000000/1000.000000 ==> Training loss: 2.158001    Training error rate: 58.000000
450.000000/1000.000000 ==> Training loss: 1.924524    Training error rate: 44.000000
500.000000/1000.000000 ==> Training loss: 1.738860    Training error rate: 52.000000
550.000000/1000.000000 ==> Training loss: 1.860035    Training error rate: 54.000000
600.000000/1000.000000 ==> Training loss: 2.002609    Training error rate: 54.000000
650.000000/1000.000000 ==> Training loss: 1.760525    Training error rate: 50.000000
700.000000/1000.000000 ==> Training loss: 1.627587    Training error rate: 56.000000
750.000000/1000.000000 ==> Training loss: 2.063627    Training error rate: 60.000000
800.000000/1000.000000 ==> Training loss: 1.956282    Training error rate: 60.000000
850.000000/1000.000000 ==> Training loss: 1.778941    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.968885    Training error rate: 62.000000
950.000000/1000.000000 ==> Training loss: 1.872229    Training error rate: 58.000000
==> Total training loss: 1933.528420    Total training error rate: 53.426000
==> Testing Epoch: 6
0.000000/100.000000 ==> Testing loss: 1.913968    Testing error rate: 47.000000
50.000000/100.000000 ==> Testing loss: 1.744333    Testing error rate: 45.000000
==> Total testing loss: 188.599868    Total testing error rate: 50.630000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 7
0.000000/1000.000000 ==> Training loss: 1.655617    Training error rate: 46.000000
50.000000/1000.000000 ==> Training loss: 2.058225    Training error rate: 58.000000
100.000000/1000.000000 ==> Training loss: 1.641984    Training error rate: 44.000000
150.000000/1000.000000 ==> Training loss: 1.519437    Training error rate: 48.000000
200.000000/1000.000000 ==> Training loss: 1.335865    Training error rate: 36.000000
250.000000/1000.000000 ==> Training loss: 1.371627    Training error rate: 44.000000
300.000000/1000.000000 ==> Training loss: 1.876528    Training error rate: 54.000000
350.000000/1000.000000 ==> Training loss: 1.786777    Training error rate: 46.000000
400.000000/1000.000000 ==> Training loss: 1.516261    Training error rate: 48.000000
450.000000/1000.000000 ==> Training loss: 1.786289    Training error rate: 50.000000
500.000000/1000.000000 ==> Training loss: 1.638345    Training error rate: 40.000000
550.000000/1000.000000 ==> Training loss: 1.813026    Training error rate: 48.000000
600.000000/1000.000000 ==> Training loss: 1.816995    Training error rate: 62.000000
650.000000/1000.000000 ==> Training loss: 1.644472    Training error rate: 54.000000
700.000000/1000.000000 ==> Training loss: 1.601784    Training error rate: 42.000000
750.000000/1000.000000 ==> Training loss: 1.614946    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.718328    Training error rate: 54.000000
850.000000/1000.000000 ==> Training loss: 1.503470    Training error rate: 42.000000
900.000000/1000.000000 ==> Training loss: 1.878428    Training error rate: 54.000000
950.000000/1000.000000 ==> Training loss: 1.960265    Training error rate: 50.000000
==> Total training loss: 1747.794708    Total training error rate: 48.896000
==> Testing Epoch: 7
0.000000/100.000000 ==> Testing loss: 1.839888    Testing error rate: 52.000000
50.000000/100.000000 ==> Testing loss: 1.753475    Testing error rate: 45.000000
==> Total testing loss: 182.689208    Total testing error rate: 49.470000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 8
0.000000/1000.000000 ==> Training loss: 2.101798    Training error rate: 50.000000
50.000000/1000.000000 ==> Training loss: 1.694197    Training error rate: 52.000000
100.000000/1000.000000 ==> Training loss: 1.381680    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 1.221705    Training error rate: 40.000000
200.000000/1000.000000 ==> Training loss: 1.771448    Training error rate: 56.000000
250.000000/1000.000000 ==> Training loss: 1.331762    Training error rate: 44.000000
300.000000/1000.000000 ==> Training loss: 1.735250    Training error rate: 44.000000
350.000000/1000.000000 ==> Training loss: 1.564412    Training error rate: 48.000000
400.000000/1000.000000 ==> Training loss: 1.583546    Training error rate: 48.000000
450.000000/1000.000000 ==> Training loss: 1.592473    Training error rate: 50.000000
500.000000/1000.000000 ==> Training loss: 1.602143    Training error rate: 38.000000
550.000000/1000.000000 ==> Training loss: 1.400313    Training error rate: 38.000000
600.000000/1000.000000 ==> Training loss: 1.742413    Training error rate: 48.000000
650.000000/1000.000000 ==> Training loss: 1.781681    Training error rate: 56.000000
700.000000/1000.000000 ==> Training loss: 1.934592    Training error rate: 56.000000
750.000000/1000.000000 ==> Training loss: 1.280964    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 2.093398    Training error rate: 48.000000
850.000000/1000.000000 ==> Training loss: 1.373587    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.557337    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.478017    Training error rate: 42.000000
==> Total training loss: 1607.199916    Total training error rate: 45.666000
==> Testing Epoch: 8
0.000000/100.000000 ==> Testing loss: 1.741315    Testing error rate: 43.000000
50.000000/100.000000 ==> Testing loss: 1.704606    Testing error rate: 47.000000
==> Total testing loss: 176.907435    Total testing error rate: 47.950000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 9
0.000000/1000.000000 ==> Training loss: 1.712361    Training error rate: 48.000000
50.000000/1000.000000 ==> Training loss: 1.524778    Training error rate: 38.000000
100.000000/1000.000000 ==> Training loss: 1.393590    Training error rate: 38.000000
150.000000/1000.000000 ==> Training loss: 1.836840    Training error rate: 44.000000
200.000000/1000.000000 ==> Training loss: 1.573191    Training error rate: 44.000000
250.000000/1000.000000 ==> Training loss: 1.445380    Training error rate: 40.000000
300.000000/1000.000000 ==> Training loss: 1.783751    Training error rate: 52.000000
350.000000/1000.000000 ==> Training loss: 1.687281    Training error rate: 52.000000
400.000000/1000.000000 ==> Training loss: 1.481537    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.447264    Training error rate: 38.000000
500.000000/1000.000000 ==> Training loss: 1.425301    Training error rate: 42.000000
550.000000/1000.000000 ==> Training loss: 1.473886    Training error rate: 44.000000
600.000000/1000.000000 ==> Training loss: 1.815346    Training error rate: 46.000000
650.000000/1000.000000 ==> Training loss: 1.521102    Training error rate: 38.000000
700.000000/1000.000000 ==> Training loss: 1.798823    Training error rate: 46.000000
750.000000/1000.000000 ==> Training loss: 1.337845    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.619673    Training error rate: 42.000000
850.000000/1000.000000 ==> Training loss: 1.812687    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.627762    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.633768    Training error rate: 54.000000
==> Total training loss: 1496.333617    Total training error rate: 42.558000
==> Testing Epoch: 9
0.000000/100.000000 ==> Testing loss: 1.805897    Testing error rate: 46.000000
50.000000/100.000000 ==> Testing loss: 1.769310    Testing error rate: 45.000000
==> Total testing loss: 181.217760    Total testing error rate: 48.140000
==> Set learning rate: 0.010000
==> Training Epoch: 10
0.000000/1000.000000 ==> Training loss: 1.308221    Training error rate: 42.000000
50.000000/1000.000000 ==> Training loss: 1.410322    Training error rate: 42.000000
100.000000/1000.000000 ==> Training loss: 1.523581    Training error rate: 40.000000
150.000000/1000.000000 ==> Training loss: 1.351217    Training error rate: 46.000000
200.000000/1000.000000 ==> Training loss: 1.427944    Training error rate: 42.000000
250.000000/1000.000000 ==> Training loss: 1.510982    Training error rate: 46.000000
300.000000/1000.000000 ==> Training loss: 1.980562    Training error rate: 48.000000
350.000000/1000.000000 ==> Training loss: 1.344460    Training error rate: 34.000000
400.000000/1000.000000 ==> Training loss: 1.590798    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.264358    Training error rate: 42.000000
500.000000/1000.000000 ==> Training loss: 1.392556    Training error rate: 38.000000
550.000000/1000.000000 ==> Training loss: 1.337679    Training error rate: 44.000000
600.000000/1000.000000 ==> Training loss: 1.452666    Training error rate: 44.000000
650.000000/1000.000000 ==> Training loss: 1.705950    Training error rate: 48.000000
700.000000/1000.000000 ==> Training loss: 1.303423    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 1.479489    Training error rate: 44.000000
800.000000/1000.000000 ==> Training loss: 1.039966    Training error rate: 28.000000
850.000000/1000.000000 ==> Training loss: 1.525991    Training error rate: 44.000000
900.000000/1000.000000 ==> Training loss: 1.504688    Training error rate: 48.000000
950.000000/1000.000000 ==> Training loss: 1.546789    Training error rate: 38.000000
==> Total training loss: 1395.739790    Total training error rate: 40.480000
==> Testing Epoch: 10
0.000000/100.000000 ==> Testing loss: 1.877012    Testing error rate: 49.000000
50.000000/100.000000 ==> Testing loss: 1.507011    Testing error rate: 44.000000
==> Total testing loss: 173.279470    Total testing error rate: 45.880000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 11
0.000000/1000.000000 ==> Training loss: 1.311020    Training error rate: 36.000000
50.000000/1000.000000 ==> Training loss: 1.559515    Training error rate: 48.000000
100.000000/1000.000000 ==> Training loss: 1.627589    Training error rate: 48.000000
150.000000/1000.000000 ==> Training loss: 1.203147    Training error rate: 36.000000
200.000000/1000.000000 ==> Training loss: 1.323186    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 1.649683    Training error rate: 52.000000
300.000000/1000.000000 ==> Training loss: 1.096161    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.966470    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 1.298168    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.596766    Training error rate: 44.000000
500.000000/1000.000000 ==> Training loss: 1.203436    Training error rate: 28.000000
550.000000/1000.000000 ==> Training loss: 1.087285    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 1.256607    Training error rate: 40.000000
650.000000/1000.000000 ==> Training loss: 1.551264    Training error rate: 50.000000
700.000000/1000.000000 ==> Training loss: 1.185382    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 1.462989    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.505102    Training error rate: 48.000000
850.000000/1000.000000 ==> Training loss: 1.326134    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 1.340112    Training error rate: 46.000000
950.000000/1000.000000 ==> Training loss: 1.878640    Training error rate: 60.000000
==> Total training loss: 1301.646671    Total training error rate: 37.654000
==> Testing Epoch: 11
0.000000/100.000000 ==> Testing loss: 1.623534    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.388697    Testing error rate: 37.000000
==> Total testing loss: 160.190827    Total testing error rate: 43.100000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 12
0.000000/1000.000000 ==> Training loss: 1.038240    Training error rate: 30.000000
50.000000/1000.000000 ==> Training loss: 0.933917    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 1.024173    Training error rate: 30.000000
150.000000/1000.000000 ==> Training loss: 1.391266    Training error rate: 38.000000
200.000000/1000.000000 ==> Training loss: 1.417287    Training error rate: 36.000000
250.000000/1000.000000 ==> Training loss: 1.024571    Training error rate: 34.000000
300.000000/1000.000000 ==> Training loss: 1.357576    Training error rate: 44.000000
350.000000/1000.000000 ==> Training loss: 0.931764    Training error rate: 32.000000
400.000000/1000.000000 ==> Training loss: 1.281607    Training error rate: 42.000000
450.000000/1000.000000 ==> Training loss: 1.186679    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 1.235408    Training error rate: 32.000000
550.000000/1000.000000 ==> Training loss: 1.012309    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 1.035232    Training error rate: 26.000000
650.000000/1000.000000 ==> Training loss: 1.675502    Training error rate: 48.000000
700.000000/1000.000000 ==> Training loss: 1.349720    Training error rate: 40.000000
750.000000/1000.000000 ==> Training loss: 0.894750    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 1.016231    Training error rate: 32.000000
850.000000/1000.000000 ==> Training loss: 1.287959    Training error rate: 38.000000
900.000000/1000.000000 ==> Training loss: 1.356178    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.030539    Training error rate: 34.000000
==> Total training loss: 1227.321164    Total training error rate: 35.586000
==> Testing Epoch: 12
0.000000/100.000000 ==> Testing loss: 1.326007    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.435211    Testing error rate: 40.000000
==> Total testing loss: 160.022756    Total testing error rate: 43.350000
==> Set learning rate: 0.010000
==> Training Epoch: 13
0.000000/1000.000000 ==> Training loss: 1.018544    Training error rate: 32.000000
50.000000/1000.000000 ==> Training loss: 1.258538    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 1.665256    Training error rate: 46.000000
150.000000/1000.000000 ==> Training loss: 1.054802    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.118211    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 1.119600    Training error rate: 34.000000
300.000000/1000.000000 ==> Training loss: 1.448340    Training error rate: 42.000000
350.000000/1000.000000 ==> Training loss: 1.036237    Training error rate: 40.000000
400.000000/1000.000000 ==> Training loss: 1.305989    Training error rate: 44.000000
450.000000/1000.000000 ==> Training loss: 1.126205    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 1.507743    Training error rate: 36.000000
550.000000/1000.000000 ==> Training loss: 1.176796    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 1.171439    Training error rate: 32.000000
650.000000/1000.000000 ==> Training loss: 1.072597    Training error rate: 42.000000
700.000000/1000.000000 ==> Training loss: 1.142354    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 1.275478    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.319710    Training error rate: 40.000000
850.000000/1000.000000 ==> Training loss: 1.223216    Training error rate: 40.000000
900.000000/1000.000000 ==> Training loss: 0.938066    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.087273    Training error rate: 40.000000
==> Total training loss: 1159.671479    Total training error rate: 34.186000
==> Testing Epoch: 13
0.000000/100.000000 ==> Testing loss: 1.461160    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.298147    Testing error rate: 35.000000
==> Total testing loss: 145.261071    Total testing error rate: 39.920000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 14
0.000000/1000.000000 ==> Training loss: 1.118261    Training error rate: 40.000000
50.000000/1000.000000 ==> Training loss: 1.124757    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 1.239783    Training error rate: 32.000000
150.000000/1000.000000 ==> Training loss: 1.187918    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 1.061010    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 0.839640    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 1.011726    Training error rate: 38.000000
350.000000/1000.000000 ==> Training loss: 1.097311    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 1.190576    Training error rate: 30.000000
450.000000/1000.000000 ==> Training loss: 0.762178    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.708490    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 1.111733    Training error rate: 34.000000
600.000000/1000.000000 ==> Training loss: 1.046133    Training error rate: 36.000000
650.000000/1000.000000 ==> Training loss: 1.255941    Training error rate: 40.000000
700.000000/1000.000000 ==> Training loss: 0.938072    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 1.017438    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 1.218119    Training error rate: 34.000000
850.000000/1000.000000 ==> Training loss: 0.997944    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 1.173154    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.205728    Training error rate: 38.000000
==> Total training loss: 1099.191433    Total training error rate: 32.438000
==> Testing Epoch: 14
0.000000/100.000000 ==> Testing loss: 1.375886    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.271874    Testing error rate: 37.000000
==> Total testing loss: 139.792277    Total testing error rate: 39.170000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 15
0.000000/1000.000000 ==> Training loss: 1.221613    Training error rate: 38.000000
50.000000/1000.000000 ==> Training loss: 0.560222    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.772762    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.710524    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.721321    Training error rate: 28.000000
250.000000/1000.000000 ==> Training loss: 1.056720    Training error rate: 30.000000
300.000000/1000.000000 ==> Training loss: 0.965970    Training error rate: 30.000000
350.000000/1000.000000 ==> Training loss: 1.331462    Training error rate: 36.000000
400.000000/1000.000000 ==> Training loss: 1.356863    Training error rate: 38.000000
450.000000/1000.000000 ==> Training loss: 1.071047    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 1.132167    Training error rate: 34.000000
550.000000/1000.000000 ==> Training loss: 1.021435    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 1.139271    Training error rate: 36.000000
650.000000/1000.000000 ==> Training loss: 0.996715    Training error rate: 32.000000
700.000000/1000.000000 ==> Training loss: 1.033458    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.971920    Training error rate: 34.000000
800.000000/1000.000000 ==> Training loss: 1.060618    Training error rate: 38.000000
850.000000/1000.000000 ==> Training loss: 1.182297    Training error rate: 40.000000
900.000000/1000.000000 ==> Training loss: 1.509563    Training error rate: 52.000000
950.000000/1000.000000 ==> Training loss: 0.985599    Training error rate: 30.000000
==> Total training loss: 1047.713929    Total training error rate: 31.170000
==> Testing Epoch: 15
0.000000/100.000000 ==> Testing loss: 1.496948    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.263095    Testing error rate: 36.000000
==> Total testing loss: 144.440971    Total testing error rate: 40.150000
==> Set learning rate: 0.010000
==> Training Epoch: 16
0.000000/1000.000000 ==> Training loss: 1.254057    Training error rate: 34.000000
50.000000/1000.000000 ==> Training loss: 0.903395    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.769630    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.850839    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.040989    Training error rate: 28.000000
250.000000/1000.000000 ==> Training loss: 0.819836    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.714233    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.939802    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.864940    Training error rate: 26.000000
450.000000/1000.000000 ==> Training loss: 1.144696    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.938208    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 1.097088    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 1.327735    Training error rate: 38.000000
650.000000/1000.000000 ==> Training loss: 1.081001    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 1.002182    Training error rate: 24.000000
750.000000/1000.000000 ==> Training loss: 0.763017    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 1.112584    Training error rate: 36.000000
850.000000/1000.000000 ==> Training loss: 1.136391    Training error rate: 36.000000
900.000000/1000.000000 ==> Training loss: 1.290845    Training error rate: 34.000000
950.000000/1000.000000 ==> Training loss: 0.902988    Training error rate: 26.000000
==> Total training loss: 990.005387    Total training error rate: 29.318000
==> Testing Epoch: 16
0.000000/100.000000 ==> Testing loss: 1.367854    Testing error rate: 40.000000
50.000000/100.000000 ==> Testing loss: 1.418253    Testing error rate: 33.000000
==> Total testing loss: 140.218684    Total testing error rate: 38.570000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 17
0.000000/1000.000000 ==> Training loss: 1.055174    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.886549    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 1.048889    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.783463    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.717315    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.899912    Training error rate: 28.000000
300.000000/1000.000000 ==> Training loss: 1.212397    Training error rate: 40.000000
350.000000/1000.000000 ==> Training loss: 0.936780    Training error rate: 24.000000
400.000000/1000.000000 ==> Training loss: 1.002606    Training error rate: 30.000000
450.000000/1000.000000 ==> Training loss: 0.864311    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 0.774292    Training error rate: 18.000000
550.000000/1000.000000 ==> Training loss: 1.071334    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.527526    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.781021    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 1.149153    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.701995    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 1.130402    Training error rate: 40.000000
850.000000/1000.000000 ==> Training loss: 1.261313    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 0.936168    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.922013    Training error rate: 30.000000
==> Total training loss: 948.588511    Total training error rate: 28.398000
==> Testing Epoch: 17
0.000000/100.000000 ==> Testing loss: 1.285158    Testing error rate: 41.000000
50.000000/100.000000 ==> Testing loss: 1.460227    Testing error rate: 37.000000
==> Total testing loss: 134.745520    Total testing error rate: 37.230000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 18
0.000000/1000.000000 ==> Training loss: 0.926093    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.796084    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.979270    Training error rate: 28.000000
150.000000/1000.000000 ==> Training loss: 1.031440    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 0.853587    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 0.909536    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.769329    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.512024    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 1.016944    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.944859    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.733042    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.744720    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 1.094880    Training error rate: 28.000000
650.000000/1000.000000 ==> Training loss: 1.038406    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 1.081145    Training error rate: 28.000000
750.000000/1000.000000 ==> Training loss: 0.731375    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 1.187885    Training error rate: 36.000000
850.000000/1000.000000 ==> Training loss: 0.683541    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.874115    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.047111    Training error rate: 38.000000
==> Total training loss: 904.076424    Total training error rate: 27.182000
==> Testing Epoch: 18
0.000000/100.000000 ==> Testing loss: 1.441281    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.389001    Testing error rate: 38.000000
==> Total testing loss: 137.738122    Total testing error rate: 37.410000
==> Set learning rate: 0.010000
==> Training Epoch: 19
0.000000/1000.000000 ==> Training loss: 0.773415    Training error rate: 28.000000
50.000000/1000.000000 ==> Training loss: 0.827497    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 0.632235    Training error rate: 24.000000
150.000000/1000.000000 ==> Training loss: 0.971510    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 0.541616    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.711499    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 1.100118    Training error rate: 46.000000
350.000000/1000.000000 ==> Training loss: 0.869232    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 0.708827    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.699850    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 1.015908    Training error rate: 28.000000
550.000000/1000.000000 ==> Training loss: 0.772177    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.835308    Training error rate: 22.000000
650.000000/1000.000000 ==> Training loss: 1.300890    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 0.876947    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 0.829903    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 0.835801    Training error rate: 32.000000
850.000000/1000.000000 ==> Training loss: 1.057488    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 0.781430    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.949653    Training error rate: 24.000000
==> Total training loss: 876.082728    Total training error rate: 26.468000
==> Testing Epoch: 19
0.000000/100.000000 ==> Testing loss: 1.321077    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.412123    Testing error rate: 34.000000
==> Total testing loss: 136.702347    Total testing error rate: 37.090000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 20
0.000000/1000.000000 ==> Training loss: 0.767655    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.823184    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 1.076606    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.674958    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.990246    Training error rate: 30.000000
250.000000/1000.000000 ==> Training loss: 0.617291    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.708325    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.889147    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.828724    Training error rate: 28.000000
450.000000/1000.000000 ==> Training loss: 0.751438    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.649178    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.726906    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.753827    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.808747    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 1.222787    Training error rate: 38.000000
750.000000/1000.000000 ==> Training loss: 0.877584    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.855438    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.967661    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.755784    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.551310    Training error rate: 8.000000
==> Total training loss: 833.038798    Total training error rate: 25.192000
==> Testing Epoch: 20
0.000000/100.000000 ==> Testing loss: 1.455191    Testing error rate: 41.000000
50.000000/100.000000 ==> Testing loss: 1.462273    Testing error rate: 39.000000
==> Total testing loss: 140.230237    Total testing error rate: 37.290000
==> Set learning rate: 0.010000
==> Training Epoch: 21
0.000000/1000.000000 ==> Training loss: 0.811763    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.613706    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.926985    Training error rate: 30.000000
150.000000/1000.000000 ==> Training loss: 0.687967    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.678507    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.755387    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.803285    Training error rate: 32.000000
350.000000/1000.000000 ==> Training loss: 0.670693    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 1.011418    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.591748    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.780168    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 0.664683    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 1.012379    Training error rate: 30.000000
650.000000/1000.000000 ==> Training loss: 0.767051    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.710810    Training error rate: 24.000000
750.000000/1000.000000 ==> Training loss: 0.566991    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.751992    Training error rate: 20.000000
850.000000/1000.000000 ==> Training loss: 0.836663    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 1.178255    Training error rate: 30.000000
950.000000/1000.000000 ==> Training loss: 0.693405    Training error rate: 24.000000
==> Total training loss: 805.103362    Total training error rate: 24.656000
==> Testing Epoch: 21
0.000000/100.000000 ==> Testing loss: 1.480474    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.286268    Testing error rate: 31.000000
==> Total testing loss: 138.010314    Total testing error rate: 36.940000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 22
0.000000/1000.000000 ==> Training loss: 0.385826    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.302237    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.778906    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.641563    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.672638    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.530859    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.903176    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.642352    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.865655    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.853225    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.812954    Training error rate: 32.000000
550.000000/1000.000000 ==> Training loss: 1.030781    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.781286    Training error rate: 24.000000
650.000000/1000.000000 ==> Training loss: 0.531625    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.764217    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.714585    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 1.226999    Training error rate: 42.000000
850.000000/1000.000000 ==> Training loss: 0.694048    Training error rate: 26.000000
900.000000/1000.000000 ==> Training loss: 0.824467    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.659504    Training error rate: 18.000000
==> Total training loss: 773.233685    Total training error rate: 23.638000
==> Testing Epoch: 22
0.000000/100.000000 ==> Testing loss: 1.505965    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.510814    Testing error rate: 33.000000
==> Total testing loss: 135.249002    Total testing error rate: 35.820000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 23
0.000000/1000.000000 ==> Training loss: 0.941841    Training error rate: 28.000000
50.000000/1000.000000 ==> Training loss: 0.799043    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.523504    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.562729    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.922137    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.944953    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.811191    Training error rate: 20.000000
350.000000/1000.000000 ==> Training loss: 0.694753    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.621766    Training error rate: 22.000000
450.000000/1000.000000 ==> Training loss: 0.710219    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.408593    Training error rate: 12.000000
550.000000/1000.000000 ==> Training loss: 0.609686    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.504516    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.899537    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 0.951560    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.618123    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.662444    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.966857    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.793802    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 1.061447    Training error rate: 30.000000
==> Total training loss: 747.542327    Total training error rate: 22.684000
==> Testing Epoch: 23
0.000000/100.000000 ==> Testing loss: 1.350777    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.434296    Testing error rate: 32.000000
==> Total testing loss: 131.407366    Total testing error rate: 34.600000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 24
0.000000/1000.000000 ==> Training loss: 0.604323    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.856048    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.563504    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.588071    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.483137    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.747936    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.987395    Training error rate: 32.000000
350.000000/1000.000000 ==> Training loss: 0.644239    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.584988    Training error rate: 22.000000
450.000000/1000.000000 ==> Training loss: 0.668496    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.608880    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.682369    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.630751    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.753236    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.949480    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.907636    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.943212    Training error rate: 30.000000
850.000000/1000.000000 ==> Training loss: 1.020969    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.845161    Training error rate: 28.000000
950.000000/1000.000000 ==> Training loss: 0.587399    Training error rate: 18.000000
==> Total training loss: 727.030024    Total training error rate: 22.458000
==> Testing Epoch: 24
0.000000/100.000000 ==> Testing loss: 1.381206    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.042953    Testing error rate: 30.000000
==> Total testing loss: 130.848066    Total testing error rate: 34.620000
==> Set learning rate: 0.010000
==> Training Epoch: 25
0.000000/1000.000000 ==> Training loss: 0.655942    Training error rate: 32.000000
50.000000/1000.000000 ==> Training loss: 0.641364    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.539858    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.518427    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.552606    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.680367    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.731961    Training error rate: 22.000000
350.000000/1000.000000 ==> Training loss: 0.944749    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.571494    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.505482    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.504386    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.896310    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.705431    Training error rate: 26.000000
650.000000/1000.000000 ==> Training loss: 0.792656    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.789050    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 0.597255    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.866821    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.579395    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.794953    Training error rate: 28.000000
950.000000/1000.000000 ==> Training loss: 0.731680    Training error rate: 24.000000
==> Total training loss: 698.406479    Total training error rate: 21.556000
==> Testing Epoch: 25
0.000000/100.000000 ==> Testing loss: 1.321187    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.090315    Testing error rate: 29.000000
==> Total testing loss: 127.379310    Total testing error rate: 34.670000
==> Set learning rate: 0.010000
==> Training Epoch: 26
0.000000/1000.000000 ==> Training loss: 0.697295    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.644665    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.567076    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.405677    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.496837    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.559085    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.541182    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.547433    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.728428    Training error rate: 20.000000
450.000000/1000.000000 ==> Training loss: 0.808685    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.722217    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.726590    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.594902    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.914522    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 0.881229    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.794620    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.932929    Training error rate: 34.000000
850.000000/1000.000000 ==> Training loss: 0.725854    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.634211    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.810605    Training error rate: 30.000000
==> Total training loss: 676.413966    Total training error rate: 20.674000
==> Testing Epoch: 26
0.000000/100.000000 ==> Testing loss: 1.136712    Testing error rate: 30.000000
50.000000/100.000000 ==> Testing loss: 1.198634    Testing error rate: 29.000000
==> Total testing loss: 127.802508    Total testing error rate: 33.790000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 27
0.000000/1000.000000 ==> Training loss: 0.637743    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.551723    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 0.610010    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.607348    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.570131    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.583517    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.595628    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.651267    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.467685    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.707869    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.679716    Training error rate: 26.000000
550.000000/1000.000000 ==> Training loss: 0.610795    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.791844    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.653148    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.767559    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.812972    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.642997    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.416368    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.686387    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.812223    Training error rate: 24.000000
==> Total training loss: 650.128888    Total training error rate: 20.248000
==> Testing Epoch: 27
0.000000/100.000000 ==> Testing loss: 1.193094    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.400613    Testing error rate: 34.000000
==> Total testing loss: 135.267944    Total testing error rate: 35.520000
==> Set learning rate: 0.010000
==> Training Epoch: 28
0.000000/1000.000000 ==> Training loss: 0.640792    Training error rate: 20.000000
50.000000/1000.000000 ==> Training loss: 0.577937    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.480967    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.599682    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.601168    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.877462    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 0.402524    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.676769    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.633084    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.767712    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.781782    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.754575    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.848044    Training error rate: 28.000000
650.000000/1000.000000 ==> Training loss: 0.378646    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.576601    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.640841    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.409048    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.735445    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.834494    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.691417    Training error rate: 22.000000
==> Total training loss: 641.710395    Total training error rate: 19.792000
==> Testing Epoch: 28
0.000000/100.000000 ==> Testing loss: 1.379735    Testing error rate: 33.000000
50.000000/100.000000 ==> Testing loss: 1.126057    Testing error rate: 28.000000
==> Total testing loss: 129.511713    Total testing error rate: 34.020000
==> Set learning rate: 0.010000
==> Training Epoch: 29
0.000000/1000.000000 ==> Training loss: 0.575846    Training error rate: 20.000000
50.000000/1000.000000 ==> Training loss: 0.522687    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.612530    Training error rate: 20.000000
150.000000/1000.000000 ==> Training loss: 0.484939    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.615149    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.590311    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.375446    Training error rate: 8.000000
350.000000/1000.000000 ==> Training loss: 0.530687    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.564189    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.462727    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.557025    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.858578    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.750054    Training error rate: 22.000000
650.000000/1000.000000 ==> Training loss: 0.747648    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.704276    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.467301    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.748628    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.761602    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.534158    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.657249    Training error rate: 24.000000
==> Total training loss: 616.169580    Total training error rate: 19.128000
==> Testing Epoch: 29
0.000000/100.000000 ==> Testing loss: 1.219901    Testing error rate: 31.000000
50.000000/100.000000 ==> Testing loss: 1.380697    Testing error rate: 40.000000
==> Total testing loss: 127.717714    Total testing error rate: 33.740000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 30
0.000000/1000.000000 ==> Training loss: 0.300561    Training error rate: 4.000000
50.000000/1000.000000 ==> Training loss: 0.698130    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.506481    Training error rate: 22.000000
150.000000/1000.000000 ==> Training loss: 0.419953    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.843976    Training error rate: 22.000000
250.000000/1000.000000 ==> Training loss: 0.270207    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.737075    Training error rate: 18.000000
350.000000/1000.000000 ==> Training loss: 0.652305    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.602375    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.645290    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.829702    Training error rate: 30.000000
550.000000/1000.000000 ==> Training loss: 0.546342    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.506395    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.622368    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.788734    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.638477    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.632868    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.650862    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.548122    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.687617    Training error rate: 22.000000
==> Total training loss: 610.352457    Total training error rate: 18.886000
==> Testing Epoch: 30
0.000000/100.000000 ==> Testing loss: 1.182697    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.471372    Testing error rate: 40.000000
==> Total testing loss: 133.991861    Total testing error rate: 34.240000
==> Set learning rate: 0.010000
==> Training Epoch: 31
0.000000/1000.000000 ==> Training loss: 0.667576    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.514208    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.617630    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.480765    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.610526    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.468969    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.945008    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.581384    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.363959    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.758432    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.634695    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.553093    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.556681    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.646187    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.553672    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.533663    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.602935    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.576443    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.706883    Training error rate: 30.000000
950.000000/1000.000000 ==> Training loss: 0.795737    Training error rate: 24.000000
==> Total training loss: 592.895187    Total training error rate: 18.432000
==> Testing Epoch: 31
0.000000/100.000000 ==> Testing loss: 1.279816    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.536136    Testing error rate: 35.000000
==> Total testing loss: 135.527912    Total testing error rate: 34.780000
==> Set learning rate: 0.010000
==> Training Epoch: 32
0.000000/1000.000000 ==> Training loss: 0.615069    Training error rate: 22.000000
50.000000/1000.000000 ==> Training loss: 0.669840    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.425765    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.628283    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.446802    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.461664    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.659031    Training error rate: 18.000000
350.000000/1000.000000 ==> Training loss: 0.471534    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.554291    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.821254    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 0.500932    Training error rate: 10.000000
550.000000/1000.000000 ==> Training loss: 0.622824    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.479912    Training error rate: 18.000000
650.000000/1000.000000 ==> Training loss: 0.642036    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.644528    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.835607    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.779741    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.658465    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.549923    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.571006    Training error rate: 14.000000
==> Total training loss: 578.608277    Total training error rate: 17.968000
==> Testing Epoch: 32
0.000000/100.000000 ==> Testing loss: 1.331419    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.472029    Testing error rate: 38.000000
==> Total testing loss: 133.881347    Total testing error rate: 34.920000
==> Set learning rate: 0.010000
==> Training Epoch: 33
0.000000/1000.000000 ==> Training loss: 0.376245    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.474759    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.368760    Training error rate: 8.000000
150.000000/1000.000000 ==> Training loss: 0.609937    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.443883    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.509917    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.451320    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.758068    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.477627    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.830653    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.529440    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 0.831604    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 0.704390    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.571816    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.589993    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.776993    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 0.440612    Training error rate: 10.000000
850.000000/1000.000000 ==> Training loss: 0.709979    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.767564    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.486793    Training error rate: 12.000000
==> Total training loss: 568.747226    Total training error rate: 17.668000
==> Testing Epoch: 33
0.000000/100.000000 ==> Testing loss: 1.496711    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.248182    Testing error rate: 35.000000
==> Total testing loss: 137.825184    Total testing error rate: 34.550000
==> Set learning rate: 0.010000
==> Training Epoch: 34
0.000000/1000.000000 ==> Training loss: 0.599273    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.453589    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.559413    Training error rate: 24.000000
150.000000/1000.000000 ==> Training loss: 0.362409    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.635144    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.385949    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.546547    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.433191    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.601622    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.451471    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.572335    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.392155    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.567442    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.609029    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.528576    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.440651    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.465503    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.741505    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.808153    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.722620    Training error rate: 24.000000
==> Total training loss: 544.274256    Total training error rate: 16.796000
==> Testing Epoch: 34
0.000000/100.000000 ==> Testing loss: 1.091064    Testing error rate: 30.000000
50.000000/100.000000 ==> Testing loss: 1.441337    Testing error rate: 34.000000
==> Total testing loss: 130.364300    Total testing error rate: 33.100000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 35
0.000000/1000.000000 ==> Training loss: 0.493398    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.517093    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.353866    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.349815    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.522168    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.778699    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.500315    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.376967    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.460305    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.368589    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.364994    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.426591    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.520659    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.488706    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.514119    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.321539    Training error rate: 8.000000
800.000000/1000.000000 ==> Training loss: 0.352800    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.747712    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.543496    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.385076    Training error rate: 18.000000
==> Total training loss: 537.894123    Total training error rate: 16.742000
==> Testing Epoch: 35
0.000000/100.000000 ==> Testing loss: 1.416957    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.298468    Testing error rate: 35.000000
==> Total testing loss: 128.735879    Total testing error rate: 33.180000
==> Set learning rate: 0.010000
==> Training Epoch: 36
0.000000/1000.000000 ==> Training loss: 0.473164    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.433537    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.467526    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.443184    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.617312    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.696765    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.382543    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.443756    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.459088    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.480450    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.516917    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.404938    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.250042    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.868545    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.427695    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.760578    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.595996    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.389359    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.581635    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.332481    Training error rate: 12.000000
==> Total training loss: 530.333477    Total training error rate: 16.518000
==> Testing Epoch: 36
0.000000/100.000000 ==> Testing loss: 1.491593    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.439892    Testing error rate: 33.000000
==> Total testing loss: 132.689520    Total testing error rate: 33.590000
==> Set learning rate: 0.010000
==> Training Epoch: 37
0.000000/1000.000000 ==> Training loss: 0.522319    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.337910    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.548351    Training error rate: 20.000000
150.000000/1000.000000 ==> Training loss: 0.572943    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.291482    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.392987    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.857531    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.416996    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.345856    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.720703    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.667278    Training error rate: 18.000000
550.000000/1000.000000 ==> Training loss: 0.518329    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.513659    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.516430    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.886918    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.589640    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.722260    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.764057    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.438010    Training error rate: 10.000000
950.000000/1000.000000 ==> Training loss: 0.519963    Training error rate: 14.000000
==> Total training loss: 514.767130    Total training error rate: 16.014000
==> Testing Epoch: 37
0.000000/100.000000 ==> Testing loss: 1.362627    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.494166    Testing error rate: 37.000000
==> Total testing loss: 135.344767    Total testing error rate: 34.270000
==> Set learning rate: 0.010000
==> Training Epoch: 38
0.000000/1000.000000 ==> Training loss: 0.390156    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.210681    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.212854    Training error rate: 6.000000
150.000000/1000.000000 ==> Training loss: 0.453237    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.567344    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.397695    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.659693    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.455969    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.388112    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.499256    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.281768    Training error rate: 10.000000
550.000000/1000.000000 ==> Training loss: 0.940469    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.354611    Training error rate: 12.000000
650.000000/1000.000000 ==> Training loss: 0.586523    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.491159    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.653063    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.430160    Training error rate: 12.000000
850.000000/1000.000000 ==> Training loss: 0.585682    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.761453    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.488902    Training error rate: 14.000000
==> Total training loss: 508.658543    Total training error rate: 15.914000
==> Testing Epoch: 38
0.000000/100.000000 ==> Testing loss: 1.669469    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.403951    Testing error rate: 35.000000
==> Total testing loss: 144.760002    Total testing error rate: 35.390000
==> Set learning rate: 0.010000
==> Training Epoch: 39
0.000000/1000.000000 ==> Training loss: 0.713053    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.360587    Training error rate: 8.000000
100.000000/1000.000000 ==> Training loss: 0.333455    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.469013    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.373242    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.424465    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.788312    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.402207    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.570696    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.537472    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.705001    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.544030    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.502664    Training error rate: 18.000000
650.000000/1000.000000 ==> Training loss: 0.349189    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.606753    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.480229    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.416004    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.641803    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.490630    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.656467    Training error rate: 22.000000
==> Total training loss: 511.452728    Total training error rate: 16.054000
==> Testing Epoch: 39
0.000000/100.000000 ==> Testing loss: 1.325889    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.391128    Testing error rate: 33.000000
==> Total testing loss: 133.601643    Total testing error rate: 33.750000
==> Set learning rate: 0.010000
==> Training Epoch: 40
0.000000/1000.000000 ==> Training loss: 0.716802    Training error rate: 30.000000
50.000000/1000.000000 ==> Training loss: 0.375767    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.348400    Training error rate: 10.000000
150.000000/1000.000000 ==> Training loss: 0.386557    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.490948    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.408495    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.239014    Training error rate: 6.000000
350.000000/1000.000000 ==> Training loss: 0.360666    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.521634    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.311205    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.692995    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.503899    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.571210    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.730871    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.454530    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.437251    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.784645    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.749598    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.507238    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.742778    Training error rate: 22.000000
==> Total training loss: 490.604032    Total training error rate: 15.276000
==> Testing Epoch: 40
0.000000/100.000000 ==> Testing loss: 1.388763    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.460940    Testing error rate: 34.000000
==> Total testing loss: 133.309104    Total testing error rate: 33.510000
==> Set learning rate: 0.010000
==> Training Epoch: 41
0.000000/1000.000000 ==> Training loss: 0.400794    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.552497    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.690024    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.691139    Training error rate: 26.000000
200.000000/1000.000000 ==> Training loss: 0.403494    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.454619    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.611817    Training error rate: 22.000000
350.000000/1000.000000 ==> Training loss: 0.284257    Training error rate: 8.000000
400.000000/1000.000000 ==> Training loss: 0.304677    Training error rate: 8.000000
450.000000/1000.000000 ==> Training loss: 0.488424    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.425047    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.485073    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.492987    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.392459    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.420543    Training error rate: 20.000000
750.000000/1000.000000 ==> Training loss: 0.349525    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.531872    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.396246    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.469083    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.736115    Training error rate: 22.000000
==> Total training loss: 482.457945    Total training error rate: 14.984000
==> Testing Epoch: 41
0.000000/100.000000 ==> Testing loss: 1.286527    Testing error rate: 31.000000
50.000000/100.000000 ==> Testing loss: 1.535186    Testing error rate: 41.000000
==> Total testing loss: 139.527525    Total testing error rate: 33.960000
==> Set learning rate: 0.010000
==> Training Epoch: 42
0.000000/1000.000000 ==> Training loss: 0.588413    Training error rate: 22.000000
50.000000/1000.000000 ==> Training loss: 0.583433    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.254410    Training error rate: 8.000000
150.000000/1000.000000 ==> Training loss: 0.348387    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.198521    Training error rate: 4.000000
250.000000/1000.000000 ==> Training loss: 0.424969    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.421748    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.449636    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.564013    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.470953    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.439111    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.566951    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.869370    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.212606    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.795025    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.538101    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.508715    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.432299    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.575001    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.418013    Training error rate: 14.000000
==> Total training loss: 485.551058    Total training error rate: 15.158000
==> Testing Epoch: 42
0.000000/100.000000 ==> Testing loss: 1.276446    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.411205    Testing error rate: 36.000000
==> Total testing loss: 133.642553    Total testing error rate: 33.830000
==> Set learning rate: 0.010000
==> Training Epoch: 43
0.000000/1000.000000 ==> Training loss: 0.268995    Training error rate: 6.000000
50.000000/1000.000000 ==> Training loss: 0.483741    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.404081    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.323366    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.669584    Training error rate: 22.000000
250.000000/1000.000000 ==> Training loss: 0.517139    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.528442    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.401222    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.408127    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.422027    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.469808    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.435880    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.676620    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.382778    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.430895    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.272249    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.807660    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.329221    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.588858    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.484126    Training error rate: 12.000000
==> Total training loss: 461.249712    Total training error rate: 14.458000
==> Testing Epoch: 43
0.000000/100.000000 ==> Testing loss: 1.401996    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.405297    Testing error rate: 31.000000
==> Total testing loss: 142.204903    Total testing error rate: 34.570000
==> Set learning rate: 0.010000
==> Training Epoch: 44
0.000000/1000.000000 ==> Training loss: 0.521284    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.326952    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.353533    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.498530    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.427460    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.349093    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.311323    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.337610    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.929363    Training error rate: 28.000000
450.000000/1000.000000 ==> Training loss: 0.502359    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.474299    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.644784    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.544656    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.418992    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.544744    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.677972    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.468367    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.603698    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.460167    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.414193    Training error rate: 12.000000
==> Total training loss: 460.236724    Total training error rate: 14.322000
==> Testing Epoch: 44
0.000000/100.000000 ==> Testing loss: 1.217281    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.287425    Testing error rate: 30.000000
==> Total testing loss: 136.454186    Total testing error rate: 33.370000
==> Set learning rate: 0.010000
==> Training Epoch: 45
0.000000/1000.000000 ==> Training loss: 0.430786    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.305765    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.326511    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.445151    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.351924    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.487034    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.348738    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.280281    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.374279    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.526731    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.709168    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.641037    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.673947    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.493031    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.863913    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 0.446785    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.953513    Training error rate: 28.000000
850.000000/1000.000000 ==> Training loss: 0.608976    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.604473    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.494909    Training error rate: 14.000000
==> Total training loss: 454.403468    Total training error rate: 14.204000
==> Testing Epoch: 45
0.000000/100.000000 ==> Testing loss: 1.304280    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.244558    Testing error rate: 33.000000
==> Total testing loss: 137.683011    Total testing error rate: 33.340000
==> Set learning rate: 0.010000
==> Training Epoch: 46
0.000000/1000.000000 ==> Training loss: 0.410263    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.319745    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.592297    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.366854    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.469393    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.320772    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.202899    Training error rate: 6.000000
350.000000/1000.000000 ==> Training loss: 0.471216    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.471814    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.523177    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.368291    Training error rate: 12.000000
550.000000/1000.000000 ==> Training loss: 0.488721    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.388250    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.337835    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.474120    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.304200    Training error rate: 8.000000
800.000000/1000.000000 ==> Training loss: 0.445077    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.430269    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.644756    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.239597    Training error rate: 8.000000
==> Total training loss: 450.199352    Total training error rate: 14.184000
==> Testing Epoch: 46
0.000000/100.000000 ==> Testing loss: 1.836052    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.678423    Testing error rate: 36.000000
==> Total testing loss: 145.251171    Total testing error rate: 34.780000
==> Set learning rate: 0.010000
==> Training Epoch: 47
0.000000/1000.000000 ==> Training loss: 0.490157    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.312375    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.246098    Training error rate: 6.000000
150.000000/1000.000000 ==> Training loss: 0.720378    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.359708    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.395107    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.394884    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.266999    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.276269    Training error rate: 8.000000
450.000000/1000.000000 ==> Training loss: 0.691242    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.646556    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.457709    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.435227    Training error rate: 12.000000
650.000000/1000.000000 ==> Training loss: 0.313825    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.354125    Training error rate: 10.000000
750.000000/1000.000000 ==> Training loss: 0.339714    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.515155    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.802391    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.374395    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.638388    Training error rate: 18.000000
==> Total training loss: 453.612544    Total training error rate: 14.124000
==> Testing Epoch: 47
0.000000/100.000000 ==> Testing loss: 1.587674    Testing error rate: 33.000000
50.000000/100.000000 ==> Testing loss: 1.214726    Testing error rate: 35.000000
==> Total testing loss: 136.184986    Total testing error rate: 33.200000
==> Set learning rate: 0.010000
==> Training Epoch: 48
0.000000/1000.000000 ==> Training loss: 0.291185    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.547348    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.419140    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.248258    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.602171    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.250053    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.498472    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.398649    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.379074    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.513914    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.417094    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.660054    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.266843    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.336535    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.667075    Training error rate: 20.000000
750.000000/1000.000000 ==> Training loss: 0.758635    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.339449    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.261778    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.472816    Training error rate: 12.000000
950.000000/1000.000000 ==> Training loss: 0.292143    Training error rate: 4.000000
==> Total training loss: 432.183565    Total training error rate: 13.526000
==> Testing Epoch: 48
0.000000/100.000000 ==> Testing loss: 1.483737    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.292743    Testing error rate: 30.000000
==> Total testing loss: 133.245294    Total testing error rate: 33.460000
==> Set learning rate: 0.010000
==> Training Epoch: 49
0.000000/1000.000000 ==> Training loss: 0.274462    Training error rate: 8.000000
50.000000/1000.000000 ==> Training loss: 0.320562    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.192347    Training error rate: 4.000000
150.000000/1000.000000 ==> Training loss: 0.296792    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.370274    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.291290    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.395462    Training error rate: 14.000000
350.000000/1000.000000 ==> Training loss: 0.447141    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.694239    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.391202    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.314588    Training error rate: 8.000000
550.000000/1000.000000 ==> Training loss: 0.522208    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.350614    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.629715    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.748747    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.634868    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.484708    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.231160    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.678211    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.405243    Training error rate: 12.000000
==> Total training loss: 434.063528    Total training error rate: 13.640000
==> Testing Epoch: 49
0.000000/100.000000 ==> Testing loss: 1.341824    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.651895    Testing error rate: 37.000000
==> Total testing loss: 132.950175    Total testing error rate: 32.860000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 50
0.000000/1000.000000 ==> Training loss: 0.271199    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.368749    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.334075    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.458919    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.540437    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.333041    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.357528    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.355388    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.452689    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.391300    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.344761    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.308382    Training error rate: 6.000000
600.000000/1000.000000 ==> Training loss: 0.434797    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.527235    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.473409    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.541115    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.221035    Training error rate: 8.000000
850.000000/1000.000000 ==> Training loss: 0.317308    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.460594    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.379441    Training error rate: 12.000000
==> Total training loss: 414.474264    Total training error rate: 13.054000
==> Testing Epoch: 50
0.000000/100.000000 ==> Testing loss: 1.411966    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.248434    Testing error rate: 29.000000
==> Total testing loss: 130.023694    Total testing error rate: 32.080000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 51
0.000000/1000.000000 ==> Training loss: 0.548065    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.530744    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.343395    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.170699    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.308693    Training error rate: 8.000000
250.000000/1000.000000 ==> Training loss: 0.087190    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.324754    Training error rate: 14.000000
350.000000/1000.000000 ==> Training loss: 0.187241    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.168844    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.186813    Training error rate: 8.000000
500.000000/1000.000000 ==> Training loss: 0.231997    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.105977    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.128095    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.162911    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.187772    Training error rate: 6.000000
750.000000/1000.000000 ==> Training loss: 0.241723    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.275930    Training error rate: 8.000000
850.000000/1000.000000 ==> Training loss: 0.246420    Training error rate: 6.000000
900.000000/1000.000000 ==> Training loss: 0.086957    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.111015    Training error rate: 2.000000
==> Total training loss: 188.944883    Total training error rate: 5.376000
==> Testing Epoch: 51
0.000000/100.000000 ==> Testing loss: 1.104177    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.076446    Testing error rate: 26.000000
==> Total testing loss: 100.681273    Total testing error rate: 25.650000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 52
0.000000/1000.000000 ==> Training loss: 0.041489    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.087955    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.106856    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.262815    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.054615    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.087994    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.076306    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.082756    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.065131    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.164704    Training error rate: 6.000000
500.000000/1000.000000 ==> Training loss: 0.093612    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.063397    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.161989    Training error rate: 6.000000
650.000000/1000.000000 ==> Training loss: 0.206366    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.144986    Training error rate: 4.000000
750.000000/1000.000000 ==> Training loss: 0.074154    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.091502    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.078383    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.158312    Training error rate: 4.000000
950.000000/1000.000000 ==> Training loss: 0.094958    Training error rate: 4.000000
==> Total training loss: 114.190603    Total training error rate: 2.926000
==> Testing Epoch: 52
0.000000/100.000000 ==> Testing loss: 1.114215    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.081510    Testing error rate: 26.000000
==> Total testing loss: 99.513879    Total testing error rate: 25.300000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 53
0.000000/1000.000000 ==> Training loss: 0.056279    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.093921    Training error rate: 4.000000
100.000000/1000.000000 ==> Training loss: 0.105373    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.177438    Training error rate: 4.000000
200.000000/1000.000000 ==> Training loss: 0.115392    Training error rate: 6.000000
250.000000/1000.000000 ==> Training loss: 0.023996    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.073734    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.022851    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.107368    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.045182    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.087432    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.057751    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.350080    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.031174    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.052472    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.086697    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.082900    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.120121    Training error rate: 6.000000
900.000000/1000.000000 ==> Training loss: 0.126391    Training error rate: 4.000000
950.000000/1000.000000 ==> Training loss: 0.106905    Training error rate: 6.000000
==> Total training loss: 90.951876    Total training error rate: 2.206000
==> Testing Epoch: 53
0.000000/100.000000 ==> Testing loss: 1.136514    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.078532    Testing error rate: 26.000000
==> Total testing loss: 98.590839    Total testing error rate: 25.240000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 54
0.000000/1000.000000 ==> Training loss: 0.113570    Training error rate: 4.000000
50.000000/1000.000000 ==> Training loss: 0.077907    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.085364    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.093794    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.048368    Training error rate: 2.000000
250.000000/1000.000000 ==> Training loss: 0.096128    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.031369    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.106214    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.044870    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.076299    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.108279    Training error rate: 2.000000
550.000000/1000.000000 ==> Training loss: 0.040064    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.039673    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.194496    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.042963    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.054935    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.045241    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.061993    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.062955    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.082277    Training error rate: 2.000000
==> Total training loss: 75.623383    Total training error rate: 1.646000
==> Testing Epoch: 54
0.000000/100.000000 ==> Testing loss: 1.154672    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.126218    Testing error rate: 25.000000
==> Total testing loss: 98.961594    Total testing error rate: 25.030000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 55
0.000000/1000.000000 ==> Training loss: 0.025742    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.078817    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.066502    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.054367    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.063125    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.050767    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.110262    Training error rate: 4.000000
350.000000/1000.000000 ==> Training loss: 0.044629    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.098108    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.044445    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.106241    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.048584    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.034466    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.038486    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.067968    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.080149    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.065024    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.097756    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.033506    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.102009    Training error rate: 2.000000
==> Total training loss: 68.041148    Total training error rate: 1.518000
==> Testing Epoch: 55
0.000000/100.000000 ==> Testing loss: 1.144079    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.141368    Testing error rate: 24.000000
==> Total testing loss: 99.233717    Total testing error rate: 24.840000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 56
0.000000/1000.000000 ==> Training loss: 0.031017    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.023896    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.062789    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.029212    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.045258    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.030282    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.014496    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.026994    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.074402    Training error rate: 4.000000
450.000000/1000.000000 ==> Training loss: 0.040946    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.048940    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.068985    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.059540    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.040985    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.037657    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.043446    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.066146    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.036403    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.074663    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.058206    Training error rate: 0.000000
==> Total training loss: 61.289695    Total training error rate: 1.348000
==> Testing Epoch: 56
0.000000/100.000000 ==> Testing loss: 1.155417    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.100886    Testing error rate: 25.000000
==> Total testing loss: 99.680632    Total testing error rate: 25.210000
==> Set learning rate: 0.001000
==> Training Epoch: 57
0.000000/1000.000000 ==> Training loss: 0.045787    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.044946    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.030281    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.097151    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.074337    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.007269    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.067873    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.095476    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.054860    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.037578    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.056823    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.036744    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.051314    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.032107    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.065067    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.042080    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.030135    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.036866    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.034153    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.032587    Training error rate: 0.000000
==> Total training loss: 55.976676    Total training error rate: 1.154000
==> Testing Epoch: 57
0.000000/100.000000 ==> Testing loss: 1.182112    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.179492    Testing error rate: 26.000000
==> Total testing loss: 99.916698    Total testing error rate: 24.790000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 58
0.000000/1000.000000 ==> Training loss: 0.061238    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.038715    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.047444    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.043027    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.011927    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.034696    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.030611    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.040962    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.068296    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.073488    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.032725    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.081250    Training error rate: 4.000000
600.000000/1000.000000 ==> Training loss: 0.036159    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.026027    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.040845    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.063127    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.048407    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.085359    Training error rate: 4.000000
900.000000/1000.000000 ==> Training loss: 0.025505    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.084730    Training error rate: 4.000000
==> Total training loss: 49.025808    Total training error rate: 0.916000
==> Testing Epoch: 58
0.000000/100.000000 ==> Testing loss: 1.220926    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.160979    Testing error rate: 26.000000
==> Total testing loss: 100.286230    Total testing error rate: 24.660000
==> Saving checkpoint..==> Init variables..
==> Init seed..
==> Download data..
Files already downloaded and verified
==> Calculate mean and std..
==> Prepare training transform..
==> Prepare testing transform..
==> Init dataloader..
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
==> Set learning rate: 0.010000
==> Training Epoch: 1
0.000000/1000.000000 ==> Training loss: 5.099441    Training error rate: 100.000000
50.000000/1000.000000 ==> Training loss: 5.162900    Training error rate: 100.000000
100.000000/1000.000000 ==> Training loss: 4.416446    Training error rate: 92.000000
150.000000/1000.000000 ==> Training loss: 4.146763    Training error rate: 92.000000
200.000000/1000.000000 ==> Training loss: 4.196798    Training error rate: 94.000000
250.000000/1000.000000 ==> Training loss: 4.275916    Training error rate: 98.000000
300.000000/1000.000000 ==> Training loss: 4.085358    Training error rate: 96.000000
350.000000/1000.000000 ==> Training loss: 3.955751    Training error rate: 92.000000
400.000000/1000.000000 ==> Training loss: 4.038115    Training error rate: 90.000000
450.000000/1000.000000 ==> Training loss: 3.933294    Training error rate: 90.000000
500.000000/1000.000000 ==> Training loss: 3.824020    Training error rate: 84.000000
550.000000/1000.000000 ==> Training loss: 4.291460    Training error rate: 98.000000
600.000000/1000.000000 ==> Training loss: 3.678003    Training error rate: 86.000000
650.000000/1000.000000 ==> Training loss: 3.824186    Training error rate: 90.000000
700.000000/1000.000000 ==> Training loss: 4.028839    Training error rate: 94.000000
750.000000/1000.000000 ==> Training loss: 3.705155    Training error rate: 82.000000
800.000000/1000.000000 ==> Training loss: 3.787447    Training error rate: 88.000000
850.000000/1000.000000 ==> Training loss: 3.356160    Training error rate: 88.000000
900.000000/1000.000000 ==> Training loss: 3.684165    Training error rate: 86.000000
950.000000/1000.000000 ==> Training loss: 3.438225    Training error rate: 82.000000
==> Total training loss: 4026.317283    Total training error rate: 92.072000
==> Testing Epoch: 1
0.000000/100.000000 ==> Testing loss: 3.758418    Testing error rate: 86.000000
50.000000/100.000000 ==> Testing loss: 3.650816    Testing error rate: 81.000000
==> Total testing loss: 365.586010    Total testing error rate: 86.580000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 2
0.000000/1000.000000 ==> Training loss: 3.549226    Training error rate: 86.000000
50.000000/1000.000000 ==> Training loss: 3.510678    Training error rate: 88.000000
100.000000/1000.000000 ==> Training loss: 3.433121    Training error rate: 84.000000
150.000000/1000.000000 ==> Training loss: 3.509465    Training error rate: 90.000000
200.000000/1000.000000 ==> Training loss: 3.349941    Training error rate: 82.000000
250.000000/1000.000000 ==> Training loss: 3.458431    Training error rate: 92.000000
300.000000/1000.000000 ==> Training loss: 3.431476    Training error rate: 82.000000
350.000000/1000.000000 ==> Training loss: 3.303687    Training error rate: 78.000000
400.000000/1000.000000 ==> Training loss: 3.742269    Training error rate: 94.000000
450.000000/1000.000000 ==> Training loss: 3.477706    Training error rate: 78.000000
500.000000/1000.000000 ==> Training loss: 3.338876    Training error rate: 86.000000
550.000000/1000.000000 ==> Training loss: 3.078589    Training error rate: 72.000000
600.000000/1000.000000 ==> Training loss: 3.103625    Training error rate: 82.000000
650.000000/1000.000000 ==> Training loss: 3.437057    Training error rate: 82.000000
700.000000/1000.000000 ==> Training loss: 3.287236    Training error rate: 76.000000
750.000000/1000.000000 ==> Training loss: 2.865732    Training error rate: 78.000000
800.000000/1000.000000 ==> Training loss: 3.272567    Training error rate: 82.000000
850.000000/1000.000000 ==> Training loss: 3.279181    Training error rate: 80.000000
900.000000/1000.000000 ==> Training loss: 2.880152    Training error rate: 74.000000
950.000000/1000.000000 ==> Training loss: 3.383698    Training error rate: 72.000000
==> Total training loss: 3359.914930    Total training error rate: 81.912000
==> Testing Epoch: 2
0.000000/100.000000 ==> Testing loss: 3.374336    Testing error rate: 80.000000
50.000000/100.000000 ==> Testing loss: 3.117177    Testing error rate: 74.000000
==> Total testing loss: 315.195562    Total testing error rate: 77.330000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 3
0.000000/1000.000000 ==> Training loss: 3.145865    Training error rate: 74.000000
50.000000/1000.000000 ==> Training loss: 2.870741    Training error rate: 62.000000
100.000000/1000.000000 ==> Training loss: 3.297317    Training error rate: 82.000000
150.000000/1000.000000 ==> Training loss: 3.799235    Training error rate: 84.000000
200.000000/1000.000000 ==> Training loss: 2.678745    Training error rate: 82.000000
250.000000/1000.000000 ==> Training loss: 2.843819    Training error rate: 74.000000
300.000000/1000.000000 ==> Training loss: 3.275985    Training error rate: 78.000000
350.000000/1000.000000 ==> Training loss: 2.756059    Training error rate: 66.000000
400.000000/1000.000000 ==> Training loss: 2.847559    Training error rate: 78.000000
450.000000/1000.000000 ==> Training loss: 2.795875    Training error rate: 72.000000
500.000000/1000.000000 ==> Training loss: 2.741857    Training error rate: 68.000000
550.000000/1000.000000 ==> Training loss: 2.882214    Training error rate: 70.000000
600.000000/1000.000000 ==> Training loss: 2.903807    Training error rate: 74.000000
650.000000/1000.000000 ==> Training loss: 2.969321    Training error rate: 72.000000
700.000000/1000.000000 ==> Training loss: 2.800308    Training error rate: 70.000000
750.000000/1000.000000 ==> Training loss: 2.433994    Training error rate: 68.000000
800.000000/1000.000000 ==> Training loss: 2.547932    Training error rate: 66.000000
850.000000/1000.000000 ==> Training loss: 2.979413    Training error rate: 68.000000
900.000000/1000.000000 ==> Training loss: 2.698562    Training error rate: 64.000000
950.000000/1000.000000 ==> Training loss: 2.946457    Training error rate: 70.000000
==> Total training loss: 2873.056162    Total training error rate: 73.292000
==> Testing Epoch: 3
0.000000/100.000000 ==> Testing loss: 2.909447    Testing error rate: 70.000000
50.000000/100.000000 ==> Testing loss: 2.580161    Testing error rate: 69.000000
==> Total testing loss: 278.947637    Total testing error rate: 70.880000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 4
0.000000/1000.000000 ==> Training loss: 2.840415    Training error rate: 72.000000
50.000000/1000.000000 ==> Training loss: 2.582368    Training error rate: 70.000000
100.000000/1000.000000 ==> Training loss: 2.590998    Training error rate: 66.000000
150.000000/1000.000000 ==> Training loss: 2.331321    Training error rate: 66.000000
200.000000/1000.000000 ==> Training loss: 2.334332    Training error rate: 64.000000
250.000000/1000.000000 ==> Training loss: 2.528769    Training error rate: 68.000000
300.000000/1000.000000 ==> Training loss: 2.625975    Training error rate: 68.000000
350.000000/1000.000000 ==> Training loss: 2.551847    Training error rate: 64.000000
400.000000/1000.000000 ==> Training loss: 2.384376    Training error rate: 62.000000
450.000000/1000.000000 ==> Training loss: 2.799548    Training error rate: 76.000000
500.000000/1000.000000 ==> Training loss: 2.656728    Training error rate: 84.000000
550.000000/1000.000000 ==> Training loss: 2.758385    Training error rate: 68.000000
600.000000/1000.000000 ==> Training loss: 2.019773    Training error rate: 50.000000
650.000000/1000.000000 ==> Training loss: 2.257355    Training error rate: 64.000000
700.000000/1000.000000 ==> Training loss: 2.132658    Training error rate: 52.000000
750.000000/1000.000000 ==> Training loss: 2.271585    Training error rate: 52.000000
800.000000/1000.000000 ==> Training loss: 2.052769    Training error rate: 60.000000
850.000000/1000.000000 ==> Training loss: 2.125876    Training error rate: 54.000000
900.000000/1000.000000 ==> Training loss: 2.733238    Training error rate: 72.000000
950.000000/1000.000000 ==> Training loss: 2.218821    Training error rate: 58.000000
==> Total training loss: 2457.501722    Total training error rate: 64.844000
==> Testing Epoch: 4
0.000000/100.000000 ==> Testing loss: 2.381531    Testing error rate: 62.000000
50.000000/100.000000 ==> Testing loss: 2.129648    Testing error rate: 52.000000
==> Total testing loss: 228.611561    Total testing error rate: 60.640000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 5
0.000000/1000.000000 ==> Training loss: 2.190807    Training error rate: 58.000000
50.000000/1000.000000 ==> Training loss: 1.896788    Training error rate: 56.000000
100.000000/1000.000000 ==> Training loss: 2.148231    Training error rate: 58.000000
150.000000/1000.000000 ==> Training loss: 2.085775    Training error rate: 62.000000
200.000000/1000.000000 ==> Training loss: 2.178115    Training error rate: 58.000000
250.000000/1000.000000 ==> Training loss: 2.192413    Training error rate: 60.000000
300.000000/1000.000000 ==> Training loss: 2.155362    Training error rate: 54.000000
350.000000/1000.000000 ==> Training loss: 2.115980    Training error rate: 56.000000
400.000000/1000.000000 ==> Training loss: 2.430694    Training error rate: 66.000000
450.000000/1000.000000 ==> Training loss: 2.306495    Training error rate: 64.000000
500.000000/1000.000000 ==> Training loss: 2.201328    Training error rate: 62.000000
550.000000/1000.000000 ==> Training loss: 1.843375    Training error rate: 52.000000
600.000000/1000.000000 ==> Training loss: 2.112009    Training error rate: 58.000000
650.000000/1000.000000 ==> Training loss: 1.933476    Training error rate: 60.000000
700.000000/1000.000000 ==> Training loss: 2.056706    Training error rate: 44.000000
750.000000/1000.000000 ==> Training loss: 1.873052    Training error rate: 58.000000
800.000000/1000.000000 ==> Training loss: 2.080417    Training error rate: 64.000000
850.000000/1000.000000 ==> Training loss: 2.575918    Training error rate: 72.000000
900.000000/1000.000000 ==> Training loss: 2.106843    Training error rate: 58.000000
950.000000/1000.000000 ==> Training loss: 1.903697    Training error rate: 56.000000
==> Total training loss: 2151.165540    Total training error rate: 58.320000
==> Testing Epoch: 5
0.000000/100.000000 ==> Testing loss: 2.217825    Testing error rate: 56.000000
50.000000/100.000000 ==> Testing loss: 2.022952    Testing error rate: 55.000000
==> Total testing loss: 209.899406    Total testing error rate: 55.800000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 6
0.000000/1000.000000 ==> Training loss: 2.059193    Training error rate: 54.000000
50.000000/1000.000000 ==> Training loss: 2.071478    Training error rate: 54.000000
100.000000/1000.000000 ==> Training loss: 2.033415    Training error rate: 60.000000
150.000000/1000.000000 ==> Training loss: 1.547388    Training error rate: 44.000000
200.000000/1000.000000 ==> Training loss: 1.880664    Training error rate: 46.000000
250.000000/1000.000000 ==> Training loss: 2.022408    Training error rate: 50.000000
300.000000/1000.000000 ==> Training loss: 2.078007    Training error rate: 56.000000
350.000000/1000.000000 ==> Training loss: 1.965082    Training error rate: 54.000000
400.000000/1000.000000 ==> Training loss: 2.158001    Training error rate: 58.000000
450.000000/1000.000000 ==> Training loss: 1.924524    Training error rate: 44.000000
500.000000/1000.000000 ==> Training loss: 1.738860    Training error rate: 52.000000
550.000000/1000.000000 ==> Training loss: 1.860035    Training error rate: 54.000000
600.000000/1000.000000 ==> Training loss: 2.002609    Training error rate: 54.000000
650.000000/1000.000000 ==> Training loss: 1.760525    Training error rate: 50.000000
700.000000/1000.000000 ==> Training loss: 1.627587    Training error rate: 56.000000
750.000000/1000.000000 ==> Training loss: 2.063627    Training error rate: 60.000000
800.000000/1000.000000 ==> Training loss: 1.956282    Training error rate: 60.000000
850.000000/1000.000000 ==> Training loss: 1.778941    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.968885    Training error rate: 62.000000
950.000000/1000.000000 ==> Training loss: 1.872229    Training error rate: 58.000000
==> Total training loss: 1933.528420    Total training error rate: 53.426000
==> Testing Epoch: 6
0.000000/100.000000 ==> Testing loss: 1.913968    Testing error rate: 47.000000
50.000000/100.000000 ==> Testing loss: 1.744333    Testing error rate: 45.000000
==> Total testing loss: 188.599868    Total testing error rate: 50.630000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 7
0.000000/1000.000000 ==> Training loss: 1.655617    Training error rate: 46.000000
50.000000/1000.000000 ==> Training loss: 2.058225    Training error rate: 58.000000
100.000000/1000.000000 ==> Training loss: 1.641984    Training error rate: 44.000000
150.000000/1000.000000 ==> Training loss: 1.519437    Training error rate: 48.000000
200.000000/1000.000000 ==> Training loss: 1.335865    Training error rate: 36.000000
250.000000/1000.000000 ==> Training loss: 1.371627    Training error rate: 44.000000
300.000000/1000.000000 ==> Training loss: 1.876528    Training error rate: 54.000000
350.000000/1000.000000 ==> Training loss: 1.786777    Training error rate: 46.000000
400.000000/1000.000000 ==> Training loss: 1.516261    Training error rate: 48.000000
450.000000/1000.000000 ==> Training loss: 1.786289    Training error rate: 50.000000
500.000000/1000.000000 ==> Training loss: 1.638345    Training error rate: 40.000000
550.000000/1000.000000 ==> Training loss: 1.813026    Training error rate: 48.000000
600.000000/1000.000000 ==> Training loss: 1.816995    Training error rate: 62.000000
650.000000/1000.000000 ==> Training loss: 1.644472    Training error rate: 54.000000
700.000000/1000.000000 ==> Training loss: 1.601784    Training error rate: 42.000000
750.000000/1000.000000 ==> Training loss: 1.614946    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.718328    Training error rate: 54.000000
850.000000/1000.000000 ==> Training loss: 1.503470    Training error rate: 42.000000
900.000000/1000.000000 ==> Training loss: 1.878428    Training error rate: 54.000000
950.000000/1000.000000 ==> Training loss: 1.960265    Training error rate: 50.000000
==> Total training loss: 1747.794708    Total training error rate: 48.896000
==> Testing Epoch: 7
0.000000/100.000000 ==> Testing loss: 1.839888    Testing error rate: 52.000000
50.000000/100.000000 ==> Testing loss: 1.753475    Testing error rate: 45.000000
==> Total testing loss: 182.689208    Total testing error rate: 49.470000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 8
0.000000/1000.000000 ==> Training loss: 2.101798    Training error rate: 50.000000
50.000000/1000.000000 ==> Training loss: 1.694197    Training error rate: 52.000000
100.000000/1000.000000 ==> Training loss: 1.381680    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 1.221705    Training error rate: 40.000000
200.000000/1000.000000 ==> Training loss: 1.771448    Training error rate: 56.000000
250.000000/1000.000000 ==> Training loss: 1.331762    Training error rate: 44.000000
300.000000/1000.000000 ==> Training loss: 1.735250    Training error rate: 44.000000
350.000000/1000.000000 ==> Training loss: 1.564412    Training error rate: 48.000000
400.000000/1000.000000 ==> Training loss: 1.583546    Training error rate: 48.000000
450.000000/1000.000000 ==> Training loss: 1.592473    Training error rate: 50.000000
500.000000/1000.000000 ==> Training loss: 1.602143    Training error rate: 38.000000
550.000000/1000.000000 ==> Training loss: 1.400313    Training error rate: 38.000000
600.000000/1000.000000 ==> Training loss: 1.742413    Training error rate: 48.000000
650.000000/1000.000000 ==> Training loss: 1.781681    Training error rate: 56.000000
700.000000/1000.000000 ==> Training loss: 1.934592    Training error rate: 56.000000
750.000000/1000.000000 ==> Training loss: 1.280964    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 2.093398    Training error rate: 48.000000
850.000000/1000.000000 ==> Training loss: 1.373587    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.557337    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.478017    Training error rate: 42.000000
==> Total training loss: 1607.199916    Total training error rate: 45.666000
==> Testing Epoch: 8
0.000000/100.000000 ==> Testing loss: 1.741315    Testing error rate: 43.000000
50.000000/100.000000 ==> Testing loss: 1.704606    Testing error rate: 47.000000
==> Total testing loss: 176.907435    Total testing error rate: 47.950000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 9
0.000000/1000.000000 ==> Training loss: 1.712361    Training error rate: 48.000000
50.000000/1000.000000 ==> Training loss: 1.524778    Training error rate: 38.000000
100.000000/1000.000000 ==> Training loss: 1.393590    Training error rate: 38.000000
150.000000/1000.000000 ==> Training loss: 1.836840    Training error rate: 44.000000
200.000000/1000.000000 ==> Training loss: 1.573191    Training error rate: 44.000000
250.000000/1000.000000 ==> Training loss: 1.445380    Training error rate: 40.000000
300.000000/1000.000000 ==> Training loss: 1.783751    Training error rate: 52.000000
350.000000/1000.000000 ==> Training loss: 1.687281    Training error rate: 52.000000
400.000000/1000.000000 ==> Training loss: 1.481537    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.447264    Training error rate: 38.000000
500.000000/1000.000000 ==> Training loss: 1.425301    Training error rate: 42.000000
550.000000/1000.000000 ==> Training loss: 1.473886    Training error rate: 44.000000
600.000000/1000.000000 ==> Training loss: 1.815346    Training error rate: 46.000000
650.000000/1000.000000 ==> Training loss: 1.521102    Training error rate: 38.000000
700.000000/1000.000000 ==> Training loss: 1.798823    Training error rate: 46.000000
750.000000/1000.000000 ==> Training loss: 1.337845    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.619673    Training error rate: 42.000000
850.000000/1000.000000 ==> Training loss: 1.812687    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.627762    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.633768    Training error rate: 54.000000
==> Total training loss: 1496.333617    Total training error rate: 42.558000
==> Testing Epoch: 9
0.000000/100.000000 ==> Testing loss: 1.805897    Testing error rate: 46.000000
50.000000/100.000000 ==> Testing loss: 1.769310    Testing error rate: 45.000000
==> Total testing loss: 181.217760    Total testing error rate: 48.140000
==> Set learning rate: 0.010000
==> Training Epoch: 10
0.000000/1000.000000 ==> Training loss: 1.308221    Training error rate: 42.000000
50.000000/1000.000000 ==> Training loss: 1.410322    Training error rate: 42.000000
100.000000/1000.000000 ==> Training loss: 1.523581    Training error rate: 40.000000
150.000000/1000.000000 ==> Training loss: 1.351217    Training error rate: 46.000000
200.000000/1000.000000 ==> Training loss: 1.427944    Training error rate: 42.000000
250.000000/1000.000000 ==> Training loss: 1.510982    Training error rate: 46.000000
300.000000/1000.000000 ==> Training loss: 1.980562    Training error rate: 48.000000
350.000000/1000.000000 ==> Training loss: 1.344460    Training error rate: 34.000000
400.000000/1000.000000 ==> Training loss: 1.590798    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.264358    Training error rate: 42.000000
500.000000/1000.000000 ==> Training loss: 1.392556    Training error rate: 38.000000
550.000000/1000.000000 ==> Training loss: 1.337679    Training error rate: 44.000000
600.000000/1000.000000 ==> Training loss: 1.452666    Training error rate: 44.000000
650.000000/1000.000000 ==> Training loss: 1.705950    Training error rate: 48.000000
700.000000/1000.000000 ==> Training loss: 1.303423    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 1.479489    Training error rate: 44.000000
800.000000/1000.000000 ==> Training loss: 1.039966    Training error rate: 28.000000
850.000000/1000.000000 ==> Training loss: 1.525991    Training error rate: 44.000000
900.000000/1000.000000 ==> Training loss: 1.504688    Training error rate: 48.000000
950.000000/1000.000000 ==> Training loss: 1.546789    Training error rate: 38.000000
==> Total training loss: 1395.739790    Total training error rate: 40.480000
==> Testing Epoch: 10
0.000000/100.000000 ==> Testing loss: 1.877012    Testing error rate: 49.000000
50.000000/100.000000 ==> Testing loss: 1.507011    Testing error rate: 44.000000
==> Total testing loss: 173.279470    Total testing error rate: 45.880000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 11
0.000000/1000.000000 ==> Training loss: 1.311020    Training error rate: 36.000000
50.000000/1000.000000 ==> Training loss: 1.559515    Training error rate: 48.000000
100.000000/1000.000000 ==> Training loss: 1.627589    Training error rate: 48.000000
150.000000/1000.000000 ==> Training loss: 1.203147    Training error rate: 36.000000
200.000000/1000.000000 ==> Training loss: 1.323186    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 1.649683    Training error rate: 52.000000
300.000000/1000.000000 ==> Training loss: 1.096161    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.966470    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 1.298168    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.596766    Training error rate: 44.000000
500.000000/1000.000000 ==> Training loss: 1.203436    Training error rate: 28.000000
550.000000/1000.000000 ==> Training loss: 1.087285    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 1.256607    Training error rate: 40.000000
650.000000/1000.000000 ==> Training loss: 1.551264    Training error rate: 50.000000
700.000000/1000.000000 ==> Training loss: 1.185382    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 1.462989    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.505102    Training error rate: 48.000000
850.000000/1000.000000 ==> Training loss: 1.326134    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 1.340112    Training error rate: 46.000000
950.000000/1000.000000 ==> Training loss: 1.878640    Training error rate: 60.000000
==> Total training loss: 1301.646671    Total training error rate: 37.654000
==> Testing Epoch: 11
0.000000/100.000000 ==> Testing loss: 1.623534    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.388697    Testing error rate: 37.000000
==> Total testing loss: 160.190827    Total testing error rate: 43.100000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 12
0.000000/1000.000000 ==> Training loss: 1.038240    Training error rate: 30.000000
50.000000/1000.000000 ==> Training loss: 0.933917    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 1.024173    Training error rate: 30.000000
150.000000/1000.000000 ==> Training loss: 1.391266    Training error rate: 38.000000
200.000000/1000.000000 ==> Training loss: 1.417287    Training error rate: 36.000000
250.000000/1000.000000 ==> Training loss: 1.024571    Training error rate: 34.000000
300.000000/1000.000000 ==> Training loss: 1.357576    Training error rate: 44.000000
350.000000/1000.000000 ==> Training loss: 0.931764    Training error rate: 32.000000
400.000000/1000.000000 ==> Training loss: 1.281607    Training error rate: 42.000000
450.000000/1000.000000 ==> Training loss: 1.186679    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 1.235408    Training error rate: 32.000000
550.000000/1000.000000 ==> Training loss: 1.012309    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 1.035232    Training error rate: 26.000000
650.000000/1000.000000 ==> Training loss: 1.675502    Training error rate: 48.000000
700.000000/1000.000000 ==> Training loss: 1.349720    Training error rate: 40.000000
750.000000/1000.000000 ==> Training loss: 0.894750    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 1.016231    Training error rate: 32.000000
850.000000/1000.000000 ==> Training loss: 1.287959    Training error rate: 38.000000
900.000000/1000.000000 ==> Training loss: 1.356178    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.030539    Training error rate: 34.000000
==> Total training loss: 1227.321164    Total training error rate: 35.586000
==> Testing Epoch: 12
0.000000/100.000000 ==> Testing loss: 1.326007    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.435211    Testing error rate: 40.000000
==> Total testing loss: 160.022756    Total testing error rate: 43.350000
==> Set learning rate: 0.010000
==> Training Epoch: 13
0.000000/1000.000000 ==> Training loss: 1.018544    Training error rate: 32.000000
50.000000/1000.000000 ==> Training loss: 1.258538    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 1.665256    Training error rate: 46.000000
150.000000/1000.000000 ==> Training loss: 1.054802    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.118211    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 1.119600    Training error rate: 34.000000
300.000000/1000.000000 ==> Training loss: 1.448340    Training error rate: 42.000000
350.000000/1000.000000 ==> Training loss: 1.036237    Training error rate: 40.000000
400.000000/1000.000000 ==> Training loss: 1.305989    Training error rate: 44.000000
450.000000/1000.000000 ==> Training loss: 1.126205    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 1.507743    Training error rate: 36.000000
550.000000/1000.000000 ==> Training loss: 1.176796    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 1.171439    Training error rate: 32.000000
650.000000/1000.000000 ==> Training loss: 1.072597    Training error rate: 42.000000
700.000000/1000.000000 ==> Training loss: 1.142354    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 1.275478    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.319710    Training error rate: 40.000000
850.000000/1000.000000 ==> Training loss: 1.223216    Training error rate: 40.000000
900.000000/1000.000000 ==> Training loss: 0.938066    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.087273    Training error rate: 40.000000
==> Total training loss: 1159.671479    Total training error rate: 34.186000
==> Testing Epoch: 13
0.000000/100.000000 ==> Testing loss: 1.461160    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.298147    Testing error rate: 35.000000
==> Total testing loss: 145.261071    Total testing error rate: 39.920000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 14
0.000000/1000.000000 ==> Training loss: 1.118261    Training error rate: 40.000000
50.000000/1000.000000 ==> Training loss: 1.124757    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 1.239783    Training error rate: 32.000000
150.000000/1000.000000 ==> Training loss: 1.187918    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 1.061010    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 0.839640    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 1.011726    Training error rate: 38.000000
350.000000/1000.000000 ==> Training loss: 1.097311    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 1.190576    Training error rate: 30.000000
450.000000/1000.000000 ==> Training loss: 0.762178    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.708490    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 1.111733    Training error rate: 34.000000
600.000000/1000.000000 ==> Training loss: 1.046133    Training error rate: 36.000000
650.000000/1000.000000 ==> Training loss: 1.255941    Training error rate: 40.000000
700.000000/1000.000000 ==> Training loss: 0.938072    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 1.017438    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 1.218119    Training error rate: 34.000000
850.000000/1000.000000 ==> Training loss: 0.997944    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 1.173154    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.205728    Training error rate: 38.000000
==> Total training loss: 1099.191433    Total training error rate: 32.438000
==> Testing Epoch: 14
0.000000/100.000000 ==> Testing loss: 1.375886    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.271874    Testing error rate: 37.000000
==> Total testing loss: 139.792277    Total testing error rate: 39.170000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 15
0.000000/1000.000000 ==> Training loss: 1.221613    Training error rate: 38.000000
50.000000/1000.000000 ==> Training loss: 0.560222    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.772762    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.710524    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.721321    Training error rate: 28.000000
250.000000/1000.000000 ==> Training loss: 1.056720    Training error rate: 30.000000
300.000000/1000.000000 ==> Training loss: 0.965970    Training error rate: 30.000000
350.000000/1000.000000 ==> Training loss: 1.331462    Training error rate: 36.000000
400.000000/1000.000000 ==> Training loss: 1.356863    Training error rate: 38.000000
450.000000/1000.000000 ==> Training loss: 1.071047    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 1.132167    Training error rate: 34.000000
550.000000/1000.000000 ==> Training loss: 1.021435    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 1.139271    Training error rate: 36.000000
650.000000/1000.000000 ==> Training loss: 0.996715    Training error rate: 32.000000
700.000000/1000.000000 ==> Training loss: 1.033458    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.971920    Training error rate: 34.000000
800.000000/1000.000000 ==> Training loss: 1.060618    Training error rate: 38.000000
850.000000/1000.000000 ==> Training loss: 1.182297    Training error rate: 40.000000
900.000000/1000.000000 ==> Training loss: 1.509563    Training error rate: 52.000000
950.000000/1000.000000 ==> Training loss: 0.985599    Training error rate: 30.000000
==> Total training loss: 1047.713929    Total training error rate: 31.170000
==> Testing Epoch: 15
0.000000/100.000000 ==> Testing loss: 1.496948    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.263095    Testing error rate: 36.000000
==> Total testing loss: 144.440971    Total testing error rate: 40.150000
==> Set learning rate: 0.010000
==> Training Epoch: 16
0.000000/1000.000000 ==> Training loss: 1.254057    Training error rate: 34.000000
50.000000/1000.000000 ==> Training loss: 0.903395    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.769630    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.850839    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.040989    Training error rate: 28.000000
250.000000/1000.000000 ==> Training loss: 0.819836    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.714233    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.939802    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.864940    Training error rate: 26.000000
450.000000/1000.000000 ==> Training loss: 1.144696    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.938208    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 1.097088    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 1.327735    Training error rate: 38.000000
650.000000/1000.000000 ==> Training loss: 1.081001    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 1.002182    Training error rate: 24.000000
750.000000/1000.000000 ==> Training loss: 0.763017    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 1.112584    Training error rate: 36.000000
850.000000/1000.000000 ==> Training loss: 1.136391    Training error rate: 36.000000
900.000000/1000.000000 ==> Training loss: 1.290845    Training error rate: 34.000000
950.000000/1000.000000 ==> Training loss: 0.902988    Training error rate: 26.000000
==> Total training loss: 990.005387    Total training error rate: 29.318000
==> Testing Epoch: 16
0.000000/100.000000 ==> Testing loss: 1.367854    Testing error rate: 40.000000
50.000000/100.000000 ==> Testing loss: 1.418253    Testing error rate: 33.000000
==> Total testing loss: 140.218684    Total testing error rate: 38.570000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 17
0.000000/1000.000000 ==> Training loss: 1.055174    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.886549    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 1.048889    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.783463    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.717315    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.899912    Training error rate: 28.000000
300.000000/1000.000000 ==> Training loss: 1.212397    Training error rate: 40.000000
350.000000/1000.000000 ==> Training loss: 0.936780    Training error rate: 24.000000
400.000000/1000.000000 ==> Training loss: 1.002606    Training error rate: 30.000000
450.000000/1000.000000 ==> Training loss: 0.864311    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 0.774292    Training error rate: 18.000000
550.000000/1000.000000 ==> Training loss: 1.071334    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.527526    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.781021    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 1.149153    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.701995    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 1.130402    Training error rate: 40.000000
850.000000/1000.000000 ==> Training loss: 1.261313    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 0.936168    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.922013    Training error rate: 30.000000
==> Total training loss: 948.588511    Total training error rate: 28.398000
==> Testing Epoch: 17
0.000000/100.000000 ==> Testing loss: 1.285158    Testing error rate: 41.000000
50.000000/100.000000 ==> Testing loss: 1.460227    Testing error rate: 37.000000
==> Total testing loss: 134.745520    Total testing error rate: 37.230000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 18
0.000000/1000.000000 ==> Training loss: 0.926093    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.796084    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.979270    Training error rate: 28.000000
150.000000/1000.000000 ==> Training loss: 1.031440    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 0.853587    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 0.909536    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.769329    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.512024    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 1.016944    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.944859    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.733042    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.744720    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 1.094880    Training error rate: 28.000000
650.000000/1000.000000 ==> Training loss: 1.038406    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 1.081145    Training error rate: 28.000000
750.000000/1000.000000 ==> Training loss: 0.731375    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 1.187885    Training error rate: 36.000000
850.000000/1000.000000 ==> Training loss: 0.683541    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.874115    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.047111    Training error rate: 38.000000
==> Total training loss: 904.076424    Total training error rate: 27.182000
==> Testing Epoch: 18
0.000000/100.000000 ==> Testing loss: 1.441281    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.389001    Testing error rate: 38.000000
==> Total testing loss: 137.738122    Total testing error rate: 37.410000
==> Set learning rate: 0.010000
==> Training Epoch: 19
0.000000/1000.000000 ==> Training loss: 0.773415    Training error rate: 28.000000
50.000000/1000.000000 ==> Training loss: 0.827497    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 0.632235    Training error rate: 24.000000
150.000000/1000.000000 ==> Training loss: 0.971510    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 0.541616    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.711499    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 1.100118    Training error rate: 46.000000
350.000000/1000.000000 ==> Training loss: 0.869232    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 0.708827    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.699850    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 1.015908    Training error rate: 28.000000
550.000000/1000.000000 ==> Training loss: 0.772177    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.835308    Training error rate: 22.000000
650.000000/1000.000000 ==> Training loss: 1.300890    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 0.876947    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 0.829903    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 0.835801    Training error rate: 32.000000
850.000000/1000.000000 ==> Training loss: 1.057488    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 0.781430    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.949653    Training error rate: 24.000000
==> Total training loss: 876.082728    Total training error rate: 26.468000
==> Testing Epoch: 19
0.000000/100.000000 ==> Testing loss: 1.321077    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.412123    Testing error rate: 34.000000
==> Total testing loss: 136.702347    Total testing error rate: 37.090000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 20
0.000000/1000.000000 ==> Training loss: 0.767655    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.823184    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 1.076606    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.674958    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.990246    Training error rate: 30.000000
250.000000/1000.000000 ==> Training loss: 0.617291    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.708325    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.889147    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.828724    Training error rate: 28.000000
450.000000/1000.000000 ==> Training loss: 0.751438    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.649178    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.726906    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.753827    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.808747    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 1.222787    Training error rate: 38.000000
750.000000/1000.000000 ==> Training loss: 0.877584    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.855438    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.967661    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.755784    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.551310    Training error rate: 8.000000
==> Total training loss: 833.038798    Total training error rate: 25.192000
==> Testing Epoch: 20
0.000000/100.000000 ==> Testing loss: 1.455191    Testing error rate: 41.000000
50.000000/100.000000 ==> Testing loss: 1.462273    Testing error rate: 39.000000
==> Total testing loss: 140.230237    Total testing error rate: 37.290000
==> Set learning rate: 0.010000
==> Training Epoch: 21
0.000000/1000.000000 ==> Training loss: 0.811763    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.613706    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.926985    Training error rate: 30.000000
150.000000/1000.000000 ==> Training loss: 0.687967    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.678507    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.755387    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.803285    Training error rate: 32.000000
350.000000/1000.000000 ==> Training loss: 0.670693    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 1.011418    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.591748    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.780168    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 0.664683    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 1.012379    Training error rate: 30.000000
650.000000/1000.000000 ==> Training loss: 0.767051    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.710810    Training error rate: 24.000000
750.000000/1000.000000 ==> Training loss: 0.566991    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.751992    Training error rate: 20.000000
850.000000/1000.000000 ==> Training loss: 0.836663    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 1.178255    Training error rate: 30.000000
950.000000/1000.000000 ==> Training loss: 0.693405    Training error rate: 24.000000
==> Total training loss: 805.103362    Total training error rate: 24.656000
==> Testing Epoch: 21
0.000000/100.000000 ==> Testing loss: 1.480474    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.286268    Testing error rate: 31.000000
==> Total testing loss: 138.010314    Total testing error rate: 36.940000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 22
0.000000/1000.000000 ==> Training loss: 0.385826    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.302237    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.778906    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.641563    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.672638    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.530859    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.903176    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.642352    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.865655    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.853225    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.812954    Training error rate: 32.000000
550.000000/1000.000000 ==> Training loss: 1.030781    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.781286    Training error rate: 24.000000
650.000000/1000.000000 ==> Training loss: 0.531625    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.764217    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.714585    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 1.226999    Training error rate: 42.000000
850.000000/1000.000000 ==> Training loss: 0.694048    Training error rate: 26.000000
900.000000/1000.000000 ==> Training loss: 0.824467    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.659504    Training error rate: 18.000000
==> Total training loss: 773.233685    Total training error rate: 23.638000
==> Testing Epoch: 22
0.000000/100.000000 ==> Testing loss: 1.505965    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.510814    Testing error rate: 33.000000
==> Total testing loss: 135.249002    Total testing error rate: 35.820000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 23
0.000000/1000.000000 ==> Training loss: 0.941841    Training error rate: 28.000000
50.000000/1000.000000 ==> Training loss: 0.799043    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.523504    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.562729    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.922137    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.944953    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.811191    Training error rate: 20.000000
350.000000/1000.000000 ==> Training loss: 0.694753    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.621766    Training error rate: 22.000000
450.000000/1000.000000 ==> Training loss: 0.710219    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.408593    Training error rate: 12.000000
550.000000/1000.000000 ==> Training loss: 0.609686    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.504516    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.899537    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 0.951560    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.618123    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.662444    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.966857    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.793802    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 1.061447    Training error rate: 30.000000
==> Total training loss: 747.542327    Total training error rate: 22.684000
==> Testing Epoch: 23
0.000000/100.000000 ==> Testing loss: 1.350777    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.434296    Testing error rate: 32.000000
==> Total testing loss: 131.407366    Total testing error rate: 34.600000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 24
0.000000/1000.000000 ==> Training loss: 0.604323    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.856048    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.563504    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.588071    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.483137    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.747936    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.987395    Training error rate: 32.000000
350.000000/1000.000000 ==> Training loss: 0.644239    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.584988    Training error rate: 22.000000
450.000000/1000.000000 ==> Training loss: 0.668496    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.608880    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.682369    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.630751    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.753236    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.949480    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.907636    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.943212    Training error rate: 30.000000
850.000000/1000.000000 ==> Training loss: 1.020969    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.845161    Training error rate: 28.000000
950.000000/1000.000000 ==> Training loss: 0.587399    Training error rate: 18.000000
==> Total training loss: 727.030024    Total training error rate: 22.458000
==> Testing Epoch: 24
0.000000/100.000000 ==> Testing loss: 1.381206    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.042953    Testing error rate: 30.000000
==> Total testing loss: 130.848066    Total testing error rate: 34.620000
==> Set learning rate: 0.010000
==> Training Epoch: 25
0.000000/1000.000000 ==> Training loss: 0.655942    Training error rate: 32.000000
50.000000/1000.000000 ==> Training loss: 0.641364    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.539858    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.518427    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.552606    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.680367    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.731961    Training error rate: 22.000000
350.000000/1000.000000 ==> Training loss: 0.944749    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.571494    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.505482    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.504386    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.896310    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.705431    Training error rate: 26.000000
650.000000/1000.000000 ==> Training loss: 0.792656    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.789050    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 0.597255    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.866821    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.579395    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.794953    Training error rate: 28.000000
950.000000/1000.000000 ==> Training loss: 0.731680    Training error rate: 24.000000
==> Total training loss: 698.406479    Total training error rate: 21.556000
==> Testing Epoch: 25
0.000000/100.000000 ==> Testing loss: 1.321187    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.090315    Testing error rate: 29.000000
==> Total testing loss: 127.379310    Total testing error rate: 34.670000
==> Set learning rate: 0.010000
==> Training Epoch: 26
0.000000/1000.000000 ==> Training loss: 0.697295    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.644665    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.567076    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.405677    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.496837    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.559085    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.541182    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.547433    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.728428    Training error rate: 20.000000
450.000000/1000.000000 ==> Training loss: 0.808685    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.722217    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.726590    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.594902    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.914522    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 0.881229    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.794620    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.932929    Training error rate: 34.000000
850.000000/1000.000000 ==> Training loss: 0.725854    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.634211    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.810605    Training error rate: 30.000000
==> Total training loss: 676.413966    Total training error rate: 20.674000
==> Testing Epoch: 26
0.000000/100.000000 ==> Testing loss: 1.136712    Testing error rate: 30.000000
50.000000/100.000000 ==> Testing loss: 1.198634    Testing error rate: 29.000000
==> Total testing loss: 127.802508    Total testing error rate: 33.790000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 27
0.000000/1000.000000 ==> Training loss: 0.637743    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.551723    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 0.610010    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.607348    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.570131    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.583517    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.595628    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.651267    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.467685    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.707869    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.679716    Training error rate: 26.000000
550.000000/1000.000000 ==> Training loss: 0.610795    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.791844    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.653148    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.767559    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.812972    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.642997    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.416368    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.686387    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.812223    Training error rate: 24.000000
==> Total training loss: 650.128888    Total training error rate: 20.248000
==> Testing Epoch: 27
0.000000/100.000000 ==> Testing loss: 1.193094    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.400613    Testing error rate: 34.000000
==> Total testing loss: 135.267944    Total testing error rate: 35.520000
==> Set learning rate: 0.010000
==> Training Epoch: 28
0.000000/1000.000000 ==> Training loss: 0.640792    Training error rate: 20.000000
50.000000/1000.000000 ==> Training loss: 0.577937    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.480967    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.599682    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.601168    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.877462    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 0.402524    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.676769    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.633084    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.767712    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.781782    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.754575    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.848044    Training error rate: 28.000000
650.000000/1000.000000 ==> Training loss: 0.378646    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.576601    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.640841    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.409048    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.735445    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.834494    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.691417    Training error rate: 22.000000
==> Total training loss: 641.710395    Total training error rate: 19.792000
==> Testing Epoch: 28
0.000000/100.000000 ==> Testing loss: 1.379735    Testing error rate: 33.000000
50.000000/100.000000 ==> Testing loss: 1.126057    Testing error rate: 28.000000
==> Total testing loss: 129.511713    Total testing error rate: 34.020000
==> Set learning rate: 0.010000
==> Training Epoch: 29
0.000000/1000.000000 ==> Training loss: 0.575846    Training error rate: 20.000000
50.000000/1000.000000 ==> Training loss: 0.522687    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.612530    Training error rate: 20.000000
150.000000/1000.000000 ==> Training loss: 0.484939    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.615149    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.590311    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.375446    Training error rate: 8.000000
350.000000/1000.000000 ==> Training loss: 0.530687    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.564189    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.462727    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.557025    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.858578    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.750054    Training error rate: 22.000000
650.000000/1000.000000 ==> Training loss: 0.747648    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.704276    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.467301    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.748628    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.761602    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.534158    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.657249    Training error rate: 24.000000
==> Total training loss: 616.169580    Total training error rate: 19.128000
==> Testing Epoch: 29
0.000000/100.000000 ==> Testing loss: 1.219901    Testing error rate: 31.000000
50.000000/100.000000 ==> Testing loss: 1.380697    Testing error rate: 40.000000
==> Total testing loss: 127.717714    Total testing error rate: 33.740000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 30
0.000000/1000.000000 ==> Training loss: 0.300561    Training error rate: 4.000000
50.000000/1000.000000 ==> Training loss: 0.698130    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.506481    Training error rate: 22.000000
150.000000/1000.000000 ==> Training loss: 0.419953    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.843976    Training error rate: 22.000000
250.000000/1000.000000 ==> Training loss: 0.270207    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.737075    Training error rate: 18.000000
350.000000/1000.000000 ==> Training loss: 0.652305    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.602375    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.645290    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.829702    Training error rate: 30.000000
550.000000/1000.000000 ==> Training loss: 0.546342    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.506395    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.622368    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.788734    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.638477    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.632868    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.650862    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.548122    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.687617    Training error rate: 22.000000
==> Total training loss: 610.352457    Total training error rate: 18.886000
==> Testing Epoch: 30
0.000000/100.000000 ==> Testing loss: 1.182697    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.471372    Testing error rate: 40.000000
==> Total testing loss: 133.991861    Total testing error rate: 34.240000
==> Set learning rate: 0.010000
==> Training Epoch: 31
0.000000/1000.000000 ==> Training loss: 0.667576    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.514208    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.617630    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.480765    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.610526    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.468969    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.945008    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.581384    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.363959    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.758432    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.634695    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.553093    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.556681    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.646187    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.553672    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.533663    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.602935    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.576443    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.706883    Training error rate: 30.000000
950.000000/1000.000000 ==> Training loss: 0.795737    Training error rate: 24.000000
==> Total training loss: 592.895187    Total training error rate: 18.432000
==> Testing Epoch: 31
0.000000/100.000000 ==> Testing loss: 1.279816    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.536136    Testing error rate: 35.000000
==> Total testing loss: 135.527912    Total testing error rate: 34.780000
==> Set learning rate: 0.010000
==> Training Epoch: 32
0.000000/1000.000000 ==> Training loss: 0.615069    Training error rate: 22.000000
50.000000/1000.000000 ==> Training loss: 0.669840    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.425765    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.628283    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.446802    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.461664    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.659031    Training error rate: 18.000000
350.000000/1000.000000 ==> Training loss: 0.471534    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.554291    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.821254    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 0.500932    Training error rate: 10.000000
550.000000/1000.000000 ==> Training loss: 0.622824    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.479912    Training error rate: 18.000000
650.000000/1000.000000 ==> Training loss: 0.642036    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.644528    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.835607    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.779741    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.658465    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.549923    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.571006    Training error rate: 14.000000
==> Total training loss: 578.608277    Total training error rate: 17.968000
==> Testing Epoch: 32
0.000000/100.000000 ==> Testing loss: 1.331419    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.472029    Testing error rate: 38.000000
==> Total testing loss: 133.881347    Total testing error rate: 34.920000
==> Set learning rate: 0.010000
==> Training Epoch: 33
0.000000/1000.000000 ==> Training loss: 0.376245    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.474759    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.368760    Training error rate: 8.000000
150.000000/1000.000000 ==> Training loss: 0.609937    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.443883    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.509917    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.451320    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.758068    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.477627    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.830653    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.529440    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 0.831604    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 0.704390    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.571816    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.589993    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.776993    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 0.440612    Training error rate: 10.000000
850.000000/1000.000000 ==> Training loss: 0.709979    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.767564    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.486793    Training error rate: 12.000000
==> Total training loss: 568.747226    Total training error rate: 17.668000
==> Testing Epoch: 33
0.000000/100.000000 ==> Testing loss: 1.496711    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.248182    Testing error rate: 35.000000
==> Total testing loss: 137.825184    Total testing error rate: 34.550000
==> Set learning rate: 0.010000
==> Training Epoch: 34
0.000000/1000.000000 ==> Training loss: 0.599273    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.453589    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.559413    Training error rate: 24.000000
150.000000/1000.000000 ==> Training loss: 0.362409    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.635144    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.385949    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.546547    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.433191    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.601622    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.451471    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.572335    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.392155    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.567442    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.609029    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.528576    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.440651    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.465503    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.741505    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.808153    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.722620    Training error rate: 24.000000
==> Total training loss: 544.274256    Total training error rate: 16.796000
==> Testing Epoch: 34
0.000000/100.000000 ==> Testing loss: 1.091064    Testing error rate: 30.000000
50.000000/100.000000 ==> Testing loss: 1.441337    Testing error rate: 34.000000
==> Total testing loss: 130.364300    Total testing error rate: 33.100000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 35
0.000000/1000.000000 ==> Training loss: 0.493398    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.517093    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.353866    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.349815    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.522168    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.778699    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.500315    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.376967    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.460305    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.368589    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.364994    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.426591    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.520659    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.488706    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.514119    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.321539    Training error rate: 8.000000
800.000000/1000.000000 ==> Training loss: 0.352800    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.747712    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.543496    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.385076    Training error rate: 18.000000
==> Total training loss: 537.894123    Total training error rate: 16.742000
==> Testing Epoch: 35
0.000000/100.000000 ==> Testing loss: 1.416957    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.298468    Testing error rate: 35.000000
==> Total testing loss: 128.735879    Total testing error rate: 33.180000
==> Set learning rate: 0.010000
==> Training Epoch: 36
0.000000/1000.000000 ==> Training loss: 0.473164    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.433537    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.467526    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.443184    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.617312    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.696765    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.382543    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.443756    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.459088    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.480450    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.516917    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.404938    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.250042    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.868545    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.427695    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.760578    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.595996    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.389359    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.581635    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.332481    Training error rate: 12.000000
==> Total training loss: 530.333477    Total training error rate: 16.518000
==> Testing Epoch: 36
0.000000/100.000000 ==> Testing loss: 1.491593    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.439892    Testing error rate: 33.000000
==> Total testing loss: 132.689520    Total testing error rate: 33.590000
==> Set learning rate: 0.010000
==> Training Epoch: 37
0.000000/1000.000000 ==> Training loss: 0.522319    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.337910    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.548351    Training error rate: 20.000000
150.000000/1000.000000 ==> Training loss: 0.572943    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.291482    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.392987    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.857531    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.416996    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.345856    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.720703    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.667278    Training error rate: 18.000000
550.000000/1000.000000 ==> Training loss: 0.518329    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.513659    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.516430    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.886918    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.589640    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.722260    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.764057    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.438010    Training error rate: 10.000000
950.000000/1000.000000 ==> Training loss: 0.519963    Training error rate: 14.000000
==> Total training loss: 514.767130    Total training error rate: 16.014000
==> Testing Epoch: 37
0.000000/100.000000 ==> Testing loss: 1.362627    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.494166    Testing error rate: 37.000000
==> Total testing loss: 135.344767    Total testing error rate: 34.270000
==> Set learning rate: 0.010000
==> Training Epoch: 38
0.000000/1000.000000 ==> Training loss: 0.390156    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.210681    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.212854    Training error rate: 6.000000
150.000000/1000.000000 ==> Training loss: 0.453237    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.567344    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.397695    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.659693    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.455969    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.388112    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.499256    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.281768    Training error rate: 10.000000
550.000000/1000.000000 ==> Training loss: 0.940469    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.354611    Training error rate: 12.000000
650.000000/1000.000000 ==> Training loss: 0.586523    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.491159    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.653063    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.430160    Training error rate: 12.000000
850.000000/1000.000000 ==> Training loss: 0.585682    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.761453    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.488902    Training error rate: 14.000000
==> Total training loss: 508.658543    Total training error rate: 15.914000
==> Testing Epoch: 38
0.000000/100.000000 ==> Testing loss: 1.669469    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.403951    Testing error rate: 35.000000
==> Total testing loss: 144.760002    Total testing error rate: 35.390000
==> Set learning rate: 0.010000
==> Training Epoch: 39
0.000000/1000.000000 ==> Training loss: 0.713053    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.360587    Training error rate: 8.000000
100.000000/1000.000000 ==> Training loss: 0.333455    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.469013    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.373242    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.424465    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.788312    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.402207    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.570696    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.537472    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.705001    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.544030    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.502664    Training error rate: 18.000000
650.000000/1000.000000 ==> Training loss: 0.349189    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.606753    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.480229    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.416004    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.641803    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.490630    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.656467    Training error rate: 22.000000
==> Total training loss: 511.452728    Total training error rate: 16.054000
==> Testing Epoch: 39
0.000000/100.000000 ==> Testing loss: 1.325889    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.391128    Testing error rate: 33.000000
==> Total testing loss: 133.601643    Total testing error rate: 33.750000
==> Set learning rate: 0.010000
==> Training Epoch: 40
0.000000/1000.000000 ==> Training loss: 0.716802    Training error rate: 30.000000
50.000000/1000.000000 ==> Training loss: 0.375767    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.348400    Training error rate: 10.000000
150.000000/1000.000000 ==> Training loss: 0.386557    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.490948    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.408495    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.239014    Training error rate: 6.000000
350.000000/1000.000000 ==> Training loss: 0.360666    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.521634    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.311205    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.692995    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.503899    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.571210    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.730871    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.454530    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.437251    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.784645    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.749598    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.507238    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.742778    Training error rate: 22.000000
==> Total training loss: 490.604032    Total training error rate: 15.276000
==> Testing Epoch: 40
0.000000/100.000000 ==> Testing loss: 1.388763    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.460940    Testing error rate: 34.000000
==> Total testing loss: 133.309104    Total testing error rate: 33.510000
==> Set learning rate: 0.010000
==> Training Epoch: 41
0.000000/1000.000000 ==> Training loss: 0.400794    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.552497    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.690024    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.691139    Training error rate: 26.000000
200.000000/1000.000000 ==> Training loss: 0.403494    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.454619    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.611817    Training error rate: 22.000000
350.000000/1000.000000 ==> Training loss: 0.284257    Training error rate: 8.000000
400.000000/1000.000000 ==> Training loss: 0.304677    Training error rate: 8.000000
450.000000/1000.000000 ==> Training loss: 0.488424    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.425047    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.485073    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.492987    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.392459    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.420543    Training error rate: 20.000000
750.000000/1000.000000 ==> Training loss: 0.349525    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.531872    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.396246    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.469083    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.736115    Training error rate: 22.000000
==> Total training loss: 482.457945    Total training error rate: 14.984000
==> Testing Epoch: 41
0.000000/100.000000 ==> Testing loss: 1.286527    Testing error rate: 31.000000
50.000000/100.000000 ==> Testing loss: 1.535186    Testing error rate: 41.000000
==> Total testing loss: 139.527525    Total testing error rate: 33.960000
==> Set learning rate: 0.010000
==> Training Epoch: 42
0.000000/1000.000000 ==> Training loss: 0.588413    Training error rate: 22.000000
50.000000/1000.000000 ==> Training loss: 0.583433    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.254410    Training error rate: 8.000000
150.000000/1000.000000 ==> Training loss: 0.348387    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.198521    Training error rate: 4.000000
250.000000/1000.000000 ==> Training loss: 0.424969    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.421748    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.449636    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.564013    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.470953    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.439111    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.566951    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.869370    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.212606    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.795025    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.538101    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.508715    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.432299    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.575001    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.418013    Training error rate: 14.000000
==> Total training loss: 485.551058    Total training error rate: 15.158000
==> Testing Epoch: 42
0.000000/100.000000 ==> Testing loss: 1.276446    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.411205    Testing error rate: 36.000000
==> Total testing loss: 133.642553    Total testing error rate: 33.830000
==> Set learning rate: 0.010000
==> Training Epoch: 43
0.000000/1000.000000 ==> Training loss: 0.268995    Training error rate: 6.000000
50.000000/1000.000000 ==> Training loss: 0.483741    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.404081    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.323366    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.669584    Training error rate: 22.000000
250.000000/1000.000000 ==> Training loss: 0.517139    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.528442    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.401222    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.408127    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.422027    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.469808    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.435880    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.676620    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.382778    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.430895    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.272249    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.807660    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.329221    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.588858    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.484126    Training error rate: 12.000000
==> Total training loss: 461.249712    Total training error rate: 14.458000
==> Testing Epoch: 43
0.000000/100.000000 ==> Testing loss: 1.401996    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.405297    Testing error rate: 31.000000
==> Total testing loss: 142.204903    Total testing error rate: 34.570000
==> Set learning rate: 0.010000
==> Training Epoch: 44
0.000000/1000.000000 ==> Training loss: 0.521284    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.326952    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.353533    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.498530    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.427460    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.349093    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.311323    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.337610    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.929363    Training error rate: 28.000000
450.000000/1000.000000 ==> Training loss: 0.502359    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.474299    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.644784    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.544656    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.418992    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.544744    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.677972    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.468367    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.603698    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.460167    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.414193    Training error rate: 12.000000
==> Total training loss: 460.236724    Total training error rate: 14.322000
==> Testing Epoch: 44
0.000000/100.000000 ==> Testing loss: 1.217281    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.287425    Testing error rate: 30.000000
==> Total testing loss: 136.454186    Total testing error rate: 33.370000
==> Set learning rate: 0.010000
==> Training Epoch: 45
0.000000/1000.000000 ==> Training loss: 0.430786    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.305765    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.326511    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.445151    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.351924    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.487034    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.348738    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.280281    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.374279    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.526731    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.709168    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.641037    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.673947    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.493031    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.863913    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 0.446785    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.953513    Training error rate: 28.000000
850.000000/1000.000000 ==> Training loss: 0.608976    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.604473    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.494909    Training error rate: 14.000000
==> Total training loss: 454.403468    Total training error rate: 14.204000
==> Testing Epoch: 45
0.000000/100.000000 ==> Testing loss: 1.304280    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.244558    Testing error rate: 33.000000
==> Total testing loss: 137.683011    Total testing error rate: 33.340000
==> Set learning rate: 0.010000
==> Training Epoch: 46
0.000000/1000.000000 ==> Training loss: 0.410263    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.319745    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.592297    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.366854    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.469393    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.320772    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.202899    Training error rate: 6.000000
350.000000/1000.000000 ==> Training loss: 0.471216    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.471814    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.523177    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.368291    Training error rate: 12.000000
550.000000/1000.000000 ==> Training loss: 0.488721    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.388250    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.337835    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.474120    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.304200    Training error rate: 8.000000
800.000000/1000.000000 ==> Training loss: 0.445077    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.430269    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.644756    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.239597    Training error rate: 8.000000
==> Total training loss: 450.199352    Total training error rate: 14.184000
==> Testing Epoch: 46
0.000000/100.000000 ==> Testing loss: 1.836052    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.678423    Testing error rate: 36.000000
==> Total testing loss: 145.251171    Total testing error rate: 34.780000
==> Set learning rate: 0.010000
==> Training Epoch: 47
0.000000/1000.000000 ==> Training loss: 0.490157    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.312375    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.246098    Training error rate: 6.000000
150.000000/1000.000000 ==> Training loss: 0.720378    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.359708    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.395107    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.394884    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.266999    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.276269    Training error rate: 8.000000
450.000000/1000.000000 ==> Training loss: 0.691242    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.646556    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.457709    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.435227    Training error rate: 12.000000
650.000000/1000.000000 ==> Training loss: 0.313825    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.354125    Training error rate: 10.000000
750.000000/1000.000000 ==> Training loss: 0.339714    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.515155    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.802391    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.374395    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.638388    Training error rate: 18.000000
==> Total training loss: 453.612544    Total training error rate: 14.124000
==> Testing Epoch: 47
0.000000/100.000000 ==> Testing loss: 1.587674    Testing error rate: 33.000000
50.000000/100.000000 ==> Testing loss: 1.214726    Testing error rate: 35.000000
==> Total testing loss: 136.184986    Total testing error rate: 33.200000
==> Set learning rate: 0.010000
==> Training Epoch: 48
0.000000/1000.000000 ==> Training loss: 0.291185    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.547348    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.419140    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.248258    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.602171    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.250053    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.498472    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.398649    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.379074    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.513914    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.417094    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.660054    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.266843    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.336535    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.667075    Training error rate: 20.000000
750.000000/1000.000000 ==> Training loss: 0.758635    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.339449    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.261778    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.472816    Training error rate: 12.000000
950.000000/1000.000000 ==> Training loss: 0.292143    Training error rate: 4.000000
==> Total training loss: 432.183565    Total training error rate: 13.526000
==> Testing Epoch: 48
0.000000/100.000000 ==> Testing loss: 1.483737    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.292743    Testing error rate: 30.000000
==> Total testing loss: 133.245294    Total testing error rate: 33.460000
==> Set learning rate: 0.010000
==> Training Epoch: 49
0.000000/1000.000000 ==> Training loss: 0.274462    Training error rate: 8.000000
50.000000/1000.000000 ==> Training loss: 0.320562    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.192347    Training error rate: 4.000000
150.000000/1000.000000 ==> Training loss: 0.296792    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.370274    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.291290    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.395462    Training error rate: 14.000000
350.000000/1000.000000 ==> Training loss: 0.447141    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.694239    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.391202    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.314588    Training error rate: 8.000000
550.000000/1000.000000 ==> Training loss: 0.522208    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.350614    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.629715    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.748747    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.634868    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.484708    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.231160    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.678211    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.405243    Training error rate: 12.000000
==> Total training loss: 434.063528    Total training error rate: 13.640000
==> Testing Epoch: 49
0.000000/100.000000 ==> Testing loss: 1.341824    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.651895    Testing error rate: 37.000000
==> Total testing loss: 132.950175    Total testing error rate: 32.860000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 50
0.000000/1000.000000 ==> Training loss: 0.271199    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.368749    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.334075    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.458919    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.540437    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.333041    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.357528    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.355388    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.452689    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.391300    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.344761    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.308382    Training error rate: 6.000000
600.000000/1000.000000 ==> Training loss: 0.434797    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.527235    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.473409    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.541115    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.221035    Training error rate: 8.000000
850.000000/1000.000000 ==> Training loss: 0.317308    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.460594    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.379441    Training error rate: 12.000000
==> Total training loss: 414.474264    Total training error rate: 13.054000
==> Testing Epoch: 50
0.000000/100.000000 ==> Testing loss: 1.411966    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.248434    Testing error rate: 29.000000
==> Total testing loss: 130.023694    Total testing error rate: 32.080000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 51
0.000000/1000.000000 ==> Training loss: 0.548065    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.530744    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.343395    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.170699    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.308693    Training error rate: 8.000000
250.000000/1000.000000 ==> Training loss: 0.087190    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.324754    Training error rate: 14.000000
350.000000/1000.000000 ==> Training loss: 0.187241    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.168844    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.186813    Training error rate: 8.000000
500.000000/1000.000000 ==> Training loss: 0.231997    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.105977    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.128095    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.162911    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.187772    Training error rate: 6.000000
750.000000/1000.000000 ==> Training loss: 0.241723    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.275930    Training error rate: 8.000000
850.000000/1000.000000 ==> Training loss: 0.246420    Training error rate: 6.000000
900.000000/1000.000000 ==> Training loss: 0.086957    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.111015    Training error rate: 2.000000
==> Total training loss: 188.944883    Total training error rate: 5.376000
==> Testing Epoch: 51
0.000000/100.000000 ==> Testing loss: 1.104177    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.076446    Testing error rate: 26.000000
==> Total testing loss: 100.681273    Total testing error rate: 25.650000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 52
0.000000/1000.000000 ==> Training loss: 0.041489    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.087955    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.106856    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.262815    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.054615    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.087994    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.076306    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.082756    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.065131    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.164704    Training error rate: 6.000000
500.000000/1000.000000 ==> Training loss: 0.093612    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.063397    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.161989    Training error rate: 6.000000
650.000000/1000.000000 ==> Training loss: 0.206366    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.144986    Training error rate: 4.000000
750.000000/1000.000000 ==> Training loss: 0.074154    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.091502    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.078383    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.158312    Training error rate: 4.000000
950.000000/1000.000000 ==> Training loss: 0.094958    Training error rate: 4.000000
==> Total training loss: 114.190603    Total training error rate: 2.926000
==> Testing Epoch: 52
0.000000/100.000000 ==> Testing loss: 1.114215    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.081510    Testing error rate: 26.000000
==> Total testing loss: 99.513879    Total testing error rate: 25.300000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 53
0.000000/1000.000000 ==> Training loss: 0.056279    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.093921    Training error rate: 4.000000
100.000000/1000.000000 ==> Training loss: 0.105373    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.177438    Training error rate: 4.000000
200.000000/1000.000000 ==> Training loss: 0.115392    Training error rate: 6.000000
250.000000/1000.000000 ==> Training loss: 0.023996    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.073734    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.022851    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.107368    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.045182    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.087432    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.057751    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.350080    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.031174    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.052472    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.086697    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.082900    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.120121    Training error rate: 6.000000
900.000000/1000.000000 ==> Training loss: 0.126391    Training error rate: 4.000000
950.000000/1000.000000 ==> Training loss: 0.106905    Training error rate: 6.000000
==> Total training loss: 90.951876    Total training error rate: 2.206000
==> Testing Epoch: 53
0.000000/100.000000 ==> Testing loss: 1.136514    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.078532    Testing error rate: 26.000000
==> Total testing loss: 98.590839    Total testing error rate: 25.240000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 54
0.000000/1000.000000 ==> Training loss: 0.113570    Training error rate: 4.000000
50.000000/1000.000000 ==> Training loss: 0.077907    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.085364    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.093794    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.048368    Training error rate: 2.000000
250.000000/1000.000000 ==> Training loss: 0.096128    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.031369    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.106214    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.044870    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.076299    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.108279    Training error rate: 2.000000
550.000000/1000.000000 ==> Training loss: 0.040064    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.039673    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.194496    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.042963    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.054935    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.045241    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.061993    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.062955    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.082277    Training error rate: 2.000000
==> Total training loss: 75.623383    Total training error rate: 1.646000
==> Testing Epoch: 54
0.000000/100.000000 ==> Testing loss: 1.154672    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.126218    Testing error rate: 25.000000
==> Total testing loss: 98.961594    Total testing error rate: 25.030000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 55
0.000000/1000.000000 ==> Training loss: 0.025742    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.078817    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.066502    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.054367    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.063125    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.050767    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.110262    Training error rate: 4.000000
350.000000/1000.000000 ==> Training loss: 0.044629    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.098108    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.044445    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.106241    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.048584    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.034466    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.038486    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.067968    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.080149    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.065024    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.097756    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.033506    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.102009    Training error rate: 2.000000
==> Total training loss: 68.041148    Total training error rate: 1.518000
==> Testing Epoch: 55
0.000000/100.000000 ==> Testing loss: 1.144079    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.141368    Testing error rate: 24.000000
==> Total testing loss: 99.233717    Total testing error rate: 24.840000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 56
0.000000/1000.000000 ==> Training loss: 0.031017    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.023896    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.062789    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.029212    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.045258    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.030282    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.014496    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.026994    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.074402    Training error rate: 4.000000
450.000000/1000.000000 ==> Training loss: 0.040946    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.048940    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.068985    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.059540    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.040985    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.037657    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.043446    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.066146    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.036403    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.074663    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.058206    Training error rate: 0.000000
==> Total training loss: 61.289695    Total training error rate: 1.348000
==> Testing Epoch: 56
0.000000/100.000000 ==> Testing loss: 1.155417    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.100886    Testing error rate: 25.000000
==> Total testing loss: 99.680632    Total testing error rate: 25.210000
==> Set learning rate: 0.001000
==> Training Epoch: 57
0.000000/1000.000000 ==> Training loss: 0.045787    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.044946    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.030281    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.097151    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.074337    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.007269    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.067873    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.095476    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.054860    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.037578    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.056823    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.036744    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.051314    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.032107    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.065067    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.042080    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.030135    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.036866    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.034153    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.032587    Training error rate: 0.000000
==> Total training loss: 55.976676    Total training error rate: 1.154000
==> Testing Epoch: 57
0.000000/100.000000 ==> Testing loss: 1.182112    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.179492    Testing error rate: 26.000000
==> Total testing loss: 99.916698    Total testing error rate: 24.790000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 58
0.000000/1000.000000 ==> Training loss: 0.061238    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.038715    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.047444    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.043027    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.011927    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.034696    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.030611    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.040962    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.068296    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.073488    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.032725    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.081250    Training error rate: 4.000000
600.000000/1000.000000 ==> Training loss: 0.036159    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.026027    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.040845    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.063127    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.048407    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.085359    Training error rate: 4.000000
900.000000/1000.000000 ==> Training loss: 0.025505    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.084730    Training error rate: 4.000000
==> Total training loss: 49.025808    Total training error rate: 0.916000
==> Testing Epoch: 58
0.000000/100.000000 ==> Testing loss: 1.220926    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.160979    Testing error rate: 26.000000
==> Total testing loss: 100.286230    Total testing error rate: 24.660000
==> Saving checkpoint..==> Init variables..
==> Init seed..
==> Download data..
Files already downloaded and verified
==> Calculate mean and std..
==> Prepare training transform..
==> Prepare testing transform..
==> Init dataloader..
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
==> Set learning rate: 0.010000
==> Training Epoch: 1
0.000000/1000.000000 ==> Training loss: 5.099441    Training error rate: 100.000000
50.000000/1000.000000 ==> Training loss: 5.162900    Training error rate: 100.000000
100.000000/1000.000000 ==> Training loss: 4.416446    Training error rate: 92.000000
150.000000/1000.000000 ==> Training loss: 4.146763    Training error rate: 92.000000
200.000000/1000.000000 ==> Training loss: 4.196798    Training error rate: 94.000000
250.000000/1000.000000 ==> Training loss: 4.275916    Training error rate: 98.000000
300.000000/1000.000000 ==> Training loss: 4.085358    Training error rate: 96.000000
350.000000/1000.000000 ==> Training loss: 3.955751    Training error rate: 92.000000
400.000000/1000.000000 ==> Training loss: 4.038115    Training error rate: 90.000000
450.000000/1000.000000 ==> Training loss: 3.933294    Training error rate: 90.000000
500.000000/1000.000000 ==> Training loss: 3.824020    Training error rate: 84.000000
550.000000/1000.000000 ==> Training loss: 4.291460    Training error rate: 98.000000
600.000000/1000.000000 ==> Training loss: 3.678003    Training error rate: 86.000000
650.000000/1000.000000 ==> Training loss: 3.824186    Training error rate: 90.000000
700.000000/1000.000000 ==> Training loss: 4.028839    Training error rate: 94.000000
750.000000/1000.000000 ==> Training loss: 3.705155    Training error rate: 82.000000
800.000000/1000.000000 ==> Training loss: 3.787447    Training error rate: 88.000000
850.000000/1000.000000 ==> Training loss: 3.356160    Training error rate: 88.000000
900.000000/1000.000000 ==> Training loss: 3.684165    Training error rate: 86.000000
950.000000/1000.000000 ==> Training loss: 3.438225    Training error rate: 82.000000
==> Total training loss: 4026.317283    Total training error rate: 92.072000
==> Testing Epoch: 1
0.000000/100.000000 ==> Testing loss: 3.758418    Testing error rate: 86.000000
50.000000/100.000000 ==> Testing loss: 3.650816    Testing error rate: 81.000000
==> Total testing loss: 365.586010    Total testing error rate: 86.580000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 2
0.000000/1000.000000 ==> Training loss: 3.549226    Training error rate: 86.000000
50.000000/1000.000000 ==> Training loss: 3.510678    Training error rate: 88.000000
100.000000/1000.000000 ==> Training loss: 3.433121    Training error rate: 84.000000
150.000000/1000.000000 ==> Training loss: 3.509465    Training error rate: 90.000000
200.000000/1000.000000 ==> Training loss: 3.349941    Training error rate: 82.000000
250.000000/1000.000000 ==> Training loss: 3.458431    Training error rate: 92.000000
300.000000/1000.000000 ==> Training loss: 3.431476    Training error rate: 82.000000
350.000000/1000.000000 ==> Training loss: 3.303687    Training error rate: 78.000000
400.000000/1000.000000 ==> Training loss: 3.742269    Training error rate: 94.000000
450.000000/1000.000000 ==> Training loss: 3.477706    Training error rate: 78.000000
500.000000/1000.000000 ==> Training loss: 3.338876    Training error rate: 86.000000
550.000000/1000.000000 ==> Training loss: 3.078589    Training error rate: 72.000000
600.000000/1000.000000 ==> Training loss: 3.103625    Training error rate: 82.000000
650.000000/1000.000000 ==> Training loss: 3.437057    Training error rate: 82.000000
700.000000/1000.000000 ==> Training loss: 3.287236    Training error rate: 76.000000
750.000000/1000.000000 ==> Training loss: 2.865732    Training error rate: 78.000000
800.000000/1000.000000 ==> Training loss: 3.272567    Training error rate: 82.000000
850.000000/1000.000000 ==> Training loss: 3.279181    Training error rate: 80.000000
900.000000/1000.000000 ==> Training loss: 2.880152    Training error rate: 74.000000
950.000000/1000.000000 ==> Training loss: 3.383698    Training error rate: 72.000000
==> Total training loss: 3359.914930    Total training error rate: 81.912000
==> Testing Epoch: 2
0.000000/100.000000 ==> Testing loss: 3.374336    Testing error rate: 80.000000
50.000000/100.000000 ==> Testing loss: 3.117177    Testing error rate: 74.000000
==> Total testing loss: 315.195562    Total testing error rate: 77.330000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 3
0.000000/1000.000000 ==> Training loss: 3.145865    Training error rate: 74.000000
50.000000/1000.000000 ==> Training loss: 2.870741    Training error rate: 62.000000
100.000000/1000.000000 ==> Training loss: 3.297317    Training error rate: 82.000000
150.000000/1000.000000 ==> Training loss: 3.799235    Training error rate: 84.000000
200.000000/1000.000000 ==> Training loss: 2.678745    Training error rate: 82.000000
250.000000/1000.000000 ==> Training loss: 2.843819    Training error rate: 74.000000
300.000000/1000.000000 ==> Training loss: 3.275985    Training error rate: 78.000000
350.000000/1000.000000 ==> Training loss: 2.756059    Training error rate: 66.000000
400.000000/1000.000000 ==> Training loss: 2.847559    Training error rate: 78.000000
450.000000/1000.000000 ==> Training loss: 2.795875    Training error rate: 72.000000
500.000000/1000.000000 ==> Training loss: 2.741857    Training error rate: 68.000000
550.000000/1000.000000 ==> Training loss: 2.882214    Training error rate: 70.000000
600.000000/1000.000000 ==> Training loss: 2.903807    Training error rate: 74.000000
650.000000/1000.000000 ==> Training loss: 2.969321    Training error rate: 72.000000
700.000000/1000.000000 ==> Training loss: 2.800308    Training error rate: 70.000000
750.000000/1000.000000 ==> Training loss: 2.433994    Training error rate: 68.000000
800.000000/1000.000000 ==> Training loss: 2.547932    Training error rate: 66.000000
850.000000/1000.000000 ==> Training loss: 2.979413    Training error rate: 68.000000
900.000000/1000.000000 ==> Training loss: 2.698562    Training error rate: 64.000000
950.000000/1000.000000 ==> Training loss: 2.946457    Training error rate: 70.000000
==> Total training loss: 2873.056162    Total training error rate: 73.292000
==> Testing Epoch: 3
0.000000/100.000000 ==> Testing loss: 2.909447    Testing error rate: 70.000000
50.000000/100.000000 ==> Testing loss: 2.580161    Testing error rate: 69.000000
==> Total testing loss: 278.947637    Total testing error rate: 70.880000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 4
0.000000/1000.000000 ==> Training loss: 2.840415    Training error rate: 72.000000
50.000000/1000.000000 ==> Training loss: 2.582368    Training error rate: 70.000000
100.000000/1000.000000 ==> Training loss: 2.590998    Training error rate: 66.000000
150.000000/1000.000000 ==> Training loss: 2.331321    Training error rate: 66.000000
200.000000/1000.000000 ==> Training loss: 2.334332    Training error rate: 64.000000
250.000000/1000.000000 ==> Training loss: 2.528769    Training error rate: 68.000000
300.000000/1000.000000 ==> Training loss: 2.625975    Training error rate: 68.000000
350.000000/1000.000000 ==> Training loss: 2.551847    Training error rate: 64.000000
400.000000/1000.000000 ==> Training loss: 2.384376    Training error rate: 62.000000
450.000000/1000.000000 ==> Training loss: 2.799548    Training error rate: 76.000000
500.000000/1000.000000 ==> Training loss: 2.656728    Training error rate: 84.000000
550.000000/1000.000000 ==> Training loss: 2.758385    Training error rate: 68.000000
600.000000/1000.000000 ==> Training loss: 2.019773    Training error rate: 50.000000
650.000000/1000.000000 ==> Training loss: 2.257355    Training error rate: 64.000000
700.000000/1000.000000 ==> Training loss: 2.132658    Training error rate: 52.000000
750.000000/1000.000000 ==> Training loss: 2.271585    Training error rate: 52.000000
800.000000/1000.000000 ==> Training loss: 2.052769    Training error rate: 60.000000
850.000000/1000.000000 ==> Training loss: 2.125876    Training error rate: 54.000000
900.000000/1000.000000 ==> Training loss: 2.733238    Training error rate: 72.000000
950.000000/1000.000000 ==> Training loss: 2.218821    Training error rate: 58.000000
==> Total training loss: 2457.501722    Total training error rate: 64.844000
==> Testing Epoch: 4
0.000000/100.000000 ==> Testing loss: 2.381531    Testing error rate: 62.000000
50.000000/100.000000 ==> Testing loss: 2.129648    Testing error rate: 52.000000
==> Total testing loss: 228.611561    Total testing error rate: 60.640000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 5
0.000000/1000.000000 ==> Training loss: 2.190807    Training error rate: 58.000000
50.000000/1000.000000 ==> Training loss: 1.896788    Training error rate: 56.000000
100.000000/1000.000000 ==> Training loss: 2.148231    Training error rate: 58.000000
150.000000/1000.000000 ==> Training loss: 2.085775    Training error rate: 62.000000
200.000000/1000.000000 ==> Training loss: 2.178115    Training error rate: 58.000000
250.000000/1000.000000 ==> Training loss: 2.192413    Training error rate: 60.000000
300.000000/1000.000000 ==> Training loss: 2.155362    Training error rate: 54.000000
350.000000/1000.000000 ==> Training loss: 2.115980    Training error rate: 56.000000
400.000000/1000.000000 ==> Training loss: 2.430694    Training error rate: 66.000000
450.000000/1000.000000 ==> Training loss: 2.306495    Training error rate: 64.000000
500.000000/1000.000000 ==> Training loss: 2.201328    Training error rate: 62.000000
550.000000/1000.000000 ==> Training loss: 1.843375    Training error rate: 52.000000
600.000000/1000.000000 ==> Training loss: 2.112009    Training error rate: 58.000000
650.000000/1000.000000 ==> Training loss: 1.933476    Training error rate: 60.000000
700.000000/1000.000000 ==> Training loss: 2.056706    Training error rate: 44.000000
750.000000/1000.000000 ==> Training loss: 1.873052    Training error rate: 58.000000
800.000000/1000.000000 ==> Training loss: 2.080417    Training error rate: 64.000000
850.000000/1000.000000 ==> Training loss: 2.575918    Training error rate: 72.000000
900.000000/1000.000000 ==> Training loss: 2.106843    Training error rate: 58.000000
950.000000/1000.000000 ==> Training loss: 1.903697    Training error rate: 56.000000
==> Total training loss: 2151.165540    Total training error rate: 58.320000
==> Testing Epoch: 5
0.000000/100.000000 ==> Testing loss: 2.217825    Testing error rate: 56.000000
50.000000/100.000000 ==> Testing loss: 2.022952    Testing error rate: 55.000000
==> Total testing loss: 209.899406    Total testing error rate: 55.800000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 6
0.000000/1000.000000 ==> Training loss: 2.059193    Training error rate: 54.000000
50.000000/1000.000000 ==> Training loss: 2.071478    Training error rate: 54.000000
100.000000/1000.000000 ==> Training loss: 2.033415    Training error rate: 60.000000
150.000000/1000.000000 ==> Training loss: 1.547388    Training error rate: 44.000000
200.000000/1000.000000 ==> Training loss: 1.880664    Training error rate: 46.000000
250.000000/1000.000000 ==> Training loss: 2.022408    Training error rate: 50.000000
300.000000/1000.000000 ==> Training loss: 2.078007    Training error rate: 56.000000
350.000000/1000.000000 ==> Training loss: 1.965082    Training error rate: 54.000000
400.000000/1000.000000 ==> Training loss: 2.158001    Training error rate: 58.000000
450.000000/1000.000000 ==> Training loss: 1.924524    Training error rate: 44.000000
500.000000/1000.000000 ==> Training loss: 1.738860    Training error rate: 52.000000
550.000000/1000.000000 ==> Training loss: 1.860035    Training error rate: 54.000000
600.000000/1000.000000 ==> Training loss: 2.002609    Training error rate: 54.000000
650.000000/1000.000000 ==> Training loss: 1.760525    Training error rate: 50.000000
700.000000/1000.000000 ==> Training loss: 1.627587    Training error rate: 56.000000
750.000000/1000.000000 ==> Training loss: 2.063627    Training error rate: 60.000000
800.000000/1000.000000 ==> Training loss: 1.956282    Training error rate: 60.000000
850.000000/1000.000000 ==> Training loss: 1.778941    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.968885    Training error rate: 62.000000
950.000000/1000.000000 ==> Training loss: 1.872229    Training error rate: 58.000000
==> Total training loss: 1933.528420    Total training error rate: 53.426000
==> Testing Epoch: 6
0.000000/100.000000 ==> Testing loss: 1.913968    Testing error rate: 47.000000
50.000000/100.000000 ==> Testing loss: 1.744333    Testing error rate: 45.000000
==> Total testing loss: 188.599868    Total testing error rate: 50.630000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 7
0.000000/1000.000000 ==> Training loss: 1.655617    Training error rate: 46.000000
50.000000/1000.000000 ==> Training loss: 2.058225    Training error rate: 58.000000
100.000000/1000.000000 ==> Training loss: 1.641984    Training error rate: 44.000000
150.000000/1000.000000 ==> Training loss: 1.519437    Training error rate: 48.000000
200.000000/1000.000000 ==> Training loss: 1.335865    Training error rate: 36.000000
250.000000/1000.000000 ==> Training loss: 1.371627    Training error rate: 44.000000
300.000000/1000.000000 ==> Training loss: 1.876528    Training error rate: 54.000000
350.000000/1000.000000 ==> Training loss: 1.786777    Training error rate: 46.000000
400.000000/1000.000000 ==> Training loss: 1.516261    Training error rate: 48.000000
450.000000/1000.000000 ==> Training loss: 1.786289    Training error rate: 50.000000
500.000000/1000.000000 ==> Training loss: 1.638345    Training error rate: 40.000000
550.000000/1000.000000 ==> Training loss: 1.813026    Training error rate: 48.000000
600.000000/1000.000000 ==> Training loss: 1.816995    Training error rate: 62.000000
650.000000/1000.000000 ==> Training loss: 1.644472    Training error rate: 54.000000
700.000000/1000.000000 ==> Training loss: 1.601784    Training error rate: 42.000000
750.000000/1000.000000 ==> Training loss: 1.614946    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.718328    Training error rate: 54.000000
850.000000/1000.000000 ==> Training loss: 1.503470    Training error rate: 42.000000
900.000000/1000.000000 ==> Training loss: 1.878428    Training error rate: 54.000000
950.000000/1000.000000 ==> Training loss: 1.960265    Training error rate: 50.000000
==> Total training loss: 1747.794708    Total training error rate: 48.896000
==> Testing Epoch: 7
0.000000/100.000000 ==> Testing loss: 1.839888    Testing error rate: 52.000000
50.000000/100.000000 ==> Testing loss: 1.753475    Testing error rate: 45.000000
==> Total testing loss: 182.689208    Total testing error rate: 49.470000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 8
0.000000/1000.000000 ==> Training loss: 2.101798    Training error rate: 50.000000
50.000000/1000.000000 ==> Training loss: 1.694197    Training error rate: 52.000000
100.000000/1000.000000 ==> Training loss: 1.381680    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 1.221705    Training error rate: 40.000000
200.000000/1000.000000 ==> Training loss: 1.771448    Training error rate: 56.000000
250.000000/1000.000000 ==> Training loss: 1.331762    Training error rate: 44.000000
300.000000/1000.000000 ==> Training loss: 1.735250    Training error rate: 44.000000
350.000000/1000.000000 ==> Training loss: 1.564412    Training error rate: 48.000000
400.000000/1000.000000 ==> Training loss: 1.583546    Training error rate: 48.000000
450.000000/1000.000000 ==> Training loss: 1.592473    Training error rate: 50.000000
500.000000/1000.000000 ==> Training loss: 1.602143    Training error rate: 38.000000
550.000000/1000.000000 ==> Training loss: 1.400313    Training error rate: 38.000000
600.000000/1000.000000 ==> Training loss: 1.742413    Training error rate: 48.000000
650.000000/1000.000000 ==> Training loss: 1.781681    Training error rate: 56.000000
700.000000/1000.000000 ==> Training loss: 1.934592    Training error rate: 56.000000
750.000000/1000.000000 ==> Training loss: 1.280964    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 2.093398    Training error rate: 48.000000
850.000000/1000.000000 ==> Training loss: 1.373587    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.557337    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.478017    Training error rate: 42.000000
==> Total training loss: 1607.199916    Total training error rate: 45.666000
==> Testing Epoch: 8
0.000000/100.000000 ==> Testing loss: 1.741315    Testing error rate: 43.000000
50.000000/100.000000 ==> Testing loss: 1.704606    Testing error rate: 47.000000
==> Total testing loss: 176.907435    Total testing error rate: 47.950000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 9
0.000000/1000.000000 ==> Training loss: 1.712361    Training error rate: 48.000000
50.000000/1000.000000 ==> Training loss: 1.524778    Training error rate: 38.000000
100.000000/1000.000000 ==> Training loss: 1.393590    Training error rate: 38.000000
150.000000/1000.000000 ==> Training loss: 1.836840    Training error rate: 44.000000
200.000000/1000.000000 ==> Training loss: 1.573191    Training error rate: 44.000000
250.000000/1000.000000 ==> Training loss: 1.445380    Training error rate: 40.000000
300.000000/1000.000000 ==> Training loss: 1.783751    Training error rate: 52.000000
350.000000/1000.000000 ==> Training loss: 1.687281    Training error rate: 52.000000
400.000000/1000.000000 ==> Training loss: 1.481537    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.447264    Training error rate: 38.000000
500.000000/1000.000000 ==> Training loss: 1.425301    Training error rate: 42.000000
550.000000/1000.000000 ==> Training loss: 1.473886    Training error rate: 44.000000
600.000000/1000.000000 ==> Training loss: 1.815346    Training error rate: 46.000000
650.000000/1000.000000 ==> Training loss: 1.521102    Training error rate: 38.000000
700.000000/1000.000000 ==> Training loss: 1.798823    Training error rate: 46.000000
750.000000/1000.000000 ==> Training loss: 1.337845    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.619673    Training error rate: 42.000000
850.000000/1000.000000 ==> Training loss: 1.812687    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.627762    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.633768    Training error rate: 54.000000
==> Total training loss: 1496.333617    Total training error rate: 42.558000
==> Testing Epoch: 9
0.000000/100.000000 ==> Testing loss: 1.805897    Testing error rate: 46.000000
50.000000/100.000000 ==> Testing loss: 1.769310    Testing error rate: 45.000000
==> Total testing loss: 181.217760    Total testing error rate: 48.140000
==> Set learning rate: 0.010000
==> Training Epoch: 10
0.000000/1000.000000 ==> Training loss: 1.308221    Training error rate: 42.000000
50.000000/1000.000000 ==> Training loss: 1.410322    Training error rate: 42.000000
100.000000/1000.000000 ==> Training loss: 1.523581    Training error rate: 40.000000
150.000000/1000.000000 ==> Training loss: 1.351217    Training error rate: 46.000000
200.000000/1000.000000 ==> Training loss: 1.427944    Training error rate: 42.000000
250.000000/1000.000000 ==> Training loss: 1.510982    Training error rate: 46.000000
300.000000/1000.000000 ==> Training loss: 1.980562    Training error rate: 48.000000
350.000000/1000.000000 ==> Training loss: 1.344460    Training error rate: 34.000000
400.000000/1000.000000 ==> Training loss: 1.590798    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.264358    Training error rate: 42.000000
500.000000/1000.000000 ==> Training loss: 1.392556    Training error rate: 38.000000
550.000000/1000.000000 ==> Training loss: 1.337679    Training error rate: 44.000000
600.000000/1000.000000 ==> Training loss: 1.452666    Training error rate: 44.000000
650.000000/1000.000000 ==> Training loss: 1.705950    Training error rate: 48.000000
700.000000/1000.000000 ==> Training loss: 1.303423    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 1.479489    Training error rate: 44.000000
800.000000/1000.000000 ==> Training loss: 1.039966    Training error rate: 28.000000
850.000000/1000.000000 ==> Training loss: 1.525991    Training error rate: 44.000000
900.000000/1000.000000 ==> Training loss: 1.504688    Training error rate: 48.000000
950.000000/1000.000000 ==> Training loss: 1.546789    Training error rate: 38.000000
==> Total training loss: 1395.739790    Total training error rate: 40.480000
==> Testing Epoch: 10
0.000000/100.000000 ==> Testing loss: 1.877012    Testing error rate: 49.000000
50.000000/100.000000 ==> Testing loss: 1.507011    Testing error rate: 44.000000
==> Total testing loss: 173.279470    Total testing error rate: 45.880000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 11
0.000000/1000.000000 ==> Training loss: 1.311020    Training error rate: 36.000000
50.000000/1000.000000 ==> Training loss: 1.559515    Training error rate: 48.000000
100.000000/1000.000000 ==> Training loss: 1.627589    Training error rate: 48.000000
150.000000/1000.000000 ==> Training loss: 1.203147    Training error rate: 36.000000
200.000000/1000.000000 ==> Training loss: 1.323186    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 1.649683    Training error rate: 52.000000
300.000000/1000.000000 ==> Training loss: 1.096161    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.966470    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 1.298168    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.596766    Training error rate: 44.000000
500.000000/1000.000000 ==> Training loss: 1.203436    Training error rate: 28.000000
550.000000/1000.000000 ==> Training loss: 1.087285    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 1.256607    Training error rate: 40.000000
650.000000/1000.000000 ==> Training loss: 1.551264    Training error rate: 50.000000
700.000000/1000.000000 ==> Training loss: 1.185382    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 1.462989    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.505102    Training error rate: 48.000000
850.000000/1000.000000 ==> Training loss: 1.326134    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 1.340112    Training error rate: 46.000000
950.000000/1000.000000 ==> Training loss: 1.878640    Training error rate: 60.000000
==> Total training loss: 1301.646671    Total training error rate: 37.654000
==> Testing Epoch: 11
0.000000/100.000000 ==> Testing loss: 1.623534    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.388697    Testing error rate: 37.000000
==> Total testing loss: 160.190827    Total testing error rate: 43.100000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 12
0.000000/1000.000000 ==> Training loss: 1.038240    Training error rate: 30.000000
50.000000/1000.000000 ==> Training loss: 0.933917    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 1.024173    Training error rate: 30.000000
150.000000/1000.000000 ==> Training loss: 1.391266    Training error rate: 38.000000
200.000000/1000.000000 ==> Training loss: 1.417287    Training error rate: 36.000000
250.000000/1000.000000 ==> Training loss: 1.024571    Training error rate: 34.000000
300.000000/1000.000000 ==> Training loss: 1.357576    Training error rate: 44.000000
350.000000/1000.000000 ==> Training loss: 0.931764    Training error rate: 32.000000
400.000000/1000.000000 ==> Training loss: 1.281607    Training error rate: 42.000000
450.000000/1000.000000 ==> Training loss: 1.186679    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 1.235408    Training error rate: 32.000000
550.000000/1000.000000 ==> Training loss: 1.012309    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 1.035232    Training error rate: 26.000000
650.000000/1000.000000 ==> Training loss: 1.675502    Training error rate: 48.000000
700.000000/1000.000000 ==> Training loss: 1.349720    Training error rate: 40.000000
750.000000/1000.000000 ==> Training loss: 0.894750    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 1.016231    Training error rate: 32.000000
850.000000/1000.000000 ==> Training loss: 1.287959    Training error rate: 38.000000
900.000000/1000.000000 ==> Training loss: 1.356178    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.030539    Training error rate: 34.000000
==> Total training loss: 1227.321164    Total training error rate: 35.586000
==> Testing Epoch: 12
0.000000/100.000000 ==> Testing loss: 1.326007    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.435211    Testing error rate: 40.000000
==> Total testing loss: 160.022756    Total testing error rate: 43.350000
==> Set learning rate: 0.010000
==> Training Epoch: 13
0.000000/1000.000000 ==> Training loss: 1.018544    Training error rate: 32.000000
50.000000/1000.000000 ==> Training loss: 1.258538    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 1.665256    Training error rate: 46.000000
150.000000/1000.000000 ==> Training loss: 1.054802    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.118211    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 1.119600    Training error rate: 34.000000
300.000000/1000.000000 ==> Training loss: 1.448340    Training error rate: 42.000000
350.000000/1000.000000 ==> Training loss: 1.036237    Training error rate: 40.000000
400.000000/1000.000000 ==> Training loss: 1.305989    Training error rate: 44.000000
450.000000/1000.000000 ==> Training loss: 1.126205    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 1.507743    Training error rate: 36.000000
550.000000/1000.000000 ==> Training loss: 1.176796    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 1.171439    Training error rate: 32.000000
650.000000/1000.000000 ==> Training loss: 1.072597    Training error rate: 42.000000
700.000000/1000.000000 ==> Training loss: 1.142354    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 1.275478    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.319710    Training error rate: 40.000000
850.000000/1000.000000 ==> Training loss: 1.223216    Training error rate: 40.000000
900.000000/1000.000000 ==> Training loss: 0.938066    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.087273    Training error rate: 40.000000
==> Total training loss: 1159.671479    Total training error rate: 34.186000
==> Testing Epoch: 13
0.000000/100.000000 ==> Testing loss: 1.461160    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.298147    Testing error rate: 35.000000
==> Total testing loss: 145.261071    Total testing error rate: 39.920000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 14
0.000000/1000.000000 ==> Training loss: 1.118261    Training error rate: 40.000000
50.000000/1000.000000 ==> Training loss: 1.124757    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 1.239783    Training error rate: 32.000000
150.000000/1000.000000 ==> Training loss: 1.187918    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 1.061010    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 0.839640    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 1.011726    Training error rate: 38.000000
350.000000/1000.000000 ==> Training loss: 1.097311    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 1.190576    Training error rate: 30.000000
450.000000/1000.000000 ==> Training loss: 0.762178    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.708490    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 1.111733    Training error rate: 34.000000
600.000000/1000.000000 ==> Training loss: 1.046133    Training error rate: 36.000000
650.000000/1000.000000 ==> Training loss: 1.255941    Training error rate: 40.000000
700.000000/1000.000000 ==> Training loss: 0.938072    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 1.017438    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 1.218119    Training error rate: 34.000000
850.000000/1000.000000 ==> Training loss: 0.997944    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 1.173154    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.205728    Training error rate: 38.000000
==> Total training loss: 1099.191433    Total training error rate: 32.438000
==> Testing Epoch: 14
0.000000/100.000000 ==> Testing loss: 1.375886    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.271874    Testing error rate: 37.000000
==> Total testing loss: 139.792277    Total testing error rate: 39.170000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 15
0.000000/1000.000000 ==> Training loss: 1.221613    Training error rate: 38.000000
50.000000/1000.000000 ==> Training loss: 0.560222    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.772762    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.710524    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.721321    Training error rate: 28.000000
250.000000/1000.000000 ==> Training loss: 1.056720    Training error rate: 30.000000
300.000000/1000.000000 ==> Training loss: 0.965970    Training error rate: 30.000000
350.000000/1000.000000 ==> Training loss: 1.331462    Training error rate: 36.000000
400.000000/1000.000000 ==> Training loss: 1.356863    Training error rate: 38.000000
450.000000/1000.000000 ==> Training loss: 1.071047    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 1.132167    Training error rate: 34.000000
550.000000/1000.000000 ==> Training loss: 1.021435    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 1.139271    Training error rate: 36.000000
650.000000/1000.000000 ==> Training loss: 0.996715    Training error rate: 32.000000
700.000000/1000.000000 ==> Training loss: 1.033458    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.971920    Training error rate: 34.000000
800.000000/1000.000000 ==> Training loss: 1.060618    Training error rate: 38.000000
850.000000/1000.000000 ==> Training loss: 1.182297    Training error rate: 40.000000
900.000000/1000.000000 ==> Training loss: 1.509563    Training error rate: 52.000000
950.000000/1000.000000 ==> Training loss: 0.985599    Training error rate: 30.000000
==> Total training loss: 1047.713929    Total training error rate: 31.170000
==> Testing Epoch: 15
0.000000/100.000000 ==> Testing loss: 1.496948    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.263095    Testing error rate: 36.000000
==> Total testing loss: 144.440971    Total testing error rate: 40.150000
==> Set learning rate: 0.010000
==> Training Epoch: 16
0.000000/1000.000000 ==> Training loss: 1.254057    Training error rate: 34.000000
50.000000/1000.000000 ==> Training loss: 0.903395    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.769630    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.850839    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.040989    Training error rate: 28.000000
250.000000/1000.000000 ==> Training loss: 0.819836    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.714233    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.939802    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.864940    Training error rate: 26.000000
450.000000/1000.000000 ==> Training loss: 1.144696    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.938208    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 1.097088    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 1.327735    Training error rate: 38.000000
650.000000/1000.000000 ==> Training loss: 1.081001    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 1.002182    Training error rate: 24.000000
750.000000/1000.000000 ==> Training loss: 0.763017    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 1.112584    Training error rate: 36.000000
850.000000/1000.000000 ==> Training loss: 1.136391    Training error rate: 36.000000
900.000000/1000.000000 ==> Training loss: 1.290845    Training error rate: 34.000000
950.000000/1000.000000 ==> Training loss: 0.902988    Training error rate: 26.000000
==> Total training loss: 990.005387    Total training error rate: 29.318000
==> Testing Epoch: 16
0.000000/100.000000 ==> Testing loss: 1.367854    Testing error rate: 40.000000
50.000000/100.000000 ==> Testing loss: 1.418253    Testing error rate: 33.000000
==> Total testing loss: 140.218684    Total testing error rate: 38.570000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 17
0.000000/1000.000000 ==> Training loss: 1.055174    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.886549    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 1.048889    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.783463    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.717315    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.899912    Training error rate: 28.000000
300.000000/1000.000000 ==> Training loss: 1.212397    Training error rate: 40.000000
350.000000/1000.000000 ==> Training loss: 0.936780    Training error rate: 24.000000
400.000000/1000.000000 ==> Training loss: 1.002606    Training error rate: 30.000000
450.000000/1000.000000 ==> Training loss: 0.864311    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 0.774292    Training error rate: 18.000000
550.000000/1000.000000 ==> Training loss: 1.071334    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.527526    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.781021    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 1.149153    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.701995    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 1.130402    Training error rate: 40.000000
850.000000/1000.000000 ==> Training loss: 1.261313    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 0.936168    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.922013    Training error rate: 30.000000
==> Total training loss: 948.588511    Total training error rate: 28.398000
==> Testing Epoch: 17
0.000000/100.000000 ==> Testing loss: 1.285158    Testing error rate: 41.000000
50.000000/100.000000 ==> Testing loss: 1.460227    Testing error rate: 37.000000
==> Total testing loss: 134.745520    Total testing error rate: 37.230000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 18
0.000000/1000.000000 ==> Training loss: 0.926093    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.796084    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.979270    Training error rate: 28.000000
150.000000/1000.000000 ==> Training loss: 1.031440    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 0.853587    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 0.909536    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.769329    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.512024    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 1.016944    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.944859    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.733042    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.744720    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 1.094880    Training error rate: 28.000000
650.000000/1000.000000 ==> Training loss: 1.038406    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 1.081145    Training error rate: 28.000000
750.000000/1000.000000 ==> Training loss: 0.731375    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 1.187885    Training error rate: 36.000000
850.000000/1000.000000 ==> Training loss: 0.683541    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.874115    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.047111    Training error rate: 38.000000
==> Total training loss: 904.076424    Total training error rate: 27.182000
==> Testing Epoch: 18
0.000000/100.000000 ==> Testing loss: 1.441281    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.389001    Testing error rate: 38.000000
==> Total testing loss: 137.738122    Total testing error rate: 37.410000
==> Set learning rate: 0.010000
==> Training Epoch: 19
0.000000/1000.000000 ==> Training loss: 0.773415    Training error rate: 28.000000
50.000000/1000.000000 ==> Training loss: 0.827497    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 0.632235    Training error rate: 24.000000
150.000000/1000.000000 ==> Training loss: 0.971510    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 0.541616    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.711499    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 1.100118    Training error rate: 46.000000
350.000000/1000.000000 ==> Training loss: 0.869232    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 0.708827    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.699850    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 1.015908    Training error rate: 28.000000
550.000000/1000.000000 ==> Training loss: 0.772177    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.835308    Training error rate: 22.000000
650.000000/1000.000000 ==> Training loss: 1.300890    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 0.876947    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 0.829903    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 0.835801    Training error rate: 32.000000
850.000000/1000.000000 ==> Training loss: 1.057488    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 0.781430    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.949653    Training error rate: 24.000000
==> Total training loss: 876.082728    Total training error rate: 26.468000
==> Testing Epoch: 19
0.000000/100.000000 ==> Testing loss: 1.321077    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.412123    Testing error rate: 34.000000
==> Total testing loss: 136.702347    Total testing error rate: 37.090000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 20
0.000000/1000.000000 ==> Training loss: 0.767655    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.823184    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 1.076606    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.674958    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.990246    Training error rate: 30.000000
250.000000/1000.000000 ==> Training loss: 0.617291    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.708325    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.889147    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.828724    Training error rate: 28.000000
450.000000/1000.000000 ==> Training loss: 0.751438    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.649178    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.726906    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.753827    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.808747    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 1.222787    Training error rate: 38.000000
750.000000/1000.000000 ==> Training loss: 0.877584    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.855438    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.967661    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.755784    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.551310    Training error rate: 8.000000
==> Total training loss: 833.038798    Total training error rate: 25.192000
==> Testing Epoch: 20
0.000000/100.000000 ==> Testing loss: 1.455191    Testing error rate: 41.000000
50.000000/100.000000 ==> Testing loss: 1.462273    Testing error rate: 39.000000
==> Total testing loss: 140.230237    Total testing error rate: 37.290000
==> Set learning rate: 0.010000
==> Training Epoch: 21
0.000000/1000.000000 ==> Training loss: 0.811763    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.613706    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.926985    Training error rate: 30.000000
150.000000/1000.000000 ==> Training loss: 0.687967    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.678507    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.755387    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.803285    Training error rate: 32.000000
350.000000/1000.000000 ==> Training loss: 0.670693    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 1.011418    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.591748    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.780168    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 0.664683    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 1.012379    Training error rate: 30.000000
650.000000/1000.000000 ==> Training loss: 0.767051    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.710810    Training error rate: 24.000000
750.000000/1000.000000 ==> Training loss: 0.566991    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.751992    Training error rate: 20.000000
850.000000/1000.000000 ==> Training loss: 0.836663    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 1.178255    Training error rate: 30.000000
950.000000/1000.000000 ==> Training loss: 0.693405    Training error rate: 24.000000
==> Total training loss: 805.103362    Total training error rate: 24.656000
==> Testing Epoch: 21
0.000000/100.000000 ==> Testing loss: 1.480474    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.286268    Testing error rate: 31.000000
==> Total testing loss: 138.010314    Total testing error rate: 36.940000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 22
0.000000/1000.000000 ==> Training loss: 0.385826    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.302237    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.778906    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.641563    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.672638    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.530859    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.903176    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.642352    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.865655    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.853225    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.812954    Training error rate: 32.000000
550.000000/1000.000000 ==> Training loss: 1.030781    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.781286    Training error rate: 24.000000
650.000000/1000.000000 ==> Training loss: 0.531625    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.764217    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.714585    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 1.226999    Training error rate: 42.000000
850.000000/1000.000000 ==> Training loss: 0.694048    Training error rate: 26.000000
900.000000/1000.000000 ==> Training loss: 0.824467    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.659504    Training error rate: 18.000000
==> Total training loss: 773.233685    Total training error rate: 23.638000
==> Testing Epoch: 22
0.000000/100.000000 ==> Testing loss: 1.505965    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.510814    Testing error rate: 33.000000
==> Total testing loss: 135.249002    Total testing error rate: 35.820000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 23
0.000000/1000.000000 ==> Training loss: 0.941841    Training error rate: 28.000000
50.000000/1000.000000 ==> Training loss: 0.799043    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.523504    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.562729    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.922137    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.944953    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.811191    Training error rate: 20.000000
350.000000/1000.000000 ==> Training loss: 0.694753    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.621766    Training error rate: 22.000000
450.000000/1000.000000 ==> Training loss: 0.710219    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.408593    Training error rate: 12.000000
550.000000/1000.000000 ==> Training loss: 0.609686    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.504516    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.899537    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 0.951560    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.618123    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.662444    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.966857    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.793802    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 1.061447    Training error rate: 30.000000
==> Total training loss: 747.542327    Total training error rate: 22.684000
==> Testing Epoch: 23
0.000000/100.000000 ==> Testing loss: 1.350777    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.434296    Testing error rate: 32.000000
==> Total testing loss: 131.407366    Total testing error rate: 34.600000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 24
0.000000/1000.000000 ==> Training loss: 0.604323    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.856048    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.563504    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.588071    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.483137    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.747936    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.987395    Training error rate: 32.000000
350.000000/1000.000000 ==> Training loss: 0.644239    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.584988    Training error rate: 22.000000
450.000000/1000.000000 ==> Training loss: 0.668496    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.608880    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.682369    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.630751    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.753236    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.949480    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.907636    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.943212    Training error rate: 30.000000
850.000000/1000.000000 ==> Training loss: 1.020969    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.845161    Training error rate: 28.000000
950.000000/1000.000000 ==> Training loss: 0.587399    Training error rate: 18.000000
==> Total training loss: 727.030024    Total training error rate: 22.458000
==> Testing Epoch: 24
0.000000/100.000000 ==> Testing loss: 1.381206    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.042953    Testing error rate: 30.000000
==> Total testing loss: 130.848066    Total testing error rate: 34.620000
==> Set learning rate: 0.010000
==> Training Epoch: 25
0.000000/1000.000000 ==> Training loss: 0.655942    Training error rate: 32.000000
50.000000/1000.000000 ==> Training loss: 0.641364    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.539858    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.518427    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.552606    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.680367    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.731961    Training error rate: 22.000000
350.000000/1000.000000 ==> Training loss: 0.944749    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.571494    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.505482    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.504386    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.896310    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.705431    Training error rate: 26.000000
650.000000/1000.000000 ==> Training loss: 0.792656    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.789050    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 0.597255    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.866821    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.579395    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.794953    Training error rate: 28.000000
950.000000/1000.000000 ==> Training loss: 0.731680    Training error rate: 24.000000
==> Total training loss: 698.406479    Total training error rate: 21.556000
==> Testing Epoch: 25
0.000000/100.000000 ==> Testing loss: 1.321187    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.090315    Testing error rate: 29.000000
==> Total testing loss: 127.379310    Total testing error rate: 34.670000
==> Set learning rate: 0.010000
==> Training Epoch: 26
0.000000/1000.000000 ==> Training loss: 0.697295    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.644665    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.567076    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.405677    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.496837    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.559085    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.541182    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.547433    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.728428    Training error rate: 20.000000
450.000000/1000.000000 ==> Training loss: 0.808685    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.722217    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.726590    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.594902    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.914522    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 0.881229    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.794620    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.932929    Training error rate: 34.000000
850.000000/1000.000000 ==> Training loss: 0.725854    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.634211    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.810605    Training error rate: 30.000000
==> Total training loss: 676.413966    Total training error rate: 20.674000
==> Testing Epoch: 26
0.000000/100.000000 ==> Testing loss: 1.136712    Testing error rate: 30.000000
50.000000/100.000000 ==> Testing loss: 1.198634    Testing error rate: 29.000000
==> Total testing loss: 127.802508    Total testing error rate: 33.790000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 27
0.000000/1000.000000 ==> Training loss: 0.637743    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.551723    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 0.610010    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.607348    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.570131    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.583517    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.595628    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.651267    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.467685    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.707869    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.679716    Training error rate: 26.000000
550.000000/1000.000000 ==> Training loss: 0.610795    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.791844    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.653148    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.767559    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.812972    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.642997    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.416368    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.686387    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.812223    Training error rate: 24.000000
==> Total training loss: 650.128888    Total training error rate: 20.248000
==> Testing Epoch: 27
0.000000/100.000000 ==> Testing loss: 1.193094    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.400613    Testing error rate: 34.000000
==> Total testing loss: 135.267944    Total testing error rate: 35.520000
==> Set learning rate: 0.010000
==> Training Epoch: 28
0.000000/1000.000000 ==> Training loss: 0.640792    Training error rate: 20.000000
50.000000/1000.000000 ==> Training loss: 0.577937    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.480967    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.599682    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.601168    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.877462    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 0.402524    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.676769    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.633084    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.767712    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.781782    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.754575    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.848044    Training error rate: 28.000000
650.000000/1000.000000 ==> Training loss: 0.378646    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.576601    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.640841    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.409048    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.735445    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.834494    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.691417    Training error rate: 22.000000
==> Total training loss: 641.710395    Total training error rate: 19.792000
==> Testing Epoch: 28
0.000000/100.000000 ==> Testing loss: 1.379735    Testing error rate: 33.000000
50.000000/100.000000 ==> Testing loss: 1.126057    Testing error rate: 28.000000
==> Total testing loss: 129.511713    Total testing error rate: 34.020000
==> Set learning rate: 0.010000
==> Training Epoch: 29
0.000000/1000.000000 ==> Training loss: 0.575846    Training error rate: 20.000000
50.000000/1000.000000 ==> Training loss: 0.522687    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.612530    Training error rate: 20.000000
150.000000/1000.000000 ==> Training loss: 0.484939    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.615149    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.590311    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.375446    Training error rate: 8.000000
350.000000/1000.000000 ==> Training loss: 0.530687    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.564189    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.462727    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.557025    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.858578    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.750054    Training error rate: 22.000000
650.000000/1000.000000 ==> Training loss: 0.747648    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.704276    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.467301    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.748628    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.761602    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.534158    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.657249    Training error rate: 24.000000
==> Total training loss: 616.169580    Total training error rate: 19.128000
==> Testing Epoch: 29
0.000000/100.000000 ==> Testing loss: 1.219901    Testing error rate: 31.000000
50.000000/100.000000 ==> Testing loss: 1.380697    Testing error rate: 40.000000
==> Total testing loss: 127.717714    Total testing error rate: 33.740000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 30
0.000000/1000.000000 ==> Training loss: 0.300561    Training error rate: 4.000000
50.000000/1000.000000 ==> Training loss: 0.698130    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.506481    Training error rate: 22.000000
150.000000/1000.000000 ==> Training loss: 0.419953    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.843976    Training error rate: 22.000000
250.000000/1000.000000 ==> Training loss: 0.270207    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.737075    Training error rate: 18.000000
350.000000/1000.000000 ==> Training loss: 0.652305    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.602375    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.645290    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.829702    Training error rate: 30.000000
550.000000/1000.000000 ==> Training loss: 0.546342    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.506395    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.622368    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.788734    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.638477    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.632868    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.650862    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.548122    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.687617    Training error rate: 22.000000
==> Total training loss: 610.352457    Total training error rate: 18.886000
==> Testing Epoch: 30
0.000000/100.000000 ==> Testing loss: 1.182697    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.471372    Testing error rate: 40.000000
==> Total testing loss: 133.991861    Total testing error rate: 34.240000
==> Set learning rate: 0.010000
==> Training Epoch: 31
0.000000/1000.000000 ==> Training loss: 0.667576    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.514208    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.617630    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.480765    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.610526    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.468969    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.945008    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.581384    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.363959    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.758432    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.634695    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.553093    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.556681    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.646187    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.553672    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.533663    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.602935    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.576443    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.706883    Training error rate: 30.000000
950.000000/1000.000000 ==> Training loss: 0.795737    Training error rate: 24.000000
==> Total training loss: 592.895187    Total training error rate: 18.432000
==> Testing Epoch: 31
0.000000/100.000000 ==> Testing loss: 1.279816    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.536136    Testing error rate: 35.000000
==> Total testing loss: 135.527912    Total testing error rate: 34.780000
==> Set learning rate: 0.010000
==> Training Epoch: 32
0.000000/1000.000000 ==> Training loss: 0.615069    Training error rate: 22.000000
50.000000/1000.000000 ==> Training loss: 0.669840    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.425765    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.628283    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.446802    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.461664    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.659031    Training error rate: 18.000000
350.000000/1000.000000 ==> Training loss: 0.471534    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.554291    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.821254    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 0.500932    Training error rate: 10.000000
550.000000/1000.000000 ==> Training loss: 0.622824    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.479912    Training error rate: 18.000000
650.000000/1000.000000 ==> Training loss: 0.642036    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.644528    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.835607    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.779741    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.658465    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.549923    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.571006    Training error rate: 14.000000
==> Total training loss: 578.608277    Total training error rate: 17.968000
==> Testing Epoch: 32
0.000000/100.000000 ==> Testing loss: 1.331419    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.472029    Testing error rate: 38.000000
==> Total testing loss: 133.881347    Total testing error rate: 34.920000
==> Set learning rate: 0.010000
==> Training Epoch: 33
0.000000/1000.000000 ==> Training loss: 0.376245    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.474759    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.368760    Training error rate: 8.000000
150.000000/1000.000000 ==> Training loss: 0.609937    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.443883    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.509917    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.451320    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.758068    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.477627    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.830653    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.529440    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 0.831604    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 0.704390    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.571816    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.589993    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.776993    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 0.440612    Training error rate: 10.000000
850.000000/1000.000000 ==> Training loss: 0.709979    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.767564    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.486793    Training error rate: 12.000000
==> Total training loss: 568.747226    Total training error rate: 17.668000
==> Testing Epoch: 33
0.000000/100.000000 ==> Testing loss: 1.496711    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.248182    Testing error rate: 35.000000
==> Total testing loss: 137.825184    Total testing error rate: 34.550000
==> Set learning rate: 0.010000
==> Training Epoch: 34
0.000000/1000.000000 ==> Training loss: 0.599273    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.453589    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.559413    Training error rate: 24.000000
150.000000/1000.000000 ==> Training loss: 0.362409    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.635144    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.385949    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.546547    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.433191    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.601622    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.451471    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.572335    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.392155    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.567442    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.609029    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.528576    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.440651    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.465503    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.741505    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.808153    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.722620    Training error rate: 24.000000
==> Total training loss: 544.274256    Total training error rate: 16.796000
==> Testing Epoch: 34
0.000000/100.000000 ==> Testing loss: 1.091064    Testing error rate: 30.000000
50.000000/100.000000 ==> Testing loss: 1.441337    Testing error rate: 34.000000
==> Total testing loss: 130.364300    Total testing error rate: 33.100000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 35
0.000000/1000.000000 ==> Training loss: 0.493398    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.517093    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.353866    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.349815    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.522168    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.778699    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.500315    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.376967    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.460305    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.368589    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.364994    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.426591    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.520659    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.488706    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.514119    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.321539    Training error rate: 8.000000
800.000000/1000.000000 ==> Training loss: 0.352800    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.747712    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.543496    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.385076    Training error rate: 18.000000
==> Total training loss: 537.894123    Total training error rate: 16.742000
==> Testing Epoch: 35
0.000000/100.000000 ==> Testing loss: 1.416957    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.298468    Testing error rate: 35.000000
==> Total testing loss: 128.735879    Total testing error rate: 33.180000
==> Set learning rate: 0.010000
==> Training Epoch: 36
0.000000/1000.000000 ==> Training loss: 0.473164    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.433537    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.467526    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.443184    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.617312    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.696765    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.382543    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.443756    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.459088    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.480450    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.516917    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.404938    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.250042    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.868545    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.427695    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.760578    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.595996    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.389359    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.581635    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.332481    Training error rate: 12.000000
==> Total training loss: 530.333477    Total training error rate: 16.518000
==> Testing Epoch: 36
0.000000/100.000000 ==> Testing loss: 1.491593    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.439892    Testing error rate: 33.000000
==> Total testing loss: 132.689520    Total testing error rate: 33.590000
==> Set learning rate: 0.010000
==> Training Epoch: 37
0.000000/1000.000000 ==> Training loss: 0.522319    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.337910    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.548351    Training error rate: 20.000000
150.000000/1000.000000 ==> Training loss: 0.572943    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.291482    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.392987    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.857531    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.416996    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.345856    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.720703    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.667278    Training error rate: 18.000000
550.000000/1000.000000 ==> Training loss: 0.518329    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.513659    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.516430    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.886918    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.589640    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.722260    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.764057    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.438010    Training error rate: 10.000000
950.000000/1000.000000 ==> Training loss: 0.519963    Training error rate: 14.000000
==> Total training loss: 514.767130    Total training error rate: 16.014000
==> Testing Epoch: 37
0.000000/100.000000 ==> Testing loss: 1.362627    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.494166    Testing error rate: 37.000000
==> Total testing loss: 135.344767    Total testing error rate: 34.270000
==> Set learning rate: 0.010000
==> Training Epoch: 38
0.000000/1000.000000 ==> Training loss: 0.390156    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.210681    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.212854    Training error rate: 6.000000
150.000000/1000.000000 ==> Training loss: 0.453237    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.567344    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.397695    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.659693    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.455969    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.388112    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.499256    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.281768    Training error rate: 10.000000
550.000000/1000.000000 ==> Training loss: 0.940469    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.354611    Training error rate: 12.000000
650.000000/1000.000000 ==> Training loss: 0.586523    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.491159    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.653063    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.430160    Training error rate: 12.000000
850.000000/1000.000000 ==> Training loss: 0.585682    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.761453    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.488902    Training error rate: 14.000000
==> Total training loss: 508.658543    Total training error rate: 15.914000
==> Testing Epoch: 38
0.000000/100.000000 ==> Testing loss: 1.669469    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.403951    Testing error rate: 35.000000
==> Total testing loss: 144.760002    Total testing error rate: 35.390000
==> Set learning rate: 0.010000
==> Training Epoch: 39
0.000000/1000.000000 ==> Training loss: 0.713053    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.360587    Training error rate: 8.000000
100.000000/1000.000000 ==> Training loss: 0.333455    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.469013    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.373242    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.424465    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.788312    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.402207    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.570696    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.537472    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.705001    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.544030    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.502664    Training error rate: 18.000000
650.000000/1000.000000 ==> Training loss: 0.349189    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.606753    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.480229    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.416004    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.641803    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.490630    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.656467    Training error rate: 22.000000
==> Total training loss: 511.452728    Total training error rate: 16.054000
==> Testing Epoch: 39
0.000000/100.000000 ==> Testing loss: 1.325889    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.391128    Testing error rate: 33.000000
==> Total testing loss: 133.601643    Total testing error rate: 33.750000
==> Set learning rate: 0.010000
==> Training Epoch: 40
0.000000/1000.000000 ==> Training loss: 0.716802    Training error rate: 30.000000
50.000000/1000.000000 ==> Training loss: 0.375767    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.348400    Training error rate: 10.000000
150.000000/1000.000000 ==> Training loss: 0.386557    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.490948    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.408495    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.239014    Training error rate: 6.000000
350.000000/1000.000000 ==> Training loss: 0.360666    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.521634    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.311205    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.692995    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.503899    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.571210    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.730871    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.454530    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.437251    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.784645    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.749598    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.507238    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.742778    Training error rate: 22.000000
==> Total training loss: 490.604032    Total training error rate: 15.276000
==> Testing Epoch: 40
0.000000/100.000000 ==> Testing loss: 1.388763    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.460940    Testing error rate: 34.000000
==> Total testing loss: 133.309104    Total testing error rate: 33.510000
==> Set learning rate: 0.010000
==> Training Epoch: 41
0.000000/1000.000000 ==> Training loss: 0.400794    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.552497    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.690024    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.691139    Training error rate: 26.000000
200.000000/1000.000000 ==> Training loss: 0.403494    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.454619    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.611817    Training error rate: 22.000000
350.000000/1000.000000 ==> Training loss: 0.284257    Training error rate: 8.000000
400.000000/1000.000000 ==> Training loss: 0.304677    Training error rate: 8.000000
450.000000/1000.000000 ==> Training loss: 0.488424    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.425047    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.485073    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.492987    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.392459    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.420543    Training error rate: 20.000000
750.000000/1000.000000 ==> Training loss: 0.349525    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.531872    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.396246    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.469083    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.736115    Training error rate: 22.000000
==> Total training loss: 482.457945    Total training error rate: 14.984000
==> Testing Epoch: 41
0.000000/100.000000 ==> Testing loss: 1.286527    Testing error rate: 31.000000
50.000000/100.000000 ==> Testing loss: 1.535186    Testing error rate: 41.000000
==> Total testing loss: 139.527525    Total testing error rate: 33.960000
==> Set learning rate: 0.010000
==> Training Epoch: 42
0.000000/1000.000000 ==> Training loss: 0.588413    Training error rate: 22.000000
50.000000/1000.000000 ==> Training loss: 0.583433    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.254410    Training error rate: 8.000000
150.000000/1000.000000 ==> Training loss: 0.348387    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.198521    Training error rate: 4.000000
250.000000/1000.000000 ==> Training loss: 0.424969    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.421748    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.449636    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.564013    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.470953    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.439111    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.566951    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.869370    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.212606    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.795025    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.538101    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.508715    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.432299    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.575001    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.418013    Training error rate: 14.000000
==> Total training loss: 485.551058    Total training error rate: 15.158000
==> Testing Epoch: 42
0.000000/100.000000 ==> Testing loss: 1.276446    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.411205    Testing error rate: 36.000000
==> Total testing loss: 133.642553    Total testing error rate: 33.830000
==> Set learning rate: 0.010000
==> Training Epoch: 43
0.000000/1000.000000 ==> Training loss: 0.268995    Training error rate: 6.000000
50.000000/1000.000000 ==> Training loss: 0.483741    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.404081    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.323366    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.669584    Training error rate: 22.000000
250.000000/1000.000000 ==> Training loss: 0.517139    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.528442    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.401222    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.408127    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.422027    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.469808    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.435880    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.676620    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.382778    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.430895    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.272249    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.807660    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.329221    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.588858    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.484126    Training error rate: 12.000000
==> Total training loss: 461.249712    Total training error rate: 14.458000
==> Testing Epoch: 43
0.000000/100.000000 ==> Testing loss: 1.401996    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.405297    Testing error rate: 31.000000
==> Total testing loss: 142.204903    Total testing error rate: 34.570000
==> Set learning rate: 0.010000
==> Training Epoch: 44
0.000000/1000.000000 ==> Training loss: 0.521284    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.326952    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.353533    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.498530    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.427460    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.349093    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.311323    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.337610    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.929363    Training error rate: 28.000000
450.000000/1000.000000 ==> Training loss: 0.502359    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.474299    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.644784    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.544656    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.418992    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.544744    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.677972    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.468367    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.603698    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.460167    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.414193    Training error rate: 12.000000
==> Total training loss: 460.236724    Total training error rate: 14.322000
==> Testing Epoch: 44
0.000000/100.000000 ==> Testing loss: 1.217281    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.287425    Testing error rate: 30.000000
==> Total testing loss: 136.454186    Total testing error rate: 33.370000
==> Set learning rate: 0.010000
==> Training Epoch: 45
0.000000/1000.000000 ==> Training loss: 0.430786    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.305765    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.326511    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.445151    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.351924    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.487034    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.348738    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.280281    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.374279    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.526731    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.709168    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.641037    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.673947    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.493031    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.863913    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 0.446785    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.953513    Training error rate: 28.000000
850.000000/1000.000000 ==> Training loss: 0.608976    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.604473    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.494909    Training error rate: 14.000000
==> Total training loss: 454.403468    Total training error rate: 14.204000
==> Testing Epoch: 45
0.000000/100.000000 ==> Testing loss: 1.304280    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.244558    Testing error rate: 33.000000
==> Total testing loss: 137.683011    Total testing error rate: 33.340000
==> Set learning rate: 0.010000
==> Training Epoch: 46
0.000000/1000.000000 ==> Training loss: 0.410263    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.319745    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.592297    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.366854    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.469393    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.320772    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.202899    Training error rate: 6.000000
350.000000/1000.000000 ==> Training loss: 0.471216    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.471814    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.523177    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.368291    Training error rate: 12.000000
550.000000/1000.000000 ==> Training loss: 0.488721    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.388250    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.337835    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.474120    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.304200    Training error rate: 8.000000
800.000000/1000.000000 ==> Training loss: 0.445077    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.430269    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.644756    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.239597    Training error rate: 8.000000
==> Total training loss: 450.199352    Total training error rate: 14.184000
==> Testing Epoch: 46
0.000000/100.000000 ==> Testing loss: 1.836052    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.678423    Testing error rate: 36.000000
==> Total testing loss: 145.251171    Total testing error rate: 34.780000
==> Set learning rate: 0.010000
==> Training Epoch: 47
0.000000/1000.000000 ==> Training loss: 0.490157    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.312375    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.246098    Training error rate: 6.000000
150.000000/1000.000000 ==> Training loss: 0.720378    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.359708    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.395107    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.394884    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.266999    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.276269    Training error rate: 8.000000
450.000000/1000.000000 ==> Training loss: 0.691242    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.646556    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.457709    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.435227    Training error rate: 12.000000
650.000000/1000.000000 ==> Training loss: 0.313825    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.354125    Training error rate: 10.000000
750.000000/1000.000000 ==> Training loss: 0.339714    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.515155    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.802391    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.374395    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.638388    Training error rate: 18.000000
==> Total training loss: 453.612544    Total training error rate: 14.124000
==> Testing Epoch: 47
0.000000/100.000000 ==> Testing loss: 1.587674    Testing error rate: 33.000000
50.000000/100.000000 ==> Testing loss: 1.214726    Testing error rate: 35.000000
==> Total testing loss: 136.184986    Total testing error rate: 33.200000
==> Set learning rate: 0.010000
==> Training Epoch: 48
0.000000/1000.000000 ==> Training loss: 0.291185    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.547348    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.419140    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.248258    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.602171    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.250053    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.498472    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.398649    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.379074    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.513914    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.417094    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.660054    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.266843    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.336535    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.667075    Training error rate: 20.000000
750.000000/1000.000000 ==> Training loss: 0.758635    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.339449    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.261778    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.472816    Training error rate: 12.000000
950.000000/1000.000000 ==> Training loss: 0.292143    Training error rate: 4.000000
==> Total training loss: 432.183565    Total training error rate: 13.526000
==> Testing Epoch: 48
0.000000/100.000000 ==> Testing loss: 1.483737    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.292743    Testing error rate: 30.000000
==> Total testing loss: 133.245294    Total testing error rate: 33.460000
==> Set learning rate: 0.010000
==> Training Epoch: 49
0.000000/1000.000000 ==> Training loss: 0.274462    Training error rate: 8.000000
50.000000/1000.000000 ==> Training loss: 0.320562    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.192347    Training error rate: 4.000000
150.000000/1000.000000 ==> Training loss: 0.296792    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.370274    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.291290    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.395462    Training error rate: 14.000000
350.000000/1000.000000 ==> Training loss: 0.447141    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.694239    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.391202    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.314588    Training error rate: 8.000000
550.000000/1000.000000 ==> Training loss: 0.522208    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.350614    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.629715    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.748747    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.634868    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.484708    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.231160    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.678211    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.405243    Training error rate: 12.000000
==> Total training loss: 434.063528    Total training error rate: 13.640000
==> Testing Epoch: 49
0.000000/100.000000 ==> Testing loss: 1.341824    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.651895    Testing error rate: 37.000000
==> Total testing loss: 132.950175    Total testing error rate: 32.860000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 50
0.000000/1000.000000 ==> Training loss: 0.271199    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.368749    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.334075    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.458919    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.540437    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.333041    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.357528    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.355388    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.452689    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.391300    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.344761    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.308382    Training error rate: 6.000000
600.000000/1000.000000 ==> Training loss: 0.434797    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.527235    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.473409    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.541115    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.221035    Training error rate: 8.000000
850.000000/1000.000000 ==> Training loss: 0.317308    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.460594    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.379441    Training error rate: 12.000000
==> Total training loss: 414.474264    Total training error rate: 13.054000
==> Testing Epoch: 50
0.000000/100.000000 ==> Testing loss: 1.411966    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.248434    Testing error rate: 29.000000
==> Total testing loss: 130.023694    Total testing error rate: 32.080000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 51
0.000000/1000.000000 ==> Training loss: 0.548065    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.530744    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.343395    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.170699    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.308693    Training error rate: 8.000000
250.000000/1000.000000 ==> Training loss: 0.087190    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.324754    Training error rate: 14.000000
350.000000/1000.000000 ==> Training loss: 0.187241    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.168844    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.186813    Training error rate: 8.000000
500.000000/1000.000000 ==> Training loss: 0.231997    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.105977    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.128095    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.162911    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.187772    Training error rate: 6.000000
750.000000/1000.000000 ==> Training loss: 0.241723    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.275930    Training error rate: 8.000000
850.000000/1000.000000 ==> Training loss: 0.246420    Training error rate: 6.000000
900.000000/1000.000000 ==> Training loss: 0.086957    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.111015    Training error rate: 2.000000
==> Total training loss: 188.944883    Total training error rate: 5.376000
==> Testing Epoch: 51
0.000000/100.000000 ==> Testing loss: 1.104177    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.076446    Testing error rate: 26.000000
==> Total testing loss: 100.681273    Total testing error rate: 25.650000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 52
0.000000/1000.000000 ==> Training loss: 0.041489    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.087955    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.106856    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.262815    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.054615    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.087994    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.076306    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.082756    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.065131    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.164704    Training error rate: 6.000000
500.000000/1000.000000 ==> Training loss: 0.093612    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.063397    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.161989    Training error rate: 6.000000
650.000000/1000.000000 ==> Training loss: 0.206366    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.144986    Training error rate: 4.000000
750.000000/1000.000000 ==> Training loss: 0.074154    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.091502    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.078383    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.158312    Training error rate: 4.000000
950.000000/1000.000000 ==> Training loss: 0.094958    Training error rate: 4.000000
==> Total training loss: 114.190603    Total training error rate: 2.926000
==> Testing Epoch: 52
0.000000/100.000000 ==> Testing loss: 1.114215    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.081510    Testing error rate: 26.000000
==> Total testing loss: 99.513879    Total testing error rate: 25.300000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 53
0.000000/1000.000000 ==> Training loss: 0.056279    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.093921    Training error rate: 4.000000
100.000000/1000.000000 ==> Training loss: 0.105373    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.177438    Training error rate: 4.000000
200.000000/1000.000000 ==> Training loss: 0.115392    Training error rate: 6.000000
250.000000/1000.000000 ==> Training loss: 0.023996    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.073734    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.022851    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.107368    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.045182    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.087432    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.057751    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.350080    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.031174    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.052472    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.086697    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.082900    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.120121    Training error rate: 6.000000
900.000000/1000.000000 ==> Training loss: 0.126391    Training error rate: 4.000000
950.000000/1000.000000 ==> Training loss: 0.106905    Training error rate: 6.000000
==> Total training loss: 90.951876    Total training error rate: 2.206000
==> Testing Epoch: 53
0.000000/100.000000 ==> Testing loss: 1.136514    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.078532    Testing error rate: 26.000000
==> Total testing loss: 98.590839    Total testing error rate: 25.240000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 54
0.000000/1000.000000 ==> Training loss: 0.113570    Training error rate: 4.000000
50.000000/1000.000000 ==> Training loss: 0.077907    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.085364    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.093794    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.048368    Training error rate: 2.000000
250.000000/1000.000000 ==> Training loss: 0.096128    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.031369    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.106214    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.044870    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.076299    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.108279    Training error rate: 2.000000
550.000000/1000.000000 ==> Training loss: 0.040064    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.039673    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.194496    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.042963    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.054935    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.045241    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.061993    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.062955    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.082277    Training error rate: 2.000000
==> Total training loss: 75.623383    Total training error rate: 1.646000
==> Testing Epoch: 54
0.000000/100.000000 ==> Testing loss: 1.154672    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.126218    Testing error rate: 25.000000
==> Total testing loss: 98.961594    Total testing error rate: 25.030000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 55
0.000000/1000.000000 ==> Training loss: 0.025742    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.078817    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.066502    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.054367    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.063125    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.050767    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.110262    Training error rate: 4.000000
350.000000/1000.000000 ==> Training loss: 0.044629    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.098108    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.044445    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.106241    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.048584    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.034466    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.038486    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.067968    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.080149    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.065024    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.097756    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.033506    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.102009    Training error rate: 2.000000
==> Total training loss: 68.041148    Total training error rate: 1.518000
==> Testing Epoch: 55
0.000000/100.000000 ==> Testing loss: 1.144079    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.141368    Testing error rate: 24.000000
==> Total testing loss: 99.233717    Total testing error rate: 24.840000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 56
0.000000/1000.000000 ==> Training loss: 0.031017    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.023896    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.062789    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.029212    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.045258    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.030282    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.014496    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.026994    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.074402    Training error rate: 4.000000
450.000000/1000.000000 ==> Training loss: 0.040946    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.048940    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.068985    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.059540    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.040985    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.037657    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.043446    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.066146    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.036403    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.074663    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.058206    Training error rate: 0.000000
==> Total training loss: 61.289695    Total training error rate: 1.348000
==> Testing Epoch: 56
0.000000/100.000000 ==> Testing loss: 1.155417    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.100886    Testing error rate: 25.000000
==> Total testing loss: 99.680632    Total testing error rate: 25.210000
==> Set learning rate: 0.001000
==> Training Epoch: 57
0.000000/1000.000000 ==> Training loss: 0.045787    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.044946    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.030281    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.097151    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.074337    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.007269    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.067873    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.095476    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.054860    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.037578    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.056823    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.036744    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.051314    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.032107    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.065067    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.042080    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.030135    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.036866    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.034153    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.032587    Training error rate: 0.000000
==> Total training loss: 55.976676    Total training error rate: 1.154000
==> Testing Epoch: 57
0.000000/100.000000 ==> Testing loss: 1.182112    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.179492    Testing error rate: 26.000000
==> Total testing loss: 99.916698    Total testing error rate: 24.790000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 58
0.000000/1000.000000 ==> Training loss: 0.061238    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.038715    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.047444    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.043027    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.011927    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.034696    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.030611    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.040962    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.068296    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.073488    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.032725    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.081250    Training error rate: 4.000000
600.000000/1000.000000 ==> Training loss: 0.036159    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.026027    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.040845    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.063127    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.048407    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.085359    Training error rate: 4.000000
900.000000/1000.000000 ==> Training loss: 0.025505    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.084730    Training error rate: 4.000000
==> Total training loss: 49.025808    Total training error rate: 0.916000
==> Testing Epoch: 58
0.000000/100.000000 ==> Testing loss: 1.220926    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.160979    Testing error rate: 26.000000
==> Total testing loss: 100.286230    Total testing error rate: 24.660000
==> Saving checkpoint..
= = >   I n i t   v a r i a b l e s . .  
 = = >   I n i t   s e e d . .  
 = = >   D o w n l o a d   d a t a . .  
 F i l e s   a l r e a d y   d o w n l o a d e d   a n d   v e r i f i e d  
 = = >   C a l c u l a t e   m e a n   a n d   s t d . .  
 = = >   P r e p a r e   t r a i n i n g   t r a n s f o r m . .  
 = = >   P r e p a r e   t e s t i n g   t r a n s f o r m . .  
 = = >   I n i t   d a t a l o a d e r . .  
 F i l e s   a l r e a d y   d o w n l o a d e d   a n d   v e r i f i e d  
 F i l e s   a l r e a d y   d o w n l o a d e d   a n d   v e r i f i e d  
 = = >   B u i l d i n g   m o d e l . .  
 = = >   R e s u m i n g   f r o m   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   5 9  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 1 2 2 9 6 3         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 2 9 3 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 6 9 7 9         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 8 0 4 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 8 6 2 7 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 5 2 8 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 0 8 0 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 4 9 4 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 9 3 1 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 2 7 5 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 1 3 0 9         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 9 8 1 5 7         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 3 1 8 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 1 5 9 7         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 8 6 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 6 4 0 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 3 5 9 8         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 0 4 3 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 2 2 4 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 1 2 8 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   4 6 . 1 5 1 4 2 5         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 8 4 4 0 0 0  
 = = >   T e s t i n g   E p o c h :   5 9  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 8 9 7 6 5         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 2 4 1 6 1         T e s t i n g   e r r o r   r a t e :   2 5 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 0 . 3 1 6 9 1 4         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 7 9 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   6 0  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 1 5 6 3         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 2 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 6 6 5 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 5 4 6 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 5 2 1 5         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 9 8 3 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 8 1 9 8         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 3 7 7 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 4 3 2 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 2 7 6 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 7 5 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 1 3 9 4 4 7         T r a i n i n g   e r r o r   r a t e :   4 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 2 9 1 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 8 8 3 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 1 0 3 2         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 0 2 5 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 5 8 8         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 0 7 4 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 3 8 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 0 5 4 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   4 2 . 2 5 6 1 3 1         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 7 5 2 0 0 0  
 = = >   T e s t i n g   E p o c h :   6 0  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 1 0 4 1 5         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 6 5 0 8 2         T e s t i n g   e r r o r   r a t e :   2 7 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 0 . 6 5 4 5 7 9         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 7 9 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   6 1  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 1 5 2 2 8 8         T r a i n i n g   e r r o r   r a t e :   4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 9 8 7 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 3 5 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 1 3 7 5 1 6         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 5 9 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 8 4 1 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 9 5 3 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 8 5 9 9         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 2 0 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 1 0 7 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 2 5 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 3 5 8 0         T r a i n i n g   e r r o r   r a t e :   4 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 9 3 3 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 3 2 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 8 0 3 6 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 3 9 1 6         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 4 6 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 1 1 5 4 4 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 1 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 8 3 6 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   3 9 . 8 6 2 1 8 3         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 7 0 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   6 1  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 3 5 8 0 1         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 6 0 0 4 0         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 0 9 8 8 8 9         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 8 0 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   6 2  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 1 3 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 1 2 5 0 3 2         T r a i n i n g   e r r o r   r a t e :   6 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 9 2 0 9 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 8 8 4 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 9 3 7 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 5 4 2 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 8 5 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 2 1 3 4         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 1 2 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 8 7 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 5 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 6 4 7 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 4 1 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 1 9 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 4 8 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 7 1 7 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 3 7 1 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 6 8 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 6 9 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 3 7 4 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   3 7 . 9 3 3 8 0 3         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 6 3 6 0 0 0  
 = = >   T e s t i n g   E p o c h :   6 2  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 6 4 1 6 5         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 8 7 0 1 4         T e s t i n g   e r r o r   r a t e :   2 6 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 3 0 6 4 2 1         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 6 4 0 0 0 0  
 = = >   S a v i n g   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   6 3  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 7 0 2 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 5 9 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 2 6 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 2 1 8 9         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 8 8 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 7 2 1 0 4         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 9 9 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 8 0 3 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 8 9 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 7 1 9 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 1 9 3 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 3 9 0 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 0 5 4 6         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 7 7 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 5 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 1 9 3         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 7 5 0 4 3         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 1 5 4 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 0 0 6 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 9 8 3 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   3 5 . 1 9 2 0 1 4         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 5 5 4 0 0 0  
 = = >   T e s t i n g   E p o c h :   6 3  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 7 7 6 9 3         T e s t i n g   e r r o r   r a t e :   2 7 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 9 7 9 5 8         T e s t i n g   e r r o r   r a t e :   2 7 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 7 5 8 8 0 5         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 4 8 0 0 0 0  
 = = >   S a v i n g   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   6 4  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 5 7 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 3 8 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 4 0 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 2 7 9 6         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 4 0 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 9 0 8 3         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 0 2 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 4 6 7 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 0 3 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 6 9 3 2         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 0 6 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 7 3 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 8 1 5 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 7 6 6 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 4 7 7 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 1 0 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 8 6 1 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 6 1 3 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 9 9 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 9 7 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   3 3 . 5 2 1 0 3 2         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 5 4 2 0 0 0  
 = = >   T e s t i n g   E p o c h :   6 4  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 0 5 2 7 7         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 1 7 9 8 6         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 8 6 9 1 9 9         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 5 0 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   6 5  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 4 3 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 0 1 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 6 4 8 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 1 2 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 9 6 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 6 0 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 0 3 4 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 1 9 7 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 3 5 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 2 7 6 8         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 1 7 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 5 0 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 5 3 3 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 7 4 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 4 8 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 3 5 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 2 3 3 9         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 8 6 8 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 7 8 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 3 0 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   3 0 . 6 0 9 4 1 1         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 4 4 2 0 0 0  
 = = >   T e s t i n g   E p o c h :   6 5  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 8 5 6 8 8         T e s t i n g   e r r o r   r a t e :   2 7 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 2 3 5 1 5         T e s t i n g   e r r o r   r a t e :   3 0 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 8 7 3 8 3 0         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 3 2 0 0 0 0  
 = = >   S a v i n g   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   6 6  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 5 8 7 3         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 8 4 1 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 5 8 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 8 7 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 5 8 5 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 7 2 2 9 5         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 4 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 6 4 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 9 5 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 8 1 8 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 6 2 5 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 1 3 8 7         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 1 8 1 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 5 3 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 0 5 8 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 8 6 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 6 8 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 6 3 5 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 3 4 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 9 1 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   3 0 . 1 3 0 3 4 7         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 4 5 2 0 0 0  
 = = >   T e s t i n g   E p o c h :   6 6  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 0 5 4 6 7         T e s t i n g   e r r o r   r a t e :   2 6 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 5 1 5 1 9         T e s t i n g   e r r o r   r a t e :   2 5 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 7 0 3 1 8 2         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 3 5 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   6 7  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 9 0 0 8         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 0 2 6 4         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 2 2 5 6         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 8 5 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 6 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 0 6 9 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 4 0 3 8         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 7 4 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 3 1 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 7 6 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 2 1 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 7 2 3 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 8 7 3 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 7 3 1 4         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 5 3 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 7 8 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 0 1 9 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 4 8 4 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 6 1 1 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 5 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   2 9 . 0 9 7 5 1 7         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 4 0 6 0 0 0  
 = = >   T e s t i n g   E p o c h :   6 7  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 1 0 4 0 1         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 3 7 4 3 0         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 3 4 2 3 3 0         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 4 6 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   6 8  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 3 5 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 8 0 2 5         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 0 3 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 1 0 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 8 2 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 2 1 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 8 7 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 7 5 1 4         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 2 3 2 2         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 2 4 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 9 8 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 0 5 0 5         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 0 7 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 5 9 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 8 0 4 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 7 6 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 7 0 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 6 2 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 6 0 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 7 9 3 0 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   2 6 . 1 0 1 0 4 1         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 3 6 2 0 0 0  
 = = >   T e s t i n g   E p o c h :   6 8  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 7 6 5 8 6         T e s t i n g   e r r o r   r a t e :   2 6 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 8 2 1 4 2         T e s t i n g   e r r o r   r a t e :   2 6 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 4 1 6 5 6 0         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 2 8 0 0 0 0  
 = = >   S a v i n g   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   6 9  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 6 7 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 5 1 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 3 1 2 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 7 3 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 7 8 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 8 7 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 6 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 2 3 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 2 1 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 0 4 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 0 7 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 6 7 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 9 1 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 7 8 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 5 1 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 4 9 3 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 5 9 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 5 1 8 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 8 1 5 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 9 4 7 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   2 6 . 0 5 7 4 5 0         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 3 4 2 0 0 0  
 = = >   T e s t i n g   E p o c h :   6 9  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 8 1 6 0 4         T e s t i n g   e r r o r   r a t e :   2 7 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 3 3 2 2 2         T e s t i n g   e r r o r   r a t e :   2 7 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 2 4 7 5 0 1         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 1 6 0 0 0 0  
 = = >   S a v i n g   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   7 0  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 8 2 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 6 9 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 7 4 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 6 7 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 9 0 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 6 1 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 2 8 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 3 4 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 1 2 8 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 3 3 7 2         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 7 2 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 5 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 0 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 3 0 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 5 7 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 6 9 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 8 0 2 3         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 3 5 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 7 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 0 5 1 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   2 5 . 7 3 6 1 8 8         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 3 5 6 0 0 0  
 = = >   T e s t i n g   E p o c h :   7 0  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 4 1 2 2 7         T e s t i n g   e r r o r   r a t e :   2 6 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 4 7 7 5 6         T e s t i n g   e r r o r   r a t e :   2 7 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 1 6 2 7 3 0         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 2 0 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   7 1  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 1 7 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 0 2 0 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 9 3 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 7 6 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 8 1 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 5 9 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 7 0 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 6 3 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 5 1 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 5 7 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 9 9 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 1 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 7 6 1 9         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 3 0 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 3 1 6 8         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 6 9 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 1 5 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 3 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 0 7 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 6 5 2 7         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   2 3 . 8 5 1 4 0 1         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 3 3 8 0 0 0  
 = = >   T e s t i n g   E p o c h :   7 1  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 0 3 6 0 3         T e s t i n g   e r r o r   r a t e :   2 6 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 5 0 1 6 0         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 4 3 1 5 8 5         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 4 4 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   7 2  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 9 2 8 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 2 4 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 9 1 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 9 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 1 6 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 7 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 8 8 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 4 3 1 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 1 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 4 2 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 4 4 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 5 3 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 1 5 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 6 4 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 7 6 8 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 1 1 4 7 5 8         T r a i n i n g   e r r o r   r a t e :   6 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 0 6 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 8 5 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 5 2 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 7 7 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   2 3 . 0 6 2 2 5 2         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 2 8 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   7 2  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 5 6 6 3 9         T e s t i n g   e r r o r   r a t e :   2 6 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 3 7 0 4 9         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 1 2 2 8 6 0         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 4 1 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   7 3  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 9 6 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 8 9 9 8         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 8 7 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 1 3 1 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 6 2 9         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 0 7 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 3 6 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 5 3 4 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 0 9 0 9         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 3 8 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 1 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 8 3 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 1 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 2 6 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 0 6 3 6         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 3 0 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 6 7 6 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 7 9 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 9 0 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 7 8 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   2 1 . 5 4 7 2 4 2         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 2 4 8 0 0 0  
 = = >   T e s t i n g   E p o c h :   7 3  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 9 8 4 4 1         T e s t i n g   e r r o r   r a t e :   2 7 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 4 7 4 5 8         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 0 6 4 3 6 1         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 1 7 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   7 4  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 9 2 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 8 4 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 9 0 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 8 9 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 1 7 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 2 9 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 8 3 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 7 6 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 4 5 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 7 9 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 1 0 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 3 9 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 1 0 5 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 0 5 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 0 7 9 9         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 5 5 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 6 3 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 1 6 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 8 0 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 6 7 8 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   2 1 . 5 2 0 0 5 4         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 2 8 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   7 4  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 0 7 7 7 2         T e s t i n g   e r r o r   r a t e :   2 6 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 5 3 9 8 0         T e s t i n g   e r r o r   r a t e :   2 6 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 6 7 5 8 9 9         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 0 9 0 0 0 0  
 = = >   S a v i n g   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   7 5  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 6 9 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 4 4 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 1 2 2 8         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 8 0 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 3 8 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 0 9 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 8 1 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 6 8 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 4 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 3 5 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 9 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 2 9 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 6 0 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 4 0 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 2 7 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 7 9 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 4 2 7 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 5 5 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 7 1 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 6 3 3 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   2 0 . 6 5 0 9 6 4         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 2 3 2 0 0 0  
 = = >   T e s t i n g   E p o c h :   7 5  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 8 3 7 3 6         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 6 3 6 5 1         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 6 4 2 5 7 2         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 4 6 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   7 6  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 9 0 8 8         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 1 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 1 2 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 8 1 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 1 7 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 9 6 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 0 5 5 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 6 1 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 8 6 0 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 1 6 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 2 2 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 7 2 2 3         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 6 4 6 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 3 9 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 6 6 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 8 1 4 0 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 7 0 1 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 1 0 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 9 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 1 5 4 7         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   2 0 . 6 2 1 5 2 3         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 2 5 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   7 6  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 3 2 5 2 9         T e s t i n g   e r r o r   r a t e :   2 6 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 5 4 3 3 6         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 3 5 9 7 5 5         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 2 9 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   7 7  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 4 6 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 5 6 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 7 8 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 4 4 5 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 0 0 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 7 5 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 1 7 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 6 3 8 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 2 2 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 1 0 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 8 2 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 9 1 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 0 3 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 8 0 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 8 3 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 7 8 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 7 2 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 6 9 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 0 2 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 4 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 9 . 4 1 0 7 0 1         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 9 8 0 0 0  
 = = >   T e s t i n g   E p o c h :   7 7  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 1 5 7 7 0         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 6 2 8 9 7         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 5 6 9 3 3 2         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 2 0 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   7 8  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 4 2 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 2 6 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 5 7 1 6         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 6 6 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 3 1 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 8 8 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 1 9 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 0 6 3 4         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 1 7 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 4 9 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 1 5 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 2 1 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 9 0 0 8         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 6 0 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 5 5 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 5 2 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 1 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 5 8 3 4         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 9 1 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 4 2 1 8         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 9 . 0 9 7 7 8 9         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 2 3 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   7 8  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 5 1 5 9 3         T e s t i n g   e r r o r   r a t e :   2 5 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 9 4 0 8 9         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 5 0 7 8 5 4         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 2 5 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   7 9  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 6 8 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 3 8 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 2 8 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 7 0 3 6 6         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 6 3 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 7 4 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 2 7 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 4 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 5 6 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 3 4 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 6 6 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 9 6 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 5 2 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 1 2 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 2 6 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 0 7 9 2         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 1 3 3 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 9 5 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 9 0 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 8 6 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 8 . 5 9 5 1 0 8         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 2 0 8 0 0 0  
 = = >   T e s t i n g   E p o c h :   7 9  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 0 8 6 2 5         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 5 4 3 5 0         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 2 5 5 7 4 2         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 1 3 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   8 0  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 2 1 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 4 4 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 9 0 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 6 7 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 5 2 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 6 3 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 5 9 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 5 9 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 8 6 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 3 7 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 8 6 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 0 6 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 4 1 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 2 4 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 2 4 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 5 7 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 9 4 3 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 4 7 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 6 6 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 2 3 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 8 . 3 1 6 7 7 1         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 8 2 0 0 0  
 = = >   T e s t i n g   E p o c h :   8 0  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 7 9 8 7 3         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 7 6 1 2 5         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 3 7 0 2 5 6         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 2 8 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   8 1  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 7 7 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 5 3 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 8 7 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 3 8 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 2 9 0 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 0 4 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 6 5 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 8 3 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 5 7 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 4 8 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 0 6 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 7 8 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 1 6 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 8 0 6 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 1 5 9         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 4 4 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 2 8 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 6 5 8 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 4 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 7 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 8 . 4 2 9 3 6 0         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 2 0 8 0 0 0  
 = = >   T e s t i n g   E p o c h :   8 1  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 0 8 1 4 9         T e s t i n g   e r r o r   r a t e :   2 5 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 1 2 4 0 9         T e s t i n g   e r r o r   r a t e :   2 7 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 3 0 3 3 3 1         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 0 8 0 0 0 0  
 = = >   S a v i n g   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   8 2  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 1 1 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 0 9 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 3 5 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 1 8 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 3 0 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 7 4 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 2 1 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 1 0 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 1 6 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 0 5 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 0 7 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 0 3 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 6 7 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 6 2 1         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 0 6 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 7 0 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 4 7 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 5 6 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 7 0 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 4 8 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 7 . 0 1 6 3 4 7         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 7 8 0 0 0  
 = = >   T e s t i n g   E p o c h :   8 2  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 5 1 1 7 1         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 4 8 2 8 7         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 6 8 4 3 3 1         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 2 0 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   8 3  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 3 5 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 1 1 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 1 2 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 2 5 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 2 9 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 7 6 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 3 3 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 9 9 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 2 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 4 5 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 1 3 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 4 9 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 0 6 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 1 3 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 6 0 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 8 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 1 9 9 4         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 1 7 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 6 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 0 7 5 9         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 7 . 6 1 6 4 2 8         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 2 0 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   8 3  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 1 4 7 4 0         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 4 2 0 5 4         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 9 9 3 4 3 1         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 8 8 0 0 0 0  
 = = >   S a v i n g   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   8 4  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 2 4 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 1 5 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 9 0 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 4 9 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 7 5 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 0 3 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 4 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 8 3 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 0 1 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 1 2 4 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 1 3 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 2 2 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 8 3 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 4 2 7 4         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 6 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 8 0 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 8 2 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 0 7 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 4 5 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 4 1 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 7 . 2 5 0 6 2 8         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 8 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   8 4  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 8 7 3 6 2         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 2 9 9 7 6         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 3 4 3 2 7 1         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 8 5 0 0 0 0  
 = = >   S a v i n g   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   8 5  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 7 0 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 2 5 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 1 0 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 0 4 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 5 4 2 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 5 9 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 8 0 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 7 7 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 1 4 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 4 4 0 5         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 7 5 9 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 1 0 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 3 0 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 8 2 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 7 3 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 4 8 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 3 4 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 1 0 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 8 4 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 1 5 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 6 . 4 6 2 9 3 6         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 5 4 0 0 0  
 = = >   T e s t i n g   E p o c h :   8 5  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 9 1 2 4 7         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 4 9 0 0 5         T e s t i n g   e r r o r   r a t e :   2 8 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 4 2 5 5 7 9         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 0 6 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   8 6  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 4 2 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 7 3 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 1 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 8 1 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 5 8 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 0 1 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 6 3 9 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 4 0 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 5 7 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 4 2 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 8 4 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 6 0 2         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 4 3 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 4 6 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 6 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 4 7 9         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 9 5 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 6 3 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 1 0 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 1 3 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 5 . 4 3 5 1 6 3         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 5 2 0 0 0  
 = = >   T e s t i n g   E p o c h :   8 6  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 0 2 3 4 2         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 2 2 9 2 7         T e s t i n g   e r r o r   r a t e :   3 0 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 7 8 3 0 1 9         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 9 4 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   8 7  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 1 9 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 8 7 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 9 5 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 0 5 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 9 5 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 3 6 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 8 2 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 2 7 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 2 3 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 6 8 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 4 4 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 5 7 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 7 9 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 4 3 8 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 3 2 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 9 9 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 1 8 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 5 0 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 6 1 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 9 0 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 5 . 6 0 9 9 7 6         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 4 8 0 0 0  
 = = >   T e s t i n g   E p o c h :   8 7  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 4 1 1 0 2         T e s t i n g   e r r o r   r a t e :   2 5 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 2 7 0 7 2         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 6 6 3 7 9 6         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 8 8 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   8 8  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 1 2 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 6 4 0 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 7 3 7 6         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 7 0 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 4 1 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 0 1 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 0 0 8 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 7 7 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 5 6 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 0 7 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 0 7 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 4 3 2 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 4 9 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 5 4 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 5 2 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 5 8 9 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 9 7 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 2 7 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 0 5 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 9 1 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 6 . 2 9 4 5 2 2         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 6 8 0 0 0  
 = = >   T e s t i n g   E p o c h :   8 8  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 3 7 2 0 1         T e s t i n g   e r r o r   r a t e :   2 5 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 7 9 9 3 8         T e s t i n g   e r r o r   r a t e :   3 0 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 0 7 7 6 5 7         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 6 9 0 0 0 0  
 = = >   S a v i n g   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   8 9  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 1 9 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 5 2 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 9 9 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 8 2 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 8 9 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 7 4 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 5 3 0 5 0         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 9 7 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 8 7 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 0 2 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 8 7 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 5 5 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 1 9 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 8 8 5 6         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 4 0 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 3 7 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 7 5 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 4 9 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 8 3 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 8 1 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 4 . 6 2 9 1 6 2         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 2 2 0 0 0  
 = = >   T e s t i n g   E p o c h :   8 9  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 2 0 3 6 8         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 6 6 5 3 1         T e s t i n g   e r r o r   r a t e :   3 0 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 2 8 4 2 9 6         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 9 7 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   9 0  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 9 3 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 1 9 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 3 7 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 2 1 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 3 9 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 1 3 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 6 7 3 1 2         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 1 9 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 9 7 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 0 1 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 7 3 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 0 9 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 9 7 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 8 4 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 6 3 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 7 9 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 5 3 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 0 0 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 2 0 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 0 9 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 5 . 0 7 4 7 2 6         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 4 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   9 0  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 0 7 6 7 5         T e s t i n g   e r r o r   r a t e :   2 6 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 8 2 0 7 2         T e s t i n g   e r r o r   r a t e :   3 0 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 2 1 9 5 4 7         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 8 8 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   9 1  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 9 2 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 7 2 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 8 5 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 6 9 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 7 5 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 6 5 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 3 0 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 0 3 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 7 6 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 7 6 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 1 6 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 7 9 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 5 5 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 3 8 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 3 7 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 7 9 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 9 4 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 7 2 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 7 6 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 7 3 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 3 . 9 7 1 8 6 2         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 2 6 0 0 0  
 = = >   T e s t i n g   E p o c h :   9 1  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 1 8 5 7 2         T e s t i n g   e r r o r   r a t e :   2 5 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 3 1 4 1 1 2         T e s t i n g   e r r o r   r a t e :   3 2 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 9 5 6 7 3 9         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 7 6 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   9 2  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 0 9 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 8 0 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 8 3 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 0 0 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 3 8 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 0 0 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 5 5 1 7         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 5 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 0 2 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 5 5 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 7 8 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 3 5 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 5 0 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 0 2 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 1 0 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 3 1 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 4 8 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 7 7 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 6 1 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 2 8 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 4 . 5 7 3 5 9 3         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 3 8 0 0 0  
 = = >   T e s t i n g   E p o c h :   9 2  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 6 5 8 5 1         T e s t i n g   e r r o r   r a t e :   2 2 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 3 1 6 1 0 1         T e s t i n g   e r r o r   r a t e :   3 0 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 1 9 0 3 9 1         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 6 9 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   9 3  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 3 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 5 9 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 2 4 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 0 8 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 2 8 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 4 2 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 1 4 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 3 0 6         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 0 7 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 4 4 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 2 7 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 7 7 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 2 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 7 3 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 5 3 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 4 4 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 3 0 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 1 6 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 2 2 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 2 2 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 4 . 2 1 7 7 5 6         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 2 6 0 0 0  
 = = >   T e s t i n g   E p o c h :   9 3  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 8 1 9 4 5         T e s t i n g   e r r o r   r a t e :   2 5 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 8 3 8 2 0         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 1 4 6 5 7 6         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 8 2 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   9 4  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 8 0 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 0 6 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 5 9 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 4 1 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 8 5 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 2 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 9 3 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 8 0 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 3 9 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 8 5 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 8 4 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 3 5 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 4 7 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 8 8 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 1 3 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 8 5 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 8 2 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 4 8 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 4 2 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 5 0 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 4 . 3 9 9 2 5 1         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 3 2 0 0 0  
 = = >   T e s t i n g   E p o c h :   9 4  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 6 6 9 3 1         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 6 8 2 9 8         T e s t i n g   e r r o r   r a t e :   2 7 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 9 5 6 5 4 1         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 7 1 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   9 5  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 4 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 2 5 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 0 4 5 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 9 8 2 0 2         T r a i n i n g   e r r o r   r a t e :   4 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 3 1 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 8 2 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 8 1 8 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 6 1 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 3 8 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 5 0 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 2 9 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 6 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 9 0 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 9 5 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 7 2 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 7 6 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 1 1 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 8 3 9 3         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 3 5 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 0 8 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 3 . 5 6 2 8 5 6         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 1 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   9 5  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 5 1 1 8 8         T e s t i n g   e r r o r   r a t e :   2 6 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 3 0 5 9 7 6         T e s t i n g   e r r o r   r a t e :   3 0 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 2 4 2 9 5 2         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 9 5 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   9 6  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 1 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 3 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 2 0 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 0 8 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 1 1 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 0 4 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 0 2 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 2 3 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 4 2 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 9 2 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 3 4 0 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 1 6 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 2 3 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 0 1 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 9 4 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 2 6 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 3 2 1 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 6 9 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 3 6 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 9 3 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 3 . 4 5 0 4 7 9         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 0 9 2 0 0 0  
 = = >   T e s t i n g   E p o c h :   9 6  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 0 3 0 4 8         T e s t i n g   e r r o r   r a t e :   2 3 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 5 6 4 4 8         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 4 2 4 8 8 8         T o t a l   t e s t i n g   e r r o r   r a t e :   2 4 . 0 3 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   9 7  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 1 3 9 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 1 3 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 2 0 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 1 4 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 3 6 9 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 9 5 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 6 0 4 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 8 5 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 1 8 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 3 0 1 4 3         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 7 4 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 4 3 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 5 4 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 1 0 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 4 6 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 7 5 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 9 4 1 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 5 4 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 2 1 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 8 8 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 3 . 8 2 4 0 7 4         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 2 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   9 7  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 2 1 6 6 1         T e s t i n g   e r r o r   r a t e :   2 5 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 7 5 6 0 8         T e s t i n g   e r r o r   r a t e :   2 7 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 7 6 4 3 8 1         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 6 6 0 0 0 0  
 = = >   S a v i n g   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   9 8  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 1 2 0 9 0 7         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 2 5 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 8 5 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 4 4 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 9 1 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 5 4 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 7 1 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 3 4 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 8 0 8 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 2 6 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 1 3 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 6 7 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 2 0 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 0 3 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 2 6 9 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 2 5 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 2 7 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 0 7 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 8 2 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 2 8 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 3 . 6 0 6 6 4 5         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 0 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   9 8  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 1 9 8 8 7 1         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 3 0 3 4 3 7         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 7 9 8 3 4 2         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 6 8 0 0 0 0  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   9 9  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 1 8 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 3 8 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 0 5 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 7 9 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 2 8 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 4 5 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 3 6 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 3 0 4 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 4 3 4 1 7         T r a i n i n g   e r r o r   r a t e :   2 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 3 9 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 7 5 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 7 4 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 3 4 0 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 3 2 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 3 3 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 7 8 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 4 9 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 4 6 8         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 2 6 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 6 6 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 3 . 3 2 3 5 9 7         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 0 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   9 9  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 0 2 1 7 9         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 3 1 4 4 8 0         T e s t i n g   e r r o r   r a t e :   2 9 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 1 . 5 8 4 8 5 5         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 4 7 0 0 0 0  
 = = >   S a v i n g   c h e c k p o i n t . .  
 = = >   S e t   l e a r n i n g   r a t e :   0 . 0 0 1 0 0 0  
 = = >   T r a i n i n g   E p o c h :   1 0 0  
 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 8 4 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 2 7 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 7 9 5 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 1 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 7 5 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 2 3 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 2 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 4 0 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 7 9 3 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 3 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 0 6 6 1         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 7 0 3         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 4 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 5 9 6 0         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 1 1 3 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 5 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 5 1 6 2         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 8 4 0 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 6 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 4 1 7 4         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 6 7 4 7         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 7 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 7 1 5         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 4 3 9 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 8 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 0 9 4 1 9         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 0 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 1 3 6 8 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 9 5 0 . 0 0 0 0 0 0 / 1 0 0 0 . 0 0 0 0 0 0   = = >   T r a i n i n g   l o s s :   0 . 0 2 2 3 4 6         T r a i n i n g   e r r o r   r a t e :   0 . 0 0 0 0 0 0  
 = = >   T o t a l   t r a i n i n g   l o s s :   1 3 . 1 2 1 2 3 3         T o t a l   t r a i n i n g   e r r o r   r a t e :   0 . 1 0 0 0 0 0  
 = = >   T e s t i n g   E p o c h :   1 0 0  
 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 5 0 1 3 5         T e s t i n g   e r r o r   r a t e :   2 4 . 0 0 0 0 0 0  
 5 0 . 0 0 0 0 0 0 / 1 0 0 . 0 0 0 0 0 0   = = >   T e s t i n g   l o s s :   1 . 2 7 8 3 5 5         T e s t i n g   e r r o r   r a t e :   3 1 . 0 0 0 0 0 0  
 = = >   T o t a l   t e s t i n g   l o s s :   1 0 2 . 1 4 6 4 5 0         T o t a l   t e s t i n g   e r r o r   r a t e :   2 3 . 8 5 0 0 0 0  
 