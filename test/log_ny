==> Init variables..
==> Init seed..
==> Download data..
Files already downloaded and verified
==> Calculate mean and std..
==> Prepare training transform..
==> Prepare testing transform..
==> Init dataloader..
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
==> Set learning rate: 0.010000
==> Training Epoch: 1
0.000000/1000.000000 ==> Training loss: 4.829558    Training error rate: 100.000000
100.000000/1000.000000 ==> Training loss: 4.466436    Training error rate: 100.000000
200.000000/1000.000000 ==> Training loss: 4.279488    Training error rate: 96.000000
300.000000/1000.000000 ==> Training loss: 3.921916    Training error rate: 96.000000
400.000000/1000.000000 ==> Training loss: 3.925182    Training error rate: 88.000000
500.000000/1000.000000 ==> Training loss: 3.965589    Training error rate: 90.000000
600.000000/1000.000000 ==> Training loss: 3.818026    Training error rate: 88.000000
700.000000/1000.000000 ==> Training loss: 3.812208    Training error rate: 96.000000
800.000000/1000.000000 ==> Training loss: 3.737211    Training error rate: 88.000000
900.000000/1000.000000 ==> Training loss: 3.827996    Training error rate: 88.000000
==> Total training loss: 4026.140430    Total training error rate: 91.808000
==> Testing Epoch: 1
0.000000/100.000000 ==> Testing loss: 3.751921    Testing error rate: 94.000000
==> Total testing loss: 360.997999    Total testing error rate: 85.670000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 2
0.000000/1000.000000 ==> Training loss: 3.837968    Training error rate: 82.000000
100.000000/1000.000000 ==> Training loss: 3.677971    Training error rate: 84.000000
200.000000/1000.000000 ==> Training loss: 3.512146    Training error rate: 80.000000
300.000000/1000.000000 ==> Training loss: 3.557781    Training error rate: 80.000000
400.000000/1000.000000 ==> Training loss: 3.260201    Training error rate: 82.000000
500.000000/1000.000000 ==> Training loss: 2.986341    Training error rate: 76.000000
600.000000/1000.000000 ==> Training loss: 3.063089    Training error rate: 90.000000
700.000000/1000.000000 ==> Training loss: 2.732310    Training error rate: 62.000000
800.000000/1000.000000 ==> Training loss: 3.481169    Training error rate: 92.000000
900.000000/1000.000000 ==> Training loss: 2.945397    Training error rate: 68.000000
==> Total training loss: 3328.362449    Total training error rate: 81.686000
==> Testing Epoch: 2
0.000000/100.000000 ==> Testing loss: 3.267619    Testing error rate: 77.000000
==> Total testing loss: 311.849465    Total testing error rate: 77.140000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 3
0.000000/1000.000000 ==> Training loss: 2.908685    Training error rate: 78.000000
100.000000/1000.000000 ==> Training loss: 3.136957    Training error rate: 70.000000
200.000000/1000.000000 ==> Training loss: 3.128044    Training error rate: 82.000000
300.000000/1000.000000 ==> Training loss: 2.717954    Training error rate: 76.000000
400.000000/1000.000000 ==> Training loss: 2.645749    Training error rate: 68.000000
500.000000/1000.000000 ==> Training loss: 2.724747    Training error rate: 72.000000
600.000000/1000.000000 ==> Training loss: 2.964746    Training error rate: 66.000000
700.000000/1000.000000 ==> Training loss: 2.441830    Training error rate: 68.000000
800.000000/1000.000000 ==> Training loss: 2.486789    Training error rate: 68.000000
900.000000/1000.000000 ==> Training loss: 2.661247    Training error rate: 66.000000
==> Total training loss: 2805.026366    Total training error rate: 71.756000
==> Testing Epoch: 3
0.000000/100.000000 ==> Testing loss: 2.874207    Testing error rate: 64.000000
==> Total testing loss: 271.386006    Total testing error rate: 68.760000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 4
0.000000/1000.000000 ==> Training loss: 2.361497    Training error rate: 68.000000
100.000000/1000.000000 ==> Training loss: 2.575184    Training error rate: 70.000000
200.000000/1000.000000 ==> Training loss: 2.461821    Training error rate: 62.000000
300.000000/1000.000000 ==> Training loss: 2.294019    Training error rate: 68.000000
400.000000/1000.000000 ==> Training loss: 2.556206    Training error rate: 68.000000
500.000000/1000.000000 ==> Training loss: 2.189085    Training error rate: 66.000000
600.000000/1000.000000 ==> Training loss: 2.443721    Training error rate: 68.000000
700.000000/1000.000000 ==> Training loss: 2.090721    Training error rate: 58.000000
800.000000/1000.000000 ==> Training loss: 2.203427    Training error rate: 58.000000
900.000000/1000.000000 ==> Training loss: 2.349290    Training error rate: 68.000000
==> Total training loss: 2398.913685    Total training error rate: 63.482000
==> Testing Epoch: 4
0.000000/100.000000 ==> Testing loss: 2.422368    Testing error rate: 63.000000
==> Total testing loss: 243.013999    Total testing error rate: 63.290000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 5
0.000000/1000.000000 ==> Training loss: 1.730997    Training error rate: 46.000000
100.000000/1000.000000 ==> Training loss: 1.939878    Training error rate: 52.000000
200.000000/1000.000000 ==> Training loss: 2.112050    Training error rate: 62.000000
300.000000/1000.000000 ==> Training loss: 1.913703    Training error rate: 44.000000
400.000000/1000.000000 ==> Training loss: 2.086591    Training error rate: 54.000000
500.000000/1000.000000 ==> Training loss: 2.071121    Training error rate: 54.000000
600.000000/1000.000000 ==> Training loss: 2.573063    Training error rate: 66.000000
700.000000/1000.000000 ==> Training loss: 2.697075    Training error rate: 64.000000
800.000000/1000.000000 ==> Training loss: 1.599023    Training error rate: 48.000000
900.000000/1000.000000 ==> Training loss: 2.128482    Training error rate: 56.000000
==> Total training loss: 2104.315667    Total training error rate: 57.062000
==> Testing Epoch: 5
0.000000/100.000000 ==> Testing loss: 2.112673    Testing error rate: 60.000000
==> Total testing loss: 212.427609    Total testing error rate: 57.020000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 6
0.000000/1000.000000 ==> Training loss: 1.910872    Training error rate: 60.000000
100.000000/1000.000000 ==> Training loss: 1.897591    Training error rate: 56.000000
200.000000/1000.000000 ==> Training loss: 1.605577    Training error rate: 46.000000
300.000000/1000.000000 ==> Training loss: 1.646226    Training error rate: 50.000000
400.000000/1000.000000 ==> Training loss: 2.470580    Training error rate: 74.000000
500.000000/1000.000000 ==> Training loss: 1.841860    Training error rate: 48.000000
600.000000/1000.000000 ==> Training loss: 1.949143    Training error rate: 52.000000
700.000000/1000.000000 ==> Training loss: 1.679940    Training error rate: 48.000000
800.000000/1000.000000 ==> Training loss: 1.848240    Training error rate: 50.000000
900.000000/1000.000000 ==> Training loss: 1.652438    Training error rate: 42.000000
==> Total training loss: 1875.133284    Total training error rate: 52.068000
==> Testing Epoch: 6
0.000000/100.000000 ==> Testing loss: 2.145375    Testing error rate: 58.000000
==> Total testing loss: 199.631051    Total testing error rate: 53.640000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 7
0.000000/1000.000000 ==> Training loss: 1.639330    Training error rate: 44.000000
100.000000/1000.000000 ==> Training loss: 2.189410    Training error rate: 56.000000
200.000000/1000.000000 ==> Training loss: 1.341021    Training error rate: 38.000000
300.000000/1000.000000 ==> Training loss: 1.616596    Training error rate: 44.000000
400.000000/1000.000000 ==> Training loss: 1.380426    Training error rate: 40.000000
500.000000/1000.000000 ==> Training loss: 2.155650    Training error rate: 56.000000
600.000000/1000.000000 ==> Training loss: 1.385332    Training error rate: 50.000000
700.000000/1000.000000 ==> Training loss: 1.604444    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.797643    Training error rate: 50.000000
900.000000/1000.000000 ==> Training loss: 1.926243    Training error rate: 50.000000
==> Total training loss: 1712.310492    Total training error rate: 48.010000
==> Testing Epoch: 7
0.000000/100.000000 ==> Testing loss: 1.778646    Testing error rate: 42.000000
==> Total testing loss: 181.928334    Total testing error rate: 48.840000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 8
0.000000/1000.000000 ==> Training loss: 1.861817    Training error rate: 56.000000
100.000000/1000.000000 ==> Training loss: 1.994103    Training error rate: 50.000000
200.000000/1000.000000 ==> Training loss: 1.605615    Training error rate: 44.000000
300.000000/1000.000000 ==> Training loss: 1.283338    Training error rate: 42.000000
400.000000/1000.000000 ==> Training loss: 1.563886    Training error rate: 38.000000
500.000000/1000.000000 ==> Training loss: 1.524676    Training error rate: 48.000000
600.000000/1000.000000 ==> Training loss: 1.342047    Training error rate: 40.000000
700.000000/1000.000000 ==> Training loss: 1.458136    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.410724    Training error rate: 40.000000
900.000000/1000.000000 ==> Training loss: 1.809129    Training error rate: 48.000000
==> Total training loss: 1578.119414    Total training error rate: 44.772000
==> Testing Epoch: 8
0.000000/100.000000 ==> Testing loss: 1.607756    Testing error rate: 45.000000
==> Total testing loss: 165.878710    Total testing error rate: 46.540000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 9
0.000000/1000.000000 ==> Training loss: 1.486775    Training error rate: 46.000000
100.000000/1000.000000 ==> Training loss: 1.752088    Training error rate: 54.000000
200.000000/1000.000000 ==> Training loss: 1.412193    Training error rate: 40.000000
300.000000/1000.000000 ==> Training loss: 1.322171    Training error rate: 36.000000
400.000000/1000.000000 ==> Training loss: 1.520271    Training error rate: 40.000000
500.000000/1000.000000 ==> Training loss: 1.395669    Training error rate: 44.000000
600.000000/1000.000000 ==> Training loss: 1.093205    Training error rate: 42.000000
700.000000/1000.000000 ==> Training loss: 1.582156    Training error rate: 46.000000
800.000000/1000.000000 ==> Training loss: 1.684539    Training error rate: 48.000000
900.000000/1000.000000 ==> Training loss: 1.250747    Training error rate: 28.000000
==> Total training loss: 1467.854387    Total training error rate: 42.114000
==> Testing Epoch: 9
0.000000/100.000000 ==> Testing loss: 1.580653    Testing error rate: 43.000000
==> Total testing loss: 158.292061    Total testing error rate: 44.090000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 10
0.000000/1000.000000 ==> Training loss: 0.964301    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 1.279634    Training error rate: 38.000000
200.000000/1000.000000 ==> Training loss: 1.473038    Training error rate: 40.000000
300.000000/1000.000000 ==> Training loss: 1.319067    Training error rate: 32.000000
400.000000/1000.000000 ==> Training loss: 1.474636    Training error rate: 38.000000
500.000000/1000.000000 ==> Training loss: 1.236006    Training error rate: 42.000000
600.000000/1000.000000 ==> Training loss: 1.221545    Training error rate: 36.000000
700.000000/1000.000000 ==> Training loss: 1.268107    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.304336    Training error rate: 40.000000
900.000000/1000.000000 ==> Training loss: 2.092972    Training error rate: 58.000000
==> Total training loss: 1377.656275    Total training error rate: 39.612000
==> Testing Epoch: 10
0.000000/100.000000 ==> Testing loss: 1.402237    Testing error rate: 40.000000
==> Total testing loss: 157.985924    Total testing error rate: 43.590000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 11
0.000000/1000.000000 ==> Training loss: 1.150578    Training error rate: 30.000000
100.000000/1000.000000 ==> Training loss: 1.043729    Training error rate: 24.000000
200.000000/1000.000000 ==> Training loss: 1.549725    Training error rate: 40.000000
300.000000/1000.000000 ==> Training loss: 1.303639    Training error rate: 42.000000
400.000000/1000.000000 ==> Training loss: 0.942845    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 1.318079    Training error rate: 32.000000
600.000000/1000.000000 ==> Training loss: 1.103444    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.999102    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 1.165166    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 1.895118    Training error rate: 52.000000
==> Total training loss: 1287.342849    Total training error rate: 37.256000
==> Testing Epoch: 11
0.000000/100.000000 ==> Testing loss: 1.693124    Testing error rate: 45.000000
==> Total testing loss: 152.282583    Total testing error rate: 42.350000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 12
0.000000/1000.000000 ==> Training loss: 1.518520    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 1.055205    Training error rate: 34.000000
200.000000/1000.000000 ==> Training loss: 1.154025    Training error rate: 38.000000
300.000000/1000.000000 ==> Training loss: 1.048600    Training error rate: 34.000000
400.000000/1000.000000 ==> Training loss: 1.010055    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 1.363673    Training error rate: 42.000000
600.000000/1000.000000 ==> Training loss: 1.171769    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 1.079425    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 0.815016    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 1.518944    Training error rate: 40.000000
==> Total training loss: 1204.726840    Total training error rate: 35.412000
==> Testing Epoch: 12
0.000000/100.000000 ==> Testing loss: 1.375928    Testing error rate: 38.000000
==> Total testing loss: 138.837462    Total testing error rate: 39.210000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 13
0.000000/1000.000000 ==> Training loss: 1.385865    Training error rate: 36.000000
100.000000/1000.000000 ==> Training loss: 1.144425    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 1.237426    Training error rate: 40.000000
300.000000/1000.000000 ==> Training loss: 0.973846    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 1.339926    Training error rate: 36.000000
500.000000/1000.000000 ==> Training loss: 1.026907    Training error rate: 34.000000
600.000000/1000.000000 ==> Training loss: 1.182673    Training error rate: 38.000000
700.000000/1000.000000 ==> Training loss: 1.212048    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.501143    Training error rate: 42.000000
900.000000/1000.000000 ==> Training loss: 1.305197    Training error rate: 46.000000
==> Total training loss: 1142.758789    Total training error rate: 33.718000
==> Testing Epoch: 13
0.000000/100.000000 ==> Testing loss: 1.342412    Testing error rate: 40.000000
==> Total testing loss: 141.939670    Total testing error rate: 39.310000
==> Set learning rate: 0.010000
==> Training Epoch: 14
0.000000/1000.000000 ==> Training loss: 0.978983    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.892468    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.265994    Training error rate: 44.000000
300.000000/1000.000000 ==> Training loss: 1.047080    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 0.861704    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 0.946411    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 1.563210    Training error rate: 42.000000
700.000000/1000.000000 ==> Training loss: 1.398085    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 0.952099    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 1.099341    Training error rate: 36.000000
==> Total training loss: 1085.041973    Total training error rate: 31.924000
==> Testing Epoch: 14
0.000000/100.000000 ==> Testing loss: 1.202197    Testing error rate: 31.000000
==> Total testing loss: 134.028651    Total testing error rate: 37.770000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 15
0.000000/1000.000000 ==> Training loss: 1.479335    Training error rate: 42.000000
100.000000/1000.000000 ==> Training loss: 0.985054    Training error rate: 24.000000
200.000000/1000.000000 ==> Training loss: 1.088652    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 0.742065    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.841343    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.784502    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 1.278125    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 0.914699    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 1.166848    Training error rate: 30.000000
900.000000/1000.000000 ==> Training loss: 1.186545    Training error rate: 36.000000
==> Total training loss: 1035.131272    Total training error rate: 30.736000
==> Testing Epoch: 15
0.000000/100.000000 ==> Testing loss: 1.413671    Testing error rate: 42.000000
==> Total testing loss: 143.862405    Total testing error rate: 39.910000
==> Set learning rate: 0.010000
==> Training Epoch: 16
0.000000/1000.000000 ==> Training loss: 1.065157    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 0.808617    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 1.142685    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 0.697836    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 1.068024    Training error rate: 42.000000
500.000000/1000.000000 ==> Training loss: 0.996128    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 1.175974    Training error rate: 36.000000
700.000000/1000.000000 ==> Training loss: 0.917403    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 0.904296    Training error rate: 30.000000
900.000000/1000.000000 ==> Training loss: 0.784630    Training error rate: 18.000000
==> Total training loss: 978.590430    Total training error rate: 29.224000
==> Testing Epoch: 16
0.000000/100.000000 ==> Testing loss: 1.257508    Testing error rate: 33.000000
==> Total testing loss: 131.515891    Total testing error rate: 36.560000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 17
0.000000/1000.000000 ==> Training loss: 1.188500    Training error rate: 38.000000
100.000000/1000.000000 ==> Training loss: 0.882042    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 1.000715    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.816502    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 1.128729    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.726732    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.986977    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.869303    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 0.681202    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.894789    Training error rate: 30.000000
==> Total training loss: 942.136851    Total training error rate: 28.338000
==> Testing Epoch: 17
0.000000/100.000000 ==> Testing loss: 1.271123    Testing error rate: 33.000000
==> Total testing loss: 140.829438    Total testing error rate: 38.370000
==> Set learning rate: 0.010000
==> Training Epoch: 18
0.000000/1000.000000 ==> Training loss: 1.079148    Training error rate: 36.000000
100.000000/1000.000000 ==> Training loss: 0.949722    Training error rate: 30.000000
200.000000/1000.000000 ==> Training loss: 1.110864    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 0.948546    Training error rate: 32.000000
400.000000/1000.000000 ==> Training loss: 0.705043    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.999207    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.878159    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 1.002304    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 1.109603    Training error rate: 38.000000
900.000000/1000.000000 ==> Training loss: 0.693505    Training error rate: 22.000000
==> Total training loss: 902.377365    Total training error rate: 27.154000
==> Testing Epoch: 18
0.000000/100.000000 ==> Testing loss: 1.180476    Testing error rate: 31.000000
==> Total testing loss: 125.769582    Total testing error rate: 34.280000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 19
0.000000/1000.000000 ==> Training loss: 1.470335    Training error rate: 48.000000
100.000000/1000.000000 ==> Training loss: 0.667512    Training error rate: 24.000000
200.000000/1000.000000 ==> Training loss: 0.789627    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.992616    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.868915    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.921455    Training error rate: 32.000000
600.000000/1000.000000 ==> Training loss: 0.948411    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.711045    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 1.169680    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.899501    Training error rate: 32.000000
==> Total training loss: 851.136266    Total training error rate: 25.694000
==> Testing Epoch: 19
0.000000/100.000000 ==> Testing loss: 1.408333    Testing error rate: 43.000000
==> Total testing loss: 134.547570    Total testing error rate: 36.760000
==> Set learning rate: 0.010000
==> Training Epoch: 20
0.000000/1000.000000 ==> Training loss: 0.762030    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.618394    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.993029    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 0.874291    Training error rate: 24.000000
400.000000/1000.000000 ==> Training loss: 0.801013    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.832443    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 0.785586    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.579455    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.533311    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.732512    Training error rate: 24.000000
==> Total training loss: 830.963798    Total training error rate: 25.238000
==> Testing Epoch: 20
0.000000/100.000000 ==> Testing loss: 1.290081    Testing error rate: 33.000000
==> Total testing loss: 129.192541    Total testing error rate: 35.410000
==> Set learning rate: 0.010000
==> Training Epoch: 21
0.000000/1000.000000 ==> Training loss: 1.069793    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 0.827455    Training error rate: 26.000000
200.000000/1000.000000 ==> Training loss: 0.573795    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.792241    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.575479    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.857297    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.751502    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.857027    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 1.246112    Training error rate: 38.000000
900.000000/1000.000000 ==> Training loss: 0.751098    Training error rate: 28.000000
==> Total training loss: 791.973427    Total training error rate: 24.180000
==> Testing Epoch: 21
0.000000/100.000000 ==> Testing loss: 1.019667    Testing error rate: 29.000000
==> Total testing loss: 129.773381    Total testing error rate: 35.440000
==> Set learning rate: 0.010000
==> Training Epoch: 22
0.000000/1000.000000 ==> Training loss: 0.803197    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.599475    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.834085    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.785379    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.965565    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 1.057683    Training error rate: 32.000000
600.000000/1000.000000 ==> Training loss: 0.615224    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.469527    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.784488    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.890629    Training error rate: 30.000000
==> Total training loss: 767.513360    Total training error rate: 23.286000
==> Testing Epoch: 22
0.000000/100.000000 ==> Testing loss: 1.285250    Testing error rate: 39.000000
==> Total testing loss: 138.181796    Total testing error rate: 36.460000
==> Set learning rate: 0.010000
==> Training Epoch: 23
0.000000/1000.000000 ==> Training loss: 0.865679    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 0.521916    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.580382    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.437893    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.809624    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.748643    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.870219    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 0.753708    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 0.845604    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.714142    Training error rate: 24.000000
==> Total training loss: 745.802195    Total training error rate: 22.862000
==> Testing Epoch: 23
0.000000/100.000000 ==> Testing loss: 1.177754    Testing error rate: 34.000000
==> Total testing loss: 132.586727    Total testing error rate: 35.040000
==> Set learning rate: 0.010000
==> Training Epoch: 24
0.000000/1000.000000 ==> Training loss: 0.526596    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.935033    Training error rate: 22.000000
200.000000/1000.000000 ==> Training loss: 0.803342    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.605559    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.440285    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.869532    Training error rate: 32.000000
600.000000/1000.000000 ==> Training loss: 0.855909    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.575946    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.793063    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.718158    Training error rate: 20.000000
==> Total training loss: 710.467281    Total training error rate: 21.804000
==> Testing Epoch: 24
0.000000/100.000000 ==> Testing loss: 1.094238    Testing error rate: 29.000000
==> Total testing loss: 126.702406    Total testing error rate: 33.660000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 25
0.000000/1000.000000 ==> Training loss: 0.560678    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.617599    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.570951    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.634076    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.386994    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.775518    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.642251    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.814923    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.615611    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.542912    Training error rate: 18.000000
==> Total training loss: 686.139633    Total training error rate: 21.248000
==> Testing Epoch: 25
0.000000/100.000000 ==> Testing loss: 1.169142    Testing error rate: 34.000000
==> Total testing loss: 127.661085    Total testing error rate: 34.360000
==> Set learning rate: 0.010000
==> Training Epoch: 26
0.000000/1000.000000 ==> Training loss: 0.610546    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.505433    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.437116    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.587011    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.717798    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 0.480004    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.726267    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.542370    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.660904    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.654080    Training error rate: 20.000000
==> Total training loss: 665.227412    Total training error rate: 20.724000
==> Testing Epoch: 26
0.000000/100.000000 ==> Testing loss: 1.407978    Testing error rate: 34.000000
==> Total testing loss: 131.001376    Total testing error rate: 34.300000
==> Set learning rate: 0.010000
==> Training Epoch: 27
0.000000/1000.000000 ==> Training loss: 0.439846    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.683455    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.987247    Training error rate: 28.000000
300.000000/1000.000000 ==> Training loss: 0.651617    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.528369    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.778778    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.665508    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.824396    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.490887    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.522041    Training error rate: 16.000000
==> Total training loss: 648.424273    Total training error rate: 20.008000
==> Testing Epoch: 27
0.000000/100.000000 ==> Testing loss: 1.268005    Testing error rate: 29.000000
==> Total testing loss: 135.042536    Total testing error rate: 35.160000
==> Set learning rate: 0.010000
==> Training Epoch: 28
0.000000/1000.000000 ==> Training loss: 0.880940    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.392410    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.503050    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.824844    Training error rate: 24.000000
400.000000/1000.000000 ==> Training loss: 0.647637    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 0.430254    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.862017    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.609213    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.969388    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.423412    Training error rate: 14.000000
==> Total training loss: 625.459615    Total training error rate: 19.512000
==> Testing Epoch: 28
0.000000/100.000000 ==> Testing loss: 1.328363    Testing error rate: 42.000000
==> Total testing loss: 126.777917    Total testing error rate: 33.330000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 29
0.000000/1000.000000 ==> Training loss: 0.480417    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.284818    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.485152    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.495359    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.629835    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.559826    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.381572    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.691857    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.536667    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 1.016779    Training error rate: 22.000000
==> Total training loss: 617.311482    Total training error rate: 19.134000
==> Testing Epoch: 29
0.000000/100.000000 ==> Testing loss: 1.321391    Testing error rate: 31.000000
==> Total testing loss: 129.853534    Total testing error rate: 33.940000
==> Set learning rate: 0.010000
==> Training Epoch: 30
0.000000/1000.000000 ==> Training loss: 0.489260    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.349893    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.583152    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.381941    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.559603    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.657855    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.719122    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.455169    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.368374    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.448267    Training error rate: 14.000000
==> Total training loss: 596.244561    Total training error rate: 18.450000
==> Testing Epoch: 30
0.000000/100.000000 ==> Testing loss: 1.423425    Testing error rate: 32.000000
==> Total testing loss: 136.697095    Total testing error rate: 35.080000
==> Set learning rate: 0.010000
==> Training Epoch: 31
0.000000/1000.000000 ==> Training loss: 0.406724    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.588856    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.267457    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.539277    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.538269    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.629017    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.695787    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.659943    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.523533    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.928901    Training error rate: 42.000000
==> Total training loss: 583.788908    Total training error rate: 18.040000
==> Testing Epoch: 31
0.000000/100.000000 ==> Testing loss: 1.349869    Testing error rate: 34.000000
==> Total testing loss: 131.394507    Total testing error rate: 34.390000
==> Set learning rate: 0.010000
==> Training Epoch: 32
0.000000/1000.000000 ==> Training loss: 0.311042    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.655770    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.751238    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.373673    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.386664    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.577992    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.609340    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.730714    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.498663    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.442479    Training error rate: 12.000000
==> Total training loss: 582.268582    Total training error rate: 18.294000
==> Testing Epoch: 32
0.000000/100.000000 ==> Testing loss: 1.394488    Testing error rate: 33.000000
==> Total testing loss: 133.826280    Total testing error rate: 34.140000
==> Set learning rate: 0.010000
==> Training Epoch: 33
0.000000/1000.000000 ==> Training loss: 0.493943    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.394799    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.319120    Training error rate: 10.000000
300.000000/1000.000000 ==> Training loss: 0.708194    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.640950    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.549188    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.531338    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.437504    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.470191    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.494362    Training error rate: 18.000000
==> Total training loss: 553.862994    Total training error rate: 17.246000
==> Testing Epoch: 33
0.000000/100.000000 ==> Testing loss: 1.359432    Testing error rate: 36.000000
==> Total testing loss: 133.839902    Total testing error rate: 34.950000
==> Set learning rate: 0.010000
==> Training Epoch: 34
0.000000/1000.000000 ==> Training loss: 0.646141    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.545764    Training error rate: 22.000000
200.000000/1000.000000 ==> Training loss: 0.612359    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.587187    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.542813    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.670373    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.595136    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.573754    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.638862    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.786390    Training error rate: 24.000000
==> Total training loss: 549.588079    Total training error rate: 17.190000
==> Testing Epoch: 34
0.000000/100.000000 ==> Testing loss: 1.388984    Testing error rate: 35.000000
==> Total testing loss: 138.652693    Total testing error rate: 34.950000
==> Set learning rate: 0.010000
==> Training Epoch: 35
0.000000/1000.000000 ==> Training loss: 0.427092    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.524279    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.294267    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.412640    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.666551    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.545618    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.389021    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.367773    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.517008    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.464080    Training error rate: 10.000000
==> Total training loss: 532.358437    Total training error rate: 16.728000
==> Testing Epoch: 35
0.000000/100.000000 ==> Testing loss: 1.212646    Testing error rate: 28.000000
==> Total testing loss: 129.706560    Total testing error rate: 32.910000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 36
0.000000/1000.000000 ==> Training loss: 0.459283    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.316574    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.537642    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.519823    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.692072    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.500588    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.757364    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.595130    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.763763    Training error rate: 26.000000
900.000000/1000.000000 ==> Training loss: 0.608755    Training error rate: 20.000000
==> Total training loss: 530.960121    Total training error rate: 16.738000
==> Testing Epoch: 36
0.000000/100.000000 ==> Testing loss: 1.326954    Testing error rate: 34.000000
==> Total testing loss: 127.923082    Total testing error rate: 32.550000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 37
0.000000/1000.000000 ==> Training loss: 0.540165    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.497326    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.602545    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.481503    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.603768    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.263585    Training error rate: 6.000000
600.000000/1000.000000 ==> Training loss: 0.405968    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.612028    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.303210    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.457102    Training error rate: 18.000000
==> Total training loss: 518.921100    Total training error rate: 16.200000
==> Testing Epoch: 37
0.000000/100.000000 ==> Testing loss: 1.347698    Testing error rate: 35.000000
==> Total testing loss: 127.177025    Total testing error rate: 32.090000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 38
0.000000/1000.000000 ==> Training loss: 0.363047    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.368025    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.519446    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.614288    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.469826    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.635466    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.691232    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.362493    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.366658    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.486049    Training error rate: 14.000000
==> Total training loss: 505.590184    Total training error rate: 15.848000
==> Testing Epoch: 38
0.000000/100.000000 ==> Testing loss: 1.401174    Testing error rate: 38.000000
==> Total testing loss: 138.637150    Total testing error rate: 34.600000
==> Set learning rate: 0.010000
==> Training Epoch: 39
0.000000/1000.000000 ==> Training loss: 0.321788    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.545106    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.478833    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.377637    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.291705    Training error rate: 8.000000
500.000000/1000.000000 ==> Training loss: 0.678008    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.497104    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.321187    Training error rate: 8.000000
800.000000/1000.000000 ==> Training loss: 0.690937    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.419758    Training error rate: 14.000000
==> Total training loss: 496.849489    Total training error rate: 15.396000
==> Testing Epoch: 39
0.000000/100.000000 ==> Testing loss: 1.593065    Testing error rate: 32.000000
==> Total testing loss: 133.381059    Total testing error rate: 33.400000
==> Set learning rate: 0.010000
==> Training Epoch: 40
0.000000/1000.000000 ==> Training loss: 0.470753    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.160629    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.262836    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.320602    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.347845    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.311400    Training error rate: 8.000000
600.000000/1000.000000 ==> Training loss: 0.470456    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.644958    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.387903    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.888987    Training error rate: 26.000000
==> Total training loss: 481.255854    Total training error rate: 15.078000
==> Testing Epoch: 40
0.000000/100.000000 ==> Testing loss: 1.361328    Testing error rate: 32.000000
==> Total testing loss: 136.291426    Total testing error rate: 33.890000
==> Set learning rate: 0.010000
==> Training Epoch: 41
0.000000/1000.000000 ==> Training loss: 0.383069    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.279559    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.469706    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.316997    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.353177    Training error rate: 8.000000
500.000000/1000.000000 ==> Training loss: 0.603025    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 0.452990    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.489628    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.726114    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.572965    Training error rate: 20.000000
==> Total training loss: 477.333023    Total training error rate: 14.872000
==> Testing Epoch: 41
0.000000/100.000000 ==> Testing loss: 1.459750    Testing error rate: 36.000000
==> Total testing loss: 135.384638    Total testing error rate: 33.740000
==> Set learning rate: 0.010000
==> Training Epoch: 42
0.000000/1000.000000 ==> Training loss: 0.497934    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.304925    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.692245    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.374382    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.253188    Training error rate: 4.000000
500.000000/1000.000000 ==> Training loss: 0.437763    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.674875    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.461980    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.374029    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.376326    Training error rate: 12.000000
==> Total training loss: 477.963049    Total training error rate: 14.972000
==> Testing Epoch: 42
0.000000/100.000000 ==> Testing loss: 1.430479    Testing error rate: 38.000000
==> Total testing loss: 134.643448    Total testing error rate: 33.870000
==> Set learning rate: 0.010000
==> Training Epoch: 43
0.000000/1000.000000 ==> Training loss: 0.372875    Training error rate: 8.000000
100.000000/1000.000000 ==> Training loss: 0.406391    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.419553    Training error rate: 10.000000
300.000000/1000.000000 ==> Training loss: 0.496074    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.505752    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.435308    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.293593    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.492814    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.445421    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.415201    Training error rate: 12.000000
==> Total training loss: 462.302860    Total training error rate: 14.270000
==> Testing Epoch: 43
0.000000/100.000000 ==> Testing loss: 1.397782    Testing error rate: 32.000000
==> Total testing loss: 140.237323    Total testing error rate: 34.130000
==> Set learning rate: 0.010000
==> Training Epoch: 44
0.000000/1000.000000 ==> Training loss: 0.482452    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.512300    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.333628    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.830994    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.380909    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.265562    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.520520    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.445300    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.556660    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.316591    Training error rate: 12.000000
==> Total training loss: 450.099102    Total training error rate: 14.092000
==> Testing Epoch: 44
0.000000/100.000000 ==> Testing loss: 1.319285    Testing error rate: 35.000000
==> Total testing loss: 139.003276    Total testing error rate: 34.140000
==> Set learning rate: 0.010000
==> Training Epoch: 45
0.000000/1000.000000 ==> Training loss: 0.432175    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.356910    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.250913    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.387661    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.431354    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.431524    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.331517    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.298183    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.299508    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.357474    Training error rate: 12.000000
==> Total training loss: 434.429895    Total training error rate: 13.634000
==> Testing Epoch: 45
0.000000/100.000000 ==> Testing loss: 1.331796    Testing error rate: 37.000000
==> Total testing loss: 141.944604    Total testing error rate: 34.250000
==> Set learning rate: 0.010000
==> Training Epoch: 46
0.000000/1000.000000 ==> Training loss: 0.540602    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.303029    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.369080    Training error rate: 10.000000
300.000000/1000.000000 ==> Training loss: 0.407865    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.384972    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.395416    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.356599    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.541596    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.321745    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.484688    Training error rate: 10.000000
==> Total training loss: 449.428145    Total training error rate: 14.130000
==> Testing Epoch: 46
0.000000/100.000000 ==> Testing loss: 1.668113    Testing error rate: 39.000000
==> Total testing loss: 150.382358    Total testing error rate: 36.040000
==> Set learning rate: 0.010000
==> Training Epoch: 47
0.000000/1000.000000 ==> Training loss: 0.452079    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.388237    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.277789    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.407368    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.465461    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.279275    Training error rate: 8.000000
600.000000/1000.000000 ==> Training loss: 0.506932    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.436852    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.532311    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.568260    Training error rate: 18.000000
==> Total training loss: 446.718925    Total training error rate: 14.112000
==> Testing Epoch: 47
0.000000/100.000000 ==> Testing loss: 1.402603    Testing error rate: 35.000000
==> Total testing loss: 142.585847    Total testing error rate: 33.510000
==> Set learning rate: 0.010000
==> Training Epoch: 48
0.000000/1000.000000 ==> Training loss: 0.340694    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.305638    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.292577    Training error rate: 10.000000
300.000000/1000.000000 ==> Training loss: 0.318274    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.355289    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.393527    Training error rate: 8.000000
600.000000/1000.000000 ==> Training loss: 0.496938    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.760723    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.440647    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.615516    Training error rate: 12.000000
==> Total training loss: 429.260102    Total training error rate: 13.526000
==> Testing Epoch: 48
0.000000/100.000000 ==> Testing loss: 1.514852    Testing error rate: 37.000000
==> Total testing loss: 134.955708    Total testing error rate: 32.960000
==> Set learning rate: 0.010000
==> Training Epoch: 49
0.000000/1000.000000 ==> Training loss: 0.427893    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.462163    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.439548    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.415215    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.218610    Training error rate: 6.000000
500.000000/1000.000000 ==> Training loss: 0.496666    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.459780    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.462985    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.358221    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.537841    Training error rate: 22.000000
==> Total training loss: 418.293187    Total training error rate: 13.042000
==> Testing Epoch: 49
0.000000/100.000000 ==> Testing loss: 1.377528    Testing error rate: 34.000000
==> Total testing loss: 137.738937    Total testing error rate: 33.590000
==> Set learning rate: 0.010000
==> Training Epoch: 50
0.000000/1000.000000 ==> Training loss: 0.370413    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.326997    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.318706    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.317263    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.390852    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.408299    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.444599    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.299755    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.525671    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.315207    Training error rate: 8.000000
==> Total training loss: 421.434382    Total training error rate: 13.108000
==> Testing Epoch: 50
0.000000/100.000000 ==> Testing loss: 1.441542    Testing error rate: 33.000000
==> Total testing loss: 153.480330    Total testing error rate: 36.530000
==> Set learning rate: 0.001000
==> Training Epoch: 51
0.000000/1000.000000 ==> Training loss: 0.285863    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.373558    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.117946    Training error rate: 4.000000
300.000000/1000.000000 ==> Training loss: 0.168524    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.185505    Training error rate: 4.000000
500.000000/1000.000000 ==> Training loss: 0.213069    Training error rate: 6.000000
600.000000/1000.000000 ==> Training loss: 0.054156    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.188387    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.289016    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.176069    Training error rate: 6.000000
==> Total training loss: 196.463360    Total training error rate: 5.548000
==> Testing Epoch: 51
0.000000/100.000000 ==> Testing loss: 1.117193    Testing error rate: 31.000000
==> Total testing loss: 99.269895    Total testing error rate: 25.810000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 52
0.000000/1000.000000 ==> Training loss: 0.108671    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.154157    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.119239    Training error rate: 4.000000
300.000000/1000.000000 ==> Training loss: 0.102659    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.103601    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.091132    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.155612    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.095699    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.056735    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.105447    Training error rate: 2.000000
==> Total training loss: 112.991492    Total training error rate: 2.748000
==> Testing Epoch: 52
0.000000/100.000000 ==> Testing loss: 1.098783    Testing error rate: 30.000000
==> Total testing loss: 99.111838    Total testing error rate: 25.170000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 53
0.000000/1000.000000 ==> Training loss: 0.091176    Training error rate: 4.000000
100.000000/1000.000000 ==> Training loss: 0.077788    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.055597    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.071437    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.110582    Training error rate: 6.000000
500.000000/1000.000000 ==> Training loss: 0.063068    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.053052    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.062663    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.028231    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.125355    Training error rate: 4.000000
==> Total training loss: 91.182135    Total training error rate: 2.178000
==> Testing Epoch: 53
0.000000/100.000000 ==> Testing loss: 1.086543    Testing error rate: 29.000000
==> Total testing loss: 97.607541    Total testing error rate: 24.800000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 54
0.000000/1000.000000 ==> Training loss: 0.147504    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.035292    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.151543    Training error rate: 4.000000
300.000000/1000.000000 ==> Training loss: 0.076932    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.047784    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.056583    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.096833    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.044773    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.072979    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.064798    Training error rate: 2.000000
==> Total training loss: 77.386284    Total training error rate: 1.742000
==> Testing Epoch: 54
0.000000/100.000000 ==> Testing loss: 1.110107    Testing error rate: 30.000000
==> Total testing loss: 98.164128    Total testing error rate: 24.490000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 55
0.000000/1000.000000 ==> Training loss: 0.048691    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.075968    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.090324    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.053328    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.062129    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.056035    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.049513    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.050203    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.060937    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.075403    Training error rate: 4.000000
==> Total training loss: 67.595770    Total training error rate: 1.480000
==> Testing Epoch: 55
0.000000/100.000000 ==> Testing loss: 1.132598    Testing error rate: 33.000000
==> Total testing loss: 98.026315    Total testing error rate: 24.190000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 56
0.000000/1000.000000 ==> Training loss: 0.058442    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.036671    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.052523    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.117993    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.029831    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.030489    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.048452    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.069971    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.081576    Training error rate: 6.000000
900.000000/1000.000000 ==> Training loss: 0.026058    Training error rate: 0.000000
==> Total training loss: 59.272013    Total training error rate: 1.160000
==> Testing Epoch: 56
0.000000/100.000000 ==> Testing loss: 1.148732    Testing error rate: 29.000000
==> Total testing loss: 99.051024    Total testing error rate: 24.180000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 57
0.000000/1000.000000 ==> Training loss: 0.068290    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.046752    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.031319    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.061557    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.029028    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.027634    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.030808    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.118869    Training error rate: 4.000000
800.000000/1000.000000 ==> Training loss: 0.060435    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.060954    Training error rate: 2.000000
==> Total training loss: 55.337727    Total training error rate: 1.094000
==> Testing Epoch: 57
0.000000/100.000000 ==> Testing loss: 1.176452    Testing error rate: 31.000000
==> Total testing loss: 98.348081    Total testing error rate: 24.050000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 58
0.000000/1000.000000 ==> Training loss: 0.067628    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.035293    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.021296    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.028584    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.038788    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.036158    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.043931    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.050565    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.063606    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.039600    Training error rate: 0.000000
==> Total training loss: 48.326139    Total training error rate: 0.884000
==> Testing Epoch: 58
0.000000/100.000000 ==> Testing loss: 1.123658    Testing error rate: 30.000000
==> Total testing loss: 98.655750    Total testing error rate: 24.130000
==> Set learning rate: 0.001000
==> Training Epoch: 59
0.000000/1000.000000 ==> Training loss: 0.065121    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.101218    Training error rate: 4.000000
200.000000/1000.000000 ==> Training loss: 0.037441    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.025912    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.029503    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.049440    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.040144    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.029000    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.097247    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.020332    Training error rate: 0.000000
==> Total training loss: 45.654283    Total training error rate: 0.882000
==> Testing Epoch: 59
0.000000/100.000000 ==> Testing loss: 1.155996    Testing error rate: 29.000000
==> Total testing loss: 99.356777    Total testing error rate: 23.680000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 60
0.000000/1000.000000 ==> Training loss: 0.038617    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.092476    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.054517    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.018573    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.054081    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.020602    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.043624    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.026138    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.043618    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.021510    Training error rate: 0.000000
==> Total training loss: 42.387118    Total training error rate: 0.732000
==> Testing Epoch: 60
0.000000/100.000000 ==> Testing loss: 1.150217    Testing error rate: 30.000000
==> Total testing loss: 99.085435    Total testing error rate: 23.570000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 61
0.000000/1000.000000 ==> Training loss: 0.066836    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.050625    Training error rate: 4.000000
200.000000/1000.000000 ==> Training loss: 0.030383    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.064530    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.022707    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.012108    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.006753    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.016510    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.023140    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.023341    Training error rate: 0.000000
==> Total training loss: 40.870797    Total training error rate: 0.720000
==> Testing Epoch: 61
0.000000/100.000000 ==> Testing loss: 1.177304    Testing error rate: 31.000000
==> Total testing loss: 100.100656    Total testing error rate: 23.880000
==> Set learning rate: 0.001000
==> Training Epoch: 62
0.000000/1000.000000 ==> Training loss: 0.020949    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.026341    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.026728    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.029070    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.025669    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.026885    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.075498    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.011523    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.024881    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.019996    Training error rate: 0.000000
==> Total training loss: 37.822657    Total training error rate: 0.632000
==> Testing Epoch: 62
0.000000/100.000000 ==> Testing loss: 1.165446    Testing error rate: 33.000000
==> Total testing loss: 99.310700    Total testing error rate: 23.740000
==> Set learning rate: 0.001000
==> Training Epoch: 63
0.000000/1000.000000 ==> Training loss: 0.029476    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.020037    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.059540    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.045527    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.086030    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.025687    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.018924    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.030153    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.072131    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.034003    Training error rate: 2.000000
==> Total training loss: 34.154621    Total training error rate: 0.518000
==> Testing Epoch: 63
0.000000/100.000000 ==> Testing loss: 1.209332    Testing error rate: 30.000000
==> Total testing loss: 99.272200    Total testing error rate: 23.960000
==> Set learning rate: 0.001000
==> Training Epoch: 64
0.000000/1000.000000 ==> Training loss: 0.010581    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.010233    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.022294    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.036547    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.031963    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.017260    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.034261    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.109755    Training error rate: 4.000000
800.000000/1000.000000 ==> Training loss: 0.017688    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.043803    Training error rate: 2.000000
==> Total training loss: 32.437895    Total training error rate: 0.478000
==> Testing Epoch: 64
0.000000/100.000000 ==> Testing loss: 1.244508    Testing error rate: 31.000000
==> Total testing loss: 100.065434    Total testing error rate: 23.660000
==> Set learning rate: 0.001000
==> Training Epoch: 65
0.000000/1000.000000 ==> Training loss: 0.019078    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.067521    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.017390    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.032822    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.022928    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.024892    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.033825    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.041217    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.033013    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.013889    Training error rate: 0.000000
==> Total training loss: 31.868908    Total training error rate: 0.502000
==> Testing Epoch: 65
0.000000/100.000000 ==> Testing loss: 1.192919    Testing error rate: 30.000000
==> Total testing loss: 100.859723    Total testing error rate: 23.900000
==> Set learning rate: 0.001000
==> Training Epoch: 66
0.000000/1000.000000 ==> Training loss: 0.009282    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.015798    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.040408    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.032356    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.018256    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.018728    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.022985    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.027871    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.028593    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.075301    Training error rate: 2.000000
==> Total training loss: 29.324722    Total training error rate: 0.412000
==> Testing Epoch: 66
0.000000/100.000000 ==> Testing loss: 1.207267    Testing error rate: 31.000000
==> Total testing loss: 100.958183    Total testing error rate: 23.550000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 67
0.000000/1000.000000 ==> Training loss: 0.011879    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.057220    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.032442    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.045434    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.025236    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.023300    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.032859    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.009229    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.012082    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.060850    Training error rate: 4.000000
==> Total training loss: 28.616844    Total training error rate: 0.414000
==> Testing Epoch: 67
0.000000/100.000000 ==> Testing loss: 1.218909    Testing error rate: 30.000000
==> Total testing loss: 101.144067    Total testing error rate: 23.750000
==> Set learning rate: 0.001000
==> Training Epoch: 68
0.000000/1000.000000 ==> Training loss: 0.022246    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.035651    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.010535    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.022717    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.009778    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.051904    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.052641    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.016562    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.024509    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.024418    Training error rate: 0.000000
==> Total training loss: 27.080665    Total training error rate: 0.402000
==> Testing Epoch: 68
0.000000/100.000000 ==> Testing loss: 1.196999    Testing error rate: 32.000000
==> Total testing loss: 100.390111    Total testing error rate: 23.550000
==> Set learning rate: 0.001000
==> Training Epoch: 69
0.000000/1000.000000 ==> Training loss: 0.009051    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.025988    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.011841    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.016314    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.031545    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.026289    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013040    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.088650    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.079111    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.017341    Training error rate: 0.000000
==> Total training loss: 26.275381    Total training error rate: 0.372000
==> Testing Epoch: 69
0.000000/100.000000 ==> Testing loss: 1.200589    Testing error rate: 28.000000
==> Total testing loss: 101.259795    Total testing error rate: 24.020000
==> Set learning rate: 0.001000
==> Training Epoch: 70
0.000000/1000.000000 ==> Training loss: 0.022138    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.018814    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008596    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.018251    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.017088    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.027824    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.007379    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.018011    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.048186    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.023335    Training error rate: 0.000000
==> Total training loss: 25.388702    Total training error rate: 0.356000
==> Testing Epoch: 70
0.000000/100.000000 ==> Testing loss: 1.204448    Testing error rate: 29.000000
==> Total testing loss: 101.571438    Total testing error rate: 23.730000
==> Set learning rate: 0.001000
==> Training Epoch: 71
0.000000/1000.000000 ==> Training loss: 0.013711    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.029260    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.031712    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.016206    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.015244    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.035182    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.009788    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.061809    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.019751    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.015192    Training error rate: 0.000000
==> Total training loss: 24.211815    Total training error rate: 0.332000
==> Testing Epoch: 71
0.000000/100.000000 ==> Testing loss: 1.219736    Testing error rate: 30.000000
==> Total testing loss: 101.653867    Total testing error rate: 23.820000
==> Set learning rate: 0.001000
==> Training Epoch: 72
0.000000/1000.000000 ==> Training loss: 0.029282    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.029253    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.028701    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.012642    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.017497    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.027249    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.010702    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.021618    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.009698    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.014207    Training error rate: 0.000000
==> Total training loss: 22.816107    Total training error rate: 0.284000
==> Testing Epoch: 72
0.000000/100.000000 ==> Testing loss: 1.190464    Testing error rate: 32.000000
==> Total testing loss: 100.860443    Total testing error rate: 23.540000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 73
0.000000/1000.000000 ==> Training loss: 0.010365    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.034069    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.015251    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.010028    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.007885    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.016935    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.006231    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.008887    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.075565    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.029178    Training error rate: 0.000000
==> Total training loss: 22.629481    Total training error rate: 0.282000
==> Testing Epoch: 73
0.000000/100.000000 ==> Testing loss: 1.181807    Testing error rate: 33.000000
==> Total testing loss: 100.932446    Total testing error rate: 23.720000
==> Set learning rate: 0.001000
==> Training Epoch: 74
0.000000/1000.000000 ==> Training loss: 0.018217    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.014412    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.014022    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.017850    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.023735    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.008713    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.016031    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.106813    Training error rate: 4.000000
800.000000/1000.000000 ==> Training loss: 0.043136    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.022047    Training error rate: 0.000000
==> Total training loss: 20.793749    Total training error rate: 0.208000
==> Testing Epoch: 74
0.000000/100.000000 ==> Testing loss: 1.144926    Testing error rate: 30.000000
==> Total testing loss: 101.178058    Total testing error rate: 23.690000
==> Set learning rate: 0.001000
==> Training Epoch: 75
0.000000/1000.000000 ==> Training loss: 0.014137    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.015625    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.020343    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.016303    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.025776    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.022723    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.016511    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.012284    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.013097    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.010403    Training error rate: 0.000000
==> Total training loss: 21.308365    Total training error rate: 0.248000
==> Testing Epoch: 75
0.000000/100.000000 ==> Testing loss: 1.217663    Testing error rate: 30.000000
==> Total testing loss: 100.823891    Total testing error rate: 23.620000
==> Set learning rate: 0.001000
==> Training Epoch: 76
0.000000/1000.000000 ==> Training loss: 0.026858    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.013407    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.014367    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.015718    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.009949    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.009664    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.016524    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.009321    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.016189    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.008749    Training error rate: 0.000000
==> Total training loss: 20.411631    Total training error rate: 0.244000
==> Testing Epoch: 76
0.000000/100.000000 ==> Testing loss: 1.162269    Testing error rate: 29.000000
==> Total testing loss: 100.463566    Total testing error rate: 23.760000
==> Set learning rate: 0.001000
==> Training Epoch: 77
0.000000/1000.000000 ==> Training loss: 0.006644    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.006470    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.021521    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.013628    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.032834    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.022301    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.010220    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.009305    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.015657    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.010572    Training error rate: 0.000000
==> Total training loss: 19.571247    Total training error rate: 0.190000
==> Testing Epoch: 77
0.000000/100.000000 ==> Testing loss: 1.249771    Testing error rate: 29.000000
==> Total testing loss: 101.015711    Total testing error rate: 23.620000
==> Set learning rate: 0.001000
==> Training Epoch: 78
0.000000/1000.000000 ==> Training loss: 0.011337    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.016259    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.018396    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.024064    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.019348    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.017952    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.017657    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.008862    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.013771    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.028841    Training error rate: 0.000000
==> Total training loss: 19.523746    Total training error rate: 0.230000
==> Testing Epoch: 78
0.000000/100.000000 ==> Testing loss: 1.213725    Testing error rate: 32.000000
==> Total testing loss: 101.675307    Total testing error rate: 23.740000
==> Set learning rate: 0.001000
==> Training Epoch: 79
0.000000/1000.000000 ==> Training loss: 0.016497    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.015988    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.019610    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.054699    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.017457    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.014239    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.009828    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.012113    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.012268    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.028292    Training error rate: 0.000000
==> Total training loss: 18.795592    Total training error rate: 0.212000
==> Testing Epoch: 79
0.000000/100.000000 ==> Testing loss: 1.153278    Testing error rate: 30.000000
==> Total testing loss: 100.653751    Total testing error rate: 23.660000
==> Set learning rate: 0.001000
==> Training Epoch: 80
0.000000/1000.000000 ==> Training loss: 0.009340    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.027270    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.006761    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.016650    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.018155    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.007964    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.009847    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.013943    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.015084    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.024221    Training error rate: 0.000000
==> Total training loss: 18.570467    Total training error rate: 0.200000
==> Testing Epoch: 80
0.000000/100.000000 ==> Testing loss: 1.181342    Testing error rate: 27.000000
==> Total testing loss: 102.097038    Total testing error rate: 23.550000
==> Set learning rate: 0.001000
==> Training Epoch: 81
0.000000/1000.000000 ==> Training loss: 0.018414    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.009537    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.018418    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.008179    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.004565    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.010582    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.015867    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.008042    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.007358    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.011962    Training error rate: 0.000000
==> Total training loss: 18.078073    Total training error rate: 0.202000
==> Testing Epoch: 81
0.000000/100.000000 ==> Testing loss: 1.240536    Testing error rate: 29.000000
==> Total testing loss: 101.850567    Total testing error rate: 23.670000
==> Set learning rate: 0.001000
==> Training Epoch: 82
0.000000/1000.000000 ==> Training loss: 0.025429    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.015207    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.010671    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.013614    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.015437    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.007542    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.015148    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.015055    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.014209    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.024024    Training error rate: 0.000000
==> Total training loss: 17.908925    Total training error rate: 0.176000
==> Testing Epoch: 82
0.000000/100.000000 ==> Testing loss: 1.249638    Testing error rate: 31.000000
==> Total testing loss: 101.498123    Total testing error rate: 23.520000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 83
0.000000/1000.000000 ==> Training loss: 0.016580    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.012106    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.015249    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.015572    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.007493    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.020624    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.012028    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.009349    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.008828    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.009832    Training error rate: 0.000000
==> Total training loss: 17.140126    Total training error rate: 0.192000
==> Testing Epoch: 83
0.000000/100.000000 ==> Testing loss: 1.293528    Testing error rate: 33.000000
==> Total testing loss: 100.976368    Total testing error rate: 23.650000
==> Set learning rate: 0.001000
==> Training Epoch: 84
0.000000/1000.000000 ==> Training loss: 0.006267    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.013266    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.014010    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.013784    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.007406    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.022200    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.006862    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.008143    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.013692    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.024456    Training error rate: 0.000000
==> Total training loss: 16.987742    Total training error rate: 0.170000
==> Testing Epoch: 84
0.000000/100.000000 ==> Testing loss: 1.316385    Testing error rate: 32.000000
==> Total testing loss: 101.142620    Total testing error rate: 23.690000
==> Set learning rate: 0.001000
==> Training Epoch: 85
0.000000/1000.000000 ==> Training loss: 0.005420    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.027812    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.009904    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.014710    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.021037    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011409    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.012833    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.013375    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.037839    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.013003    Training error rate: 0.000000
==> Total training loss: 16.539238    Total training error rate: 0.164000
==> Testing Epoch: 85
0.000000/100.000000 ==> Testing loss: 1.258882    Testing error rate: 28.000000
==> Total testing loss: 101.062141    Total testing error rate: 23.280000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 86
0.000000/1000.000000 ==> Training loss: 0.009635    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.011266    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.009137    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.013649    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.009031    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.016258    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.025716    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.010496    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.011859    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.005143    Training error rate: 0.000000
==> Total training loss: 15.586525    Total training error rate: 0.128000
==> Testing Epoch: 86
0.000000/100.000000 ==> Testing loss: 1.229558    Testing error rate: 32.000000
==> Total testing loss: 100.892397    Total testing error rate: 23.140000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 87
0.000000/1000.000000 ==> Training loss: 0.016567    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.022129    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.011169    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.011694    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.015461    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.016331    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013813    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.009575    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.011686    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.021492    Training error rate: 0.000000
==> Total training loss: 16.029175    Total training error rate: 0.138000
==> Testing Epoch: 87
0.000000/100.000000 ==> Testing loss: 1.289886    Testing error rate: 31.000000
==> Total testing loss: 101.183883    Total testing error rate: 23.610000
==> Set learning rate: 0.001000
==> Training Epoch: 88
0.000000/1000.000000 ==> Training loss: 0.015219    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.009886    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013485    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.008533    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.016637    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.015785    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.014188    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.012446    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.021837    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.020935    Training error rate: 0.000000
==> Total training loss: 15.516588    Total training error rate: 0.164000
==> Testing Epoch: 88
0.000000/100.000000 ==> Testing loss: 1.240527    Testing error rate: 28.000000
==> Total testing loss: 101.170859    Total testing error rate: 23.370000
==> Set learning rate: 0.001000
==> Training Epoch: 89
0.000000/1000.000000 ==> Training loss: 0.011694    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.011401    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.009055    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.012135    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.032126    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.011448    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.008530    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.012197    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.013978    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.007597    Training error rate: 0.000000
==> Total training loss: 15.926203    Total training error rate: 0.190000
==> Testing Epoch: 89
0.000000/100.000000 ==> Testing loss: 1.210815    Testing error rate: 29.000000
==> Total testing loss: 101.481453    Total testing error rate: 23.630000
==> Set learning rate: 0.001000
==> Training Epoch: 90
0.000000/1000.000000 ==> Training loss: 0.016770    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.014300    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.028765    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.003682    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.006791    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011402    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013199    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.006470    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.011507    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.019649    Training error rate: 0.000000
==> Total training loss: 15.232429    Total training error rate: 0.150000
==> Testing Epoch: 90
0.000000/100.000000 ==> Testing loss: 1.295915    Testing error rate: 30.000000
==> Total testing loss: 102.216428    Total testing error rate: 23.540000
==> Set learning rate: 0.001000
==> Training Epoch: 91
0.000000/1000.000000 ==> Training loss: 0.024469    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.013234    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.026082    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.010978    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.007243    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.014894    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.009905    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.011486    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.005453    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.015015    Training error rate: 0.000000
==> Total training loss: 14.875214    Total training error rate: 0.134000
==> Testing Epoch: 91
0.000000/100.000000 ==> Testing loss: 1.277833    Testing error rate: 30.000000
==> Total testing loss: 101.257743    Total testing error rate: 23.400000
==> Set learning rate: 0.001000
==> Training Epoch: 92
0.000000/1000.000000 ==> Training loss: 0.005795    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.010938    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.009405    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.015133    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.070004    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.017857    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.005586    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.007248    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.018831    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.006781    Training error rate: 0.000000
==> Total training loss: 14.334602    Total training error rate: 0.138000
==> Testing Epoch: 92
0.000000/100.000000 ==> Testing loss: 1.232698    Testing error rate: 27.000000
==> Total testing loss: 101.359840    Total testing error rate: 23.480000
==> Set learning rate: 0.001000
==> Training Epoch: 93
0.000000/1000.000000 ==> Training loss: 0.005525    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.015040    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.032060    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.004988    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.007031    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.009876    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.012697    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.047590    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.012655    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.019506    Training error rate: 0.000000
==> Total training loss: 14.647751    Total training error rate: 0.146000
==> Testing Epoch: 93
0.000000/100.000000 ==> Testing loss: 1.206329    Testing error rate: 29.000000
==> Total testing loss: 100.416630    Total testing error rate: 23.290000
==> Set learning rate: 0.001000
==> Training Epoch: 94
0.000000/1000.000000 ==> Training loss: 0.009779    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.022168    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.009141    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.002590    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.016636    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.008726    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.005329    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.006713    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010656    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.005873    Training error rate: 0.000000
==> Total training loss: 13.787061    Total training error rate: 0.106000
==> Testing Epoch: 94
0.000000/100.000000 ==> Testing loss: 1.267880    Testing error rate: 28.000000
==> Total testing loss: 101.209705    Total testing error rate: 23.400000
==> Set learning rate: 0.001000
==> Training Epoch: 95
0.000000/1000.000000 ==> Training loss: 0.006003    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.011457    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008135    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.006819    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.007997    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011929    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.008477    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.007539    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.021163    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.028329    Training error rate: 0.000000
==> Total training loss: 14.571057    Total training error rate: 0.146000
==> Testing Epoch: 95
0.000000/100.000000 ==> Testing loss: 1.264716    Testing error rate: 29.000000
==> Total testing loss: 100.451803    Total testing error rate: 23.130000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 96
0.000000/1000.000000 ==> Training loss: 0.012390    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.011454    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.020796    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.005463    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.007318    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011709    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.015296    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.025394    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.008091    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.009097    Training error rate: 0.000000
==> Total training loss: 13.927147    Total training error rate: 0.122000
==> Testing Epoch: 96
0.000000/100.000000 ==> Testing loss: 1.238340    Testing error rate: 27.000000
==> Total testing loss: 101.472873    Total testing error rate: 23.410000
==> Set learning rate: 0.001000
==> Training Epoch: 97
0.000000/1000.000000 ==> Training loss: 0.006128    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.005536    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008703    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.009465    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.013725    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.027969    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.014550    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.009393    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.014178    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.005154    Training error rate: 0.000000
==> Total training loss: 14.384531    Total training error rate: 0.130000
==> Testing Epoch: 97
0.000000/100.000000 ==> Testing loss: 1.232892    Testing error rate: 28.000000
==> Total testing loss: 101.068937    Total testing error rate: 23.410000
==> Set learning rate: 0.001000
==> Training Epoch: 98
0.000000/1000.000000 ==> Training loss: 0.010560    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.020989    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.009337    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.004727    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.019777    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.007640    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013104    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.015495    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.020819    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.016318    Training error rate: 0.000000
==> Total training loss: 13.992880    Total training error rate: 0.122000
==> Testing Epoch: 98
0.000000/100.000000 ==> Testing loss: 1.256284    Testing error rate: 31.000000
==> Total testing loss: 100.515638    Total testing error rate: 23.450000
==> Set learning rate: 0.001000
==> Training Epoch: 99
0.000000/1000.000000 ==> Training loss: 0.028366    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.010791    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.051717    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.023430    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.025124    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.009347    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.011753    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.006589    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010492    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.009773    Training error rate: 0.000000
==> Total training loss: 13.264787    Total training error rate: 0.104000
==> Testing Epoch: 99
0.000000/100.000000 ==> Testing loss: 1.210586    Testing error rate: 28.000000
==> Total testing loss: 101.692558    Total testing error rate: 23.520000
==> Set learning rate: 0.001000
==> Training Epoch: 100
0.000000/1000.000000 ==> Training loss: 0.009878    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.020276    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.012999    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.004662    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.014276    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.003849    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.020673    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.018800    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010415    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.019488    Training error rate: 0.000000
==> Total training loss: 13.098386    Total training error rate: 0.094000
==> Testing Epoch: 100
0.000000/100.000000 ==> Testing loss: 1.274429    Testing error rate: 29.000000
==> Total testing loss: 101.192337    Total testing error rate: 23.440000
