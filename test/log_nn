==> Init variables..
==> Init seed..
==> Download data..
Files already downloaded and verified
==> Calculate mean and std..
==> Prepare training transform..
==> Prepare testing transform..
==> Init dataloader..
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
==> Set learning rate: 0.010000
==> Training Epoch: 1
0.000000/1000.000000 ==> Training loss: 4.869855    Training error rate: 100.000000
100.000000/1000.000000 ==> Training loss: 4.358538    Training error rate: 100.000000
200.000000/1000.000000 ==> Training loss: 4.211967    Training error rate: 96.000000
300.000000/1000.000000 ==> Training loss: 3.890228    Training error rate: 98.000000
400.000000/1000.000000 ==> Training loss: 3.826101    Training error rate: 88.000000
500.000000/1000.000000 ==> Training loss: 3.878711    Training error rate: 90.000000
600.000000/1000.000000 ==> Training loss: 3.758152    Training error rate: 88.000000
700.000000/1000.000000 ==> Training loss: 3.604835    Training error rate: 88.000000
800.000000/1000.000000 ==> Training loss: 3.710564    Training error rate: 86.000000
900.000000/1000.000000 ==> Training loss: 3.753931    Training error rate: 88.000000
==> Total training loss: 3941.657706    Total training error rate: 90.654000
==> Testing Epoch: 1
0.000000/100.000000 ==> Testing loss: 3.525717    Testing error rate: 88.000000
==> Total testing loss: 354.915110    Total testing error rate: 84.160000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 2
0.000000/1000.000000 ==> Training loss: 3.789062    Training error rate: 88.000000
100.000000/1000.000000 ==> Training loss: 3.651617    Training error rate: 92.000000
200.000000/1000.000000 ==> Training loss: 3.439308    Training error rate: 82.000000
300.000000/1000.000000 ==> Training loss: 3.357955    Training error rate: 82.000000
400.000000/1000.000000 ==> Training loss: 2.959576    Training error rate: 82.000000
500.000000/1000.000000 ==> Training loss: 2.823551    Training error rate: 66.000000
600.000000/1000.000000 ==> Training loss: 3.010523    Training error rate: 78.000000
700.000000/1000.000000 ==> Training loss: 2.830281    Training error rate: 70.000000
800.000000/1000.000000 ==> Training loss: 3.177057    Training error rate: 80.000000
900.000000/1000.000000 ==> Training loss: 2.974884    Training error rate: 72.000000
==> Total training loss: 3255.619215    Total training error rate: 80.028000
==> Testing Epoch: 2
0.000000/100.000000 ==> Testing loss: 3.300396    Testing error rate: 73.000000
==> Total testing loss: 307.762229    Total testing error rate: 75.750000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 3
0.000000/1000.000000 ==> Training loss: 2.994791    Training error rate: 78.000000
100.000000/1000.000000 ==> Training loss: 3.106786    Training error rate: 68.000000
200.000000/1000.000000 ==> Training loss: 3.133022    Training error rate: 88.000000
300.000000/1000.000000 ==> Training loss: 2.601504    Training error rate: 64.000000
400.000000/1000.000000 ==> Training loss: 2.835931    Training error rate: 68.000000
500.000000/1000.000000 ==> Training loss: 2.735807    Training error rate: 76.000000
600.000000/1000.000000 ==> Training loss: 2.762809    Training error rate: 68.000000
700.000000/1000.000000 ==> Training loss: 2.308450    Training error rate: 66.000000
800.000000/1000.000000 ==> Training loss: 2.400020    Training error rate: 74.000000
900.000000/1000.000000 ==> Training loss: 2.655152    Training error rate: 70.000000
==> Total training loss: 2767.634300    Total training error rate: 70.912000
==> Testing Epoch: 3
0.000000/100.000000 ==> Testing loss: 2.477586    Testing error rate: 63.000000
==> Total testing loss: 252.026759    Total testing error rate: 65.810000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 4
0.000000/1000.000000 ==> Training loss: 2.263833    Training error rate: 64.000000
100.000000/1000.000000 ==> Training loss: 2.528336    Training error rate: 70.000000
200.000000/1000.000000 ==> Training loss: 2.349341    Training error rate: 62.000000
300.000000/1000.000000 ==> Training loss: 2.331708    Training error rate: 66.000000
400.000000/1000.000000 ==> Training loss: 2.644176    Training error rate: 72.000000
500.000000/1000.000000 ==> Training loss: 2.307448    Training error rate: 66.000000
600.000000/1000.000000 ==> Training loss: 2.394206    Training error rate: 68.000000
700.000000/1000.000000 ==> Training loss: 1.897128    Training error rate: 48.000000
800.000000/1000.000000 ==> Training loss: 2.168140    Training error rate: 60.000000
900.000000/1000.000000 ==> Training loss: 2.570299    Training error rate: 74.000000
==> Total training loss: 2366.072856    Total training error rate: 62.810000
==> Testing Epoch: 4
0.000000/100.000000 ==> Testing loss: 2.385868    Testing error rate: 59.000000
==> Total testing loss: 236.087084    Total testing error rate: 61.320000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 5
0.000000/1000.000000 ==> Training loss: 1.732553    Training error rate: 44.000000
100.000000/1000.000000 ==> Training loss: 1.892209    Training error rate: 56.000000
200.000000/1000.000000 ==> Training loss: 2.103673    Training error rate: 60.000000
300.000000/1000.000000 ==> Training loss: 1.782671    Training error rate: 48.000000
400.000000/1000.000000 ==> Training loss: 2.215803    Training error rate: 58.000000
500.000000/1000.000000 ==> Training loss: 1.908248    Training error rate: 52.000000
600.000000/1000.000000 ==> Training loss: 2.523222    Training error rate: 66.000000
700.000000/1000.000000 ==> Training loss: 2.523626    Training error rate: 74.000000
800.000000/1000.000000 ==> Training loss: 1.807067    Training error rate: 42.000000
900.000000/1000.000000 ==> Training loss: 1.835128    Training error rate: 56.000000
==> Total training loss: 2066.991249    Total training error rate: 56.342000
==> Testing Epoch: 5
0.000000/100.000000 ==> Testing loss: 2.082717    Testing error rate: 60.000000
==> Total testing loss: 202.956053    Total testing error rate: 54.510000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 6
0.000000/1000.000000 ==> Training loss: 2.041536    Training error rate: 50.000000
100.000000/1000.000000 ==> Training loss: 2.034532    Training error rate: 50.000000
200.000000/1000.000000 ==> Training loss: 1.641754    Training error rate: 48.000000
300.000000/1000.000000 ==> Training loss: 1.742832    Training error rate: 52.000000
400.000000/1000.000000 ==> Training loss: 2.330810    Training error rate: 68.000000
500.000000/1000.000000 ==> Training loss: 1.785637    Training error rate: 50.000000
600.000000/1000.000000 ==> Training loss: 1.798626    Training error rate: 54.000000
700.000000/1000.000000 ==> Training loss: 1.702543    Training error rate: 50.000000
800.000000/1000.000000 ==> Training loss: 1.852901    Training error rate: 52.000000
900.000000/1000.000000 ==> Training loss: 1.508682    Training error rate: 48.000000
==> Total training loss: 1847.988540    Total training error rate: 51.210000
==> Testing Epoch: 6
0.000000/100.000000 ==> Testing loss: 2.068659    Testing error rate: 53.000000
==> Total testing loss: 192.720212    Total testing error rate: 51.680000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 7
0.000000/1000.000000 ==> Training loss: 1.450717    Training error rate: 46.000000
100.000000/1000.000000 ==> Training loss: 2.327500    Training error rate: 64.000000
200.000000/1000.000000 ==> Training loss: 1.160961    Training error rate: 30.000000
300.000000/1000.000000 ==> Training loss: 1.434068    Training error rate: 44.000000
400.000000/1000.000000 ==> Training loss: 1.416714    Training error rate: 36.000000
500.000000/1000.000000 ==> Training loss: 2.102792    Training error rate: 58.000000
600.000000/1000.000000 ==> Training loss: 1.336333    Training error rate: 36.000000
700.000000/1000.000000 ==> Training loss: 1.647042    Training error rate: 40.000000
800.000000/1000.000000 ==> Training loss: 1.807467    Training error rate: 54.000000
900.000000/1000.000000 ==> Training loss: 1.827432    Training error rate: 54.000000
==> Total training loss: 1685.126183    Total training error rate: 47.430000
==> Testing Epoch: 7
0.000000/100.000000 ==> Testing loss: 1.712761    Testing error rate: 46.000000
==> Total testing loss: 182.240819    Total testing error rate: 49.450000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 8
0.000000/1000.000000 ==> Training loss: 1.714832    Training error rate: 48.000000
100.000000/1000.000000 ==> Training loss: 1.938863    Training error rate: 54.000000
200.000000/1000.000000 ==> Training loss: 1.703970    Training error rate: 46.000000
300.000000/1000.000000 ==> Training loss: 1.219590    Training error rate: 44.000000
400.000000/1000.000000 ==> Training loss: 1.747904    Training error rate: 46.000000
500.000000/1000.000000 ==> Training loss: 1.595328    Training error rate: 38.000000
600.000000/1000.000000 ==> Training loss: 1.223815    Training error rate: 40.000000
700.000000/1000.000000 ==> Training loss: 1.390288    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.557176    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.646633    Training error rate: 50.000000
==> Total training loss: 1545.660048    Total training error rate: 43.768000
==> Testing Epoch: 8
0.000000/100.000000 ==> Testing loss: 1.510017    Testing error rate: 39.000000
==> Total testing loss: 164.122103    Total testing error rate: 45.170000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 9
0.000000/1000.000000 ==> Training loss: 1.486579    Training error rate: 46.000000
100.000000/1000.000000 ==> Training loss: 1.780580    Training error rate: 52.000000
200.000000/1000.000000 ==> Training loss: 1.532690    Training error rate: 46.000000
300.000000/1000.000000 ==> Training loss: 1.030266    Training error rate: 36.000000
400.000000/1000.000000 ==> Training loss: 1.371879    Training error rate: 38.000000
500.000000/1000.000000 ==> Training loss: 1.288981    Training error rate: 40.000000
600.000000/1000.000000 ==> Training loss: 1.213581    Training error rate: 42.000000
700.000000/1000.000000 ==> Training loss: 1.644442    Training error rate: 46.000000
800.000000/1000.000000 ==> Training loss: 1.678889    Training error rate: 52.000000
900.000000/1000.000000 ==> Training loss: 1.638825    Training error rate: 40.000000
==> Total training loss: 1437.363471    Total training error rate: 41.232000
==> Testing Epoch: 9
0.000000/100.000000 ==> Testing loss: 1.697934    Testing error rate: 49.000000
==> Total testing loss: 161.562922    Total testing error rate: 44.310000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 10
0.000000/1000.000000 ==> Training loss: 1.034328    Training error rate: 30.000000
100.000000/1000.000000 ==> Training loss: 1.173399    Training error rate: 36.000000
200.000000/1000.000000 ==> Training loss: 1.509074    Training error rate: 38.000000
300.000000/1000.000000 ==> Training loss: 1.297257    Training error rate: 42.000000
400.000000/1000.000000 ==> Training loss: 1.649478    Training error rate: 48.000000
500.000000/1000.000000 ==> Training loss: 1.204050    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 1.378891    Training error rate: 32.000000
700.000000/1000.000000 ==> Training loss: 1.507996    Training error rate: 40.000000
800.000000/1000.000000 ==> Training loss: 1.504043    Training error rate: 44.000000
900.000000/1000.000000 ==> Training loss: 2.186598    Training error rate: 58.000000
==> Total training loss: 1341.485559    Total training error rate: 38.652000
==> Testing Epoch: 10
0.000000/100.000000 ==> Testing loss: 1.538443    Testing error rate: 42.000000
==> Total testing loss: 167.026760    Total testing error rate: 45.120000
==> Set learning rate: 0.010000
==> Training Epoch: 11
0.000000/1000.000000 ==> Training loss: 1.109900    Training error rate: 30.000000
100.000000/1000.000000 ==> Training loss: 0.835794    Training error rate: 24.000000
200.000000/1000.000000 ==> Training loss: 1.507680    Training error rate: 48.000000
300.000000/1000.000000 ==> Training loss: 1.103310    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.973561    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 1.277068    Training error rate: 32.000000
600.000000/1000.000000 ==> Training loss: 1.231104    Training error rate: 32.000000
700.000000/1000.000000 ==> Training loss: 0.917351    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 1.097754    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 1.562742    Training error rate: 46.000000
==> Total training loss: 1263.484937    Total training error rate: 36.558000
==> Testing Epoch: 11
0.000000/100.000000 ==> Testing loss: 1.457085    Testing error rate: 39.000000
==> Total testing loss: 149.069990    Total testing error rate: 41.020000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 12
0.000000/1000.000000 ==> Training loss: 1.263428    Training error rate: 38.000000
100.000000/1000.000000 ==> Training loss: 1.074583    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.118280    Training error rate: 42.000000
300.000000/1000.000000 ==> Training loss: 0.993709    Training error rate: 32.000000
400.000000/1000.000000 ==> Training loss: 0.965728    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 1.355349    Training error rate: 46.000000
600.000000/1000.000000 ==> Training loss: 1.421277    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 1.107254    Training error rate: 34.000000
800.000000/1000.000000 ==> Training loss: 0.879305    Training error rate: 30.000000
900.000000/1000.000000 ==> Training loss: 1.374546    Training error rate: 38.000000
==> Total training loss: 1184.144643    Total training error rate: 34.660000
==> Testing Epoch: 12
0.000000/100.000000 ==> Testing loss: 1.564059    Testing error rate: 41.000000
==> Total testing loss: 140.011547    Total testing error rate: 39.370000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 13
0.000000/1000.000000 ==> Training loss: 1.469511    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 0.974558    Training error rate: 26.000000
200.000000/1000.000000 ==> Training loss: 1.256840    Training error rate: 40.000000
300.000000/1000.000000 ==> Training loss: 1.020087    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 1.302295    Training error rate: 42.000000
500.000000/1000.000000 ==> Training loss: 1.064123    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 1.299635    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 1.298798    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.228308    Training error rate: 38.000000
900.000000/1000.000000 ==> Training loss: 0.966402    Training error rate: 36.000000
==> Total training loss: 1119.596969    Total training error rate: 33.004000
==> Testing Epoch: 13
0.000000/100.000000 ==> Testing loss: 1.311621    Testing error rate: 31.000000
==> Total testing loss: 143.007641    Total testing error rate: 39.420000
==> Set learning rate: 0.010000
==> Training Epoch: 14
0.000000/1000.000000 ==> Training loss: 0.948215    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.817605    Training error rate: 22.000000
200.000000/1000.000000 ==> Training loss: 1.428853    Training error rate: 42.000000
300.000000/1000.000000 ==> Training loss: 1.103407    Training error rate: 32.000000
400.000000/1000.000000 ==> Training loss: 0.848976    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 0.993201    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 1.689766    Training error rate: 50.000000
700.000000/1000.000000 ==> Training loss: 1.325015    Training error rate: 40.000000
800.000000/1000.000000 ==> Training loss: 0.953128    Training error rate: 26.000000
900.000000/1000.000000 ==> Training loss: 1.041420    Training error rate: 32.000000
==> Total training loss: 1064.527040    Total training error rate: 31.396000
==> Testing Epoch: 14
0.000000/100.000000 ==> Testing loss: 1.326931    Testing error rate: 37.000000
==> Total testing loss: 136.076620    Total testing error rate: 37.800000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 15
0.000000/1000.000000 ==> Training loss: 1.218634    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 0.897913    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.042830    Training error rate: 28.000000
300.000000/1000.000000 ==> Training loss: 0.916887    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.710703    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.683752    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 1.164849    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.784234    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 1.048603    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 1.149278    Training error rate: 30.000000
==> Total training loss: 1007.361663    Total training error rate: 29.864000
==> Testing Epoch: 15
0.000000/100.000000 ==> Testing loss: 1.329830    Testing error rate: 36.000000
==> Total testing loss: 137.031575    Total testing error rate: 37.720000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 16
0.000000/1000.000000 ==> Training loss: 1.180357    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 1.021582    Training error rate: 22.000000
200.000000/1000.000000 ==> Training loss: 0.950739    Training error rate: 36.000000
300.000000/1000.000000 ==> Training loss: 0.723653    Training error rate: 24.000000
400.000000/1000.000000 ==> Training loss: 1.140704    Training error rate: 38.000000
500.000000/1000.000000 ==> Training loss: 1.049041    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 1.080709    Training error rate: 36.000000
700.000000/1000.000000 ==> Training loss: 0.865685    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 0.816984    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.719396    Training error rate: 22.000000
==> Total training loss: 958.678444    Total training error rate: 28.726000
==> Testing Epoch: 16
0.000000/100.000000 ==> Testing loss: 1.344280    Testing error rate: 35.000000
==> Total testing loss: 133.028810    Total testing error rate: 36.680000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 17
0.000000/1000.000000 ==> Training loss: 0.974356    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.823696    Training error rate: 22.000000
200.000000/1000.000000 ==> Training loss: 1.029355    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 0.663732    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 1.042654    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 0.883714    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.850431    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.801505    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.851955    Training error rate: 26.000000
900.000000/1000.000000 ==> Training loss: 0.831684    Training error rate: 22.000000
==> Total training loss: 927.766743    Total training error rate: 27.708000
==> Testing Epoch: 17
0.000000/100.000000 ==> Testing loss: 1.337865    Testing error rate: 33.000000
==> Total testing loss: 134.592522    Total testing error rate: 37.500000
==> Set learning rate: 0.010000
==> Training Epoch: 18
0.000000/1000.000000 ==> Training loss: 0.968451    Training error rate: 34.000000
100.000000/1000.000000 ==> Training loss: 0.901268    Training error rate: 24.000000
200.000000/1000.000000 ==> Training loss: 0.917370    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 1.034352    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.861365    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.988812    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.860217    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 1.019088    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.814366    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.655883    Training error rate: 20.000000
==> Total training loss: 875.951426    Total training error rate: 26.550000
==> Testing Epoch: 18
0.000000/100.000000 ==> Testing loss: 1.292058    Testing error rate: 36.000000
==> Total testing loss: 132.701523    Total testing error rate: 36.560000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 19
0.000000/1000.000000 ==> Training loss: 1.302603    Training error rate: 38.000000
100.000000/1000.000000 ==> Training loss: 0.624370    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.643050    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.738689    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 1.138721    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 0.973278    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.804607    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.911364    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.947396    Training error rate: 30.000000
900.000000/1000.000000 ==> Training loss: 0.985897    Training error rate: 28.000000
==> Total training loss: 835.024107    Total training error rate: 25.176000
==> Testing Epoch: 19
0.000000/100.000000 ==> Testing loss: 1.498088    Testing error rate: 39.000000
==> Total testing loss: 142.130565    Total testing error rate: 38.140000
==> Set learning rate: 0.010000
==> Training Epoch: 20
0.000000/1000.000000 ==> Training loss: 0.799987    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.498435    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.746387    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.919200    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 0.840477    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.728902    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.761860    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.706955    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.606276    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.820168    Training error rate: 24.000000
==> Total training loss: 811.042491    Total training error rate: 24.650000
==> Testing Epoch: 20
0.000000/100.000000 ==> Testing loss: 1.330157    Testing error rate: 34.000000
==> Total testing loss: 127.823214    Total testing error rate: 34.650000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 21
0.000000/1000.000000 ==> Training loss: 0.967566    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.668279    Training error rate: 26.000000
200.000000/1000.000000 ==> Training loss: 0.726684    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.884777    Training error rate: 24.000000
400.000000/1000.000000 ==> Training loss: 0.522819    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.865553    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.808368    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.890441    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 1.116206    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 0.753715    Training error rate: 22.000000
==> Total training loss: 777.810645    Total training error rate: 23.744000
==> Testing Epoch: 21
0.000000/100.000000 ==> Testing loss: 1.267004    Testing error rate: 34.000000
==> Total testing loss: 146.331390    Total testing error rate: 38.040000
==> Set learning rate: 0.010000
==> Training Epoch: 22
0.000000/1000.000000 ==> Training loss: 0.754211    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.789018    Training error rate: 24.000000
200.000000/1000.000000 ==> Training loss: 0.673513    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.852564    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 1.121763    Training error rate: 32.000000
500.000000/1000.000000 ==> Training loss: 1.203190    Training error rate: 32.000000
600.000000/1000.000000 ==> Training loss: 0.697386    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.583738    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.807640    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.914643    Training error rate: 26.000000
==> Total training loss: 750.354930    Total training error rate: 22.880000
==> Testing Epoch: 22
0.000000/100.000000 ==> Testing loss: 1.382615    Testing error rate: 32.000000
==> Total testing loss: 133.927012    Total testing error rate: 35.510000
==> Set learning rate: 0.010000
==> Training Epoch: 23
0.000000/1000.000000 ==> Training loss: 0.656923    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.500581    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.584092    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.558575    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.987978    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 0.525882    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.773584    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.755684    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.760430    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.788111    Training error rate: 26.000000
==> Total training loss: 719.674496    Total training error rate: 22.164000
==> Testing Epoch: 23
0.000000/100.000000 ==> Testing loss: 1.230378    Testing error rate: 32.000000
==> Total testing loss: 126.013944    Total testing error rate: 33.660000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 24
0.000000/1000.000000 ==> Training loss: 0.553953    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.817850    Training error rate: 24.000000
200.000000/1000.000000 ==> Training loss: 0.923415    Training error rate: 30.000000
300.000000/1000.000000 ==> Training loss: 0.694356    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.689098    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.811734    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.911936    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.677397    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.559099    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.644960    Training error rate: 20.000000
==> Total training loss: 698.993767    Total training error rate: 21.358000
==> Testing Epoch: 24
0.000000/100.000000 ==> Testing loss: 1.061799    Testing error rate: 34.000000
==> Total testing loss: 127.128404    Total testing error rate: 33.870000
==> Set learning rate: 0.010000
==> Training Epoch: 25
0.000000/1000.000000 ==> Training loss: 0.372191    Training error rate: 8.000000
100.000000/1000.000000 ==> Training loss: 0.494992    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.391155    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.671915    Training error rate: 24.000000
400.000000/1000.000000 ==> Training loss: 0.335137    Training error rate: 8.000000
500.000000/1000.000000 ==> Training loss: 0.741266    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.753843    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.636363    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.557716    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.600107    Training error rate: 16.000000
==> Total training loss: 666.061617    Total training error rate: 20.524000
==> Testing Epoch: 25
0.000000/100.000000 ==> Testing loss: 1.273704    Testing error rate: 34.000000
==> Total testing loss: 132.170022    Total testing error rate: 34.610000
==> Set learning rate: 0.010000
==> Training Epoch: 26
0.000000/1000.000000 ==> Training loss: 0.641471    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.540385    Training error rate: 24.000000
200.000000/1000.000000 ==> Training loss: 0.406609    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.443448    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.695621    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.523517    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.648079    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.661153    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 0.575688    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.755095    Training error rate: 22.000000
==> Total training loss: 654.433841    Total training error rate: 20.342000
==> Testing Epoch: 26
0.000000/100.000000 ==> Testing loss: 1.514068    Testing error rate: 37.000000
==> Total testing loss: 133.613616    Total testing error rate: 34.250000
==> Set learning rate: 0.010000
==> Training Epoch: 27
0.000000/1000.000000 ==> Training loss: 0.336658    Training error rate: 8.000000
100.000000/1000.000000 ==> Training loss: 0.411426    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.912744    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 0.833967    Training error rate: 32.000000
400.000000/1000.000000 ==> Training loss: 0.843131    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 0.687963    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.686264    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.821970    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.492143    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.494318    Training error rate: 18.000000
==> Total training loss: 633.912556    Total training error rate: 19.456000
==> Testing Epoch: 27
0.000000/100.000000 ==> Testing loss: 1.373184    Testing error rate: 38.000000
==> Total testing loss: 134.475855    Total testing error rate: 34.990000
==> Set learning rate: 0.010000
==> Training Epoch: 28
0.000000/1000.000000 ==> Training loss: 0.843506    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.500835    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.472572    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.582791    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.596966    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.555500    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 1.092971    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.665281    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.915791    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.365807    Training error rate: 10.000000
==> Total training loss: 625.465700    Total training error rate: 19.464000
==> Testing Epoch: 28
0.000000/100.000000 ==> Testing loss: 1.112153    Testing error rate: 34.000000
==> Total testing loss: 130.219959    Total testing error rate: 34.230000
==> Set learning rate: 0.010000
==> Training Epoch: 29
0.000000/1000.000000 ==> Training loss: 0.383375    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.259943    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.566803    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.461688    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.573263    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.491497    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.432172    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.590779    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.498719    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.938174    Training error rate: 28.000000
==> Total training loss: 605.283920    Total training error rate: 18.818000
==> Testing Epoch: 29
0.000000/100.000000 ==> Testing loss: 1.433215    Testing error rate: 37.000000
==> Total testing loss: 133.238492    Total testing error rate: 34.380000
==> Set learning rate: 0.010000
==> Training Epoch: 30
0.000000/1000.000000 ==> Training loss: 0.571252    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.557468    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.616466    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.323533    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.555235    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.430568    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.462085    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.442017    Training error rate: 12.000000
800.000000/1000.000000 ==> Training loss: 0.490675    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.649113    Training error rate: 16.000000
==> Total training loss: 584.535841    Total training error rate: 18.130000
==> Testing Epoch: 30
0.000000/100.000000 ==> Testing loss: 1.467064    Testing error rate: 34.000000
==> Total testing loss: 138.459961    Total testing error rate: 35.660000
==> Set learning rate: 0.010000
==> Training Epoch: 31
0.000000/1000.000000 ==> Training loss: 0.394498    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.725825    Training error rate: 24.000000
200.000000/1000.000000 ==> Training loss: 0.318426    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.441240    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.319020    Training error rate: 6.000000
500.000000/1000.000000 ==> Training loss: 0.628137    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.322735    Training error rate: 4.000000
700.000000/1000.000000 ==> Training loss: 0.516884    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.663562    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.689741    Training error rate: 22.000000
==> Total training loss: 569.011375    Total training error rate: 17.784000
==> Testing Epoch: 31
0.000000/100.000000 ==> Testing loss: 1.386536    Testing error rate: 34.000000
==> Total testing loss: 143.972610    Total testing error rate: 36.070000
==> Set learning rate: 0.010000
==> Training Epoch: 32
0.000000/1000.000000 ==> Training loss: 0.326671    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.542511    Training error rate: 22.000000
200.000000/1000.000000 ==> Training loss: 0.700294    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.528636    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.498778    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.493406    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.486974    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.776410    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 0.379513    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.556703    Training error rate: 12.000000
==> Total training loss: 559.268744    Total training error rate: 17.478000
==> Testing Epoch: 32
0.000000/100.000000 ==> Testing loss: 1.358039    Testing error rate: 34.000000
==> Total testing loss: 134.316468    Total testing error rate: 34.520000
==> Set learning rate: 0.010000
==> Training Epoch: 33
0.000000/1000.000000 ==> Training loss: 0.553364    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.532763    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.446982    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.473854    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.622739    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.642700    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.682490    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.452729    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.556781    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.525688    Training error rate: 14.000000
==> Total training loss: 546.547289    Total training error rate: 17.032000
==> Testing Epoch: 33
0.000000/100.000000 ==> Testing loss: 1.342280    Testing error rate: 39.000000
==> Total testing loss: 128.514355    Total testing error rate: 33.340000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 34
0.000000/1000.000000 ==> Training loss: 0.560865    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.290328    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.675141    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.459387    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.486631    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.505812    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.643030    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.490247    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.628492    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.728919    Training error rate: 24.000000
==> Total training loss: 531.693478    Total training error rate: 16.624000
==> Testing Epoch: 34
0.000000/100.000000 ==> Testing loss: 1.297024    Testing error rate: 31.000000
==> Total testing loss: 132.880393    Total testing error rate: 34.000000
==> Set learning rate: 0.010000
==> Training Epoch: 35
0.000000/1000.000000 ==> Training loss: 0.492839    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.545818    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.509981    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.355291    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.818608    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.360878    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.806582    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.296174    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.500519    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.634158    Training error rate: 28.000000
==> Total training loss: 516.893889    Total training error rate: 16.070000
==> Testing Epoch: 35
0.000000/100.000000 ==> Testing loss: 1.272861    Testing error rate: 27.000000
==> Total testing loss: 132.906602    Total testing error rate: 33.240000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 36
0.000000/1000.000000 ==> Training loss: 0.430201    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.523079    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.382085    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.553162    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.483456    Training error rate: 8.000000
500.000000/1000.000000 ==> Training loss: 0.471719    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.480006    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.554825    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.319532    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.458903    Training error rate: 14.000000
==> Total training loss: 507.486534    Total training error rate: 15.884000
==> Testing Epoch: 36
0.000000/100.000000 ==> Testing loss: 1.313751    Testing error rate: 32.000000
==> Total testing loss: 135.736796    Total testing error rate: 33.620000
==> Set learning rate: 0.010000
==> Training Epoch: 37
0.000000/1000.000000 ==> Training loss: 0.373532    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.422136    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.523002    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.458311    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.341679    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.413305    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.498524    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.552020    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.330679    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.580848    Training error rate: 16.000000
==> Total training loss: 502.239270    Total training error rate: 15.690000
==> Testing Epoch: 37
0.000000/100.000000 ==> Testing loss: 1.429219    Testing error rate: 31.000000
==> Total testing loss: 135.559223    Total testing error rate: 34.110000
==> Set learning rate: 0.010000
==> Training Epoch: 38
0.000000/1000.000000 ==> Training loss: 0.356732    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.291366    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.277456    Training error rate: 10.000000
300.000000/1000.000000 ==> Training loss: 0.418224    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.357170    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.706290    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.487229    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.407470    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.663060    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.709442    Training error rate: 24.000000
==> Total training loss: 500.337695    Total training error rate: 15.784000
==> Testing Epoch: 38
0.000000/100.000000 ==> Testing loss: 1.389759    Testing error rate: 35.000000
==> Total testing loss: 138.374572    Total testing error rate: 35.500000
==> Set learning rate: 0.010000
==> Training Epoch: 39
0.000000/1000.000000 ==> Training loss: 0.459274    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.536465    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.617700    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.615587    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.495534    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.791599    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.513105    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.492338    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.627102    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.455044    Training error rate: 20.000000
==> Total training loss: 480.449114    Total training error rate: 15.076000
==> Testing Epoch: 39
0.000000/100.000000 ==> Testing loss: 1.363815    Testing error rate: 31.000000
==> Total testing loss: 127.696419    Total testing error rate: 32.310000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 40
0.000000/1000.000000 ==> Training loss: 0.194182    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.241697    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.281048    Training error rate: 10.000000
300.000000/1000.000000 ==> Training loss: 0.363479    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.284195    Training error rate: 8.000000
500.000000/1000.000000 ==> Training loss: 0.445039    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.278938    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.684486    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.481810    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.778540    Training error rate: 24.000000
==> Total training loss: 478.136966    Total training error rate: 14.926000
==> Testing Epoch: 40
0.000000/100.000000 ==> Testing loss: 1.350138    Testing error rate: 30.000000
==> Total testing loss: 135.521294    Total testing error rate: 33.850000
==> Set learning rate: 0.010000
==> Training Epoch: 41
0.000000/1000.000000 ==> Training loss: 0.315646    Training error rate: 8.000000
100.000000/1000.000000 ==> Training loss: 0.298736    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.259637    Training error rate: 4.000000
300.000000/1000.000000 ==> Training loss: 0.359596    Training error rate: 6.000000
400.000000/1000.000000 ==> Training loss: 0.342898    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.227061    Training error rate: 4.000000
600.000000/1000.000000 ==> Training loss: 0.393142    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.560591    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.519107    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.495639    Training error rate: 12.000000
==> Total training loss: 469.311807    Total training error rate: 14.712000
==> Testing Epoch: 41
0.000000/100.000000 ==> Testing loss: 1.327663    Testing error rate: 27.000000
==> Total testing loss: 136.094154    Total testing error rate: 33.550000
==> Set learning rate: 0.010000
==> Training Epoch: 42
0.000000/1000.000000 ==> Training loss: 0.580629    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.310130    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.697132    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.324427    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.353179    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.308508    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.487446    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.632748    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.518349    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.448894    Training error rate: 14.000000
==> Total training loss: 452.187179    Total training error rate: 14.088000
==> Testing Epoch: 42
0.000000/100.000000 ==> Testing loss: 1.455637    Testing error rate: 29.000000
==> Total testing loss: 135.545314    Total testing error rate: 33.300000
==> Set learning rate: 0.010000
==> Training Epoch: 43
0.000000/1000.000000 ==> Training loss: 0.262289    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.598058    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.508348    Training error rate: 10.000000
300.000000/1000.000000 ==> Training loss: 0.466317    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.397651    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.530625    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.368787    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.505289    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.625920    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.370113    Training error rate: 14.000000
==> Total training loss: 449.741242    Total training error rate: 14.174000
==> Testing Epoch: 43
0.000000/100.000000 ==> Testing loss: 1.388586    Testing error rate: 37.000000
==> Total testing loss: 130.192113    Total testing error rate: 32.210000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 44
0.000000/1000.000000 ==> Training loss: 0.397166    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.384444    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.590873    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.392634    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.478567    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.247635    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.338059    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.310488    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.537389    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.364929    Training error rate: 12.000000
==> Total training loss: 445.031131    Total training error rate: 13.970000
==> Testing Epoch: 44
0.000000/100.000000 ==> Testing loss: 1.320859    Testing error rate: 34.000000
==> Total testing loss: 136.628323    Total testing error rate: 33.460000
==> Set learning rate: 0.010000
==> Training Epoch: 45
0.000000/1000.000000 ==> Training loss: 0.301867    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.337747    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.452306    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.373118    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.495840    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.380963    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.619707    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.158824    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.138150    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.466147    Training error rate: 12.000000
==> Total training loss: 430.894954    Total training error rate: 13.492000
==> Testing Epoch: 45
0.000000/100.000000 ==> Testing loss: 1.620647    Testing error rate: 36.000000
==> Total testing loss: 140.221108    Total testing error rate: 34.600000
==> Set learning rate: 0.010000
==> Training Epoch: 46
0.000000/1000.000000 ==> Training loss: 0.518643    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.366951    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.479379    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.250875    Training error rate: 8.000000
400.000000/1000.000000 ==> Training loss: 0.283451    Training error rate: 8.000000
500.000000/1000.000000 ==> Training loss: 0.400439    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.182968    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.890913    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 0.551189    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.526570    Training error rate: 18.000000
==> Total training loss: 439.111197    Total training error rate: 13.782000
==> Testing Epoch: 46
0.000000/100.000000 ==> Testing loss: 1.476084    Testing error rate: 31.000000
==> Total testing loss: 134.898781    Total testing error rate: 33.190000
==> Set learning rate: 0.010000
==> Training Epoch: 47
0.000000/1000.000000 ==> Training loss: 0.556968    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.726485    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.375010    Training error rate: 10.000000
300.000000/1000.000000 ==> Training loss: 0.591657    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.552503    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.433610    Training error rate: 8.000000
600.000000/1000.000000 ==> Training loss: 0.361427    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.407168    Training error rate: 12.000000
800.000000/1000.000000 ==> Training loss: 0.646904    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.409015    Training error rate: 8.000000
==> Total training loss: 425.480752    Total training error rate: 13.330000
==> Testing Epoch: 47
0.000000/100.000000 ==> Testing loss: 1.619508    Testing error rate: 35.000000
==> Total testing loss: 139.018222    Total testing error rate: 33.850000
==> Set learning rate: 0.010000
==> Training Epoch: 48
0.000000/1000.000000 ==> Training loss: 0.387175    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.344849    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.464742    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.280891    Training error rate: 8.000000
400.000000/1000.000000 ==> Training loss: 0.374151    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.405832    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.316595    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.343609    Training error rate: 12.000000
800.000000/1000.000000 ==> Training loss: 0.440579    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.514646    Training error rate: 14.000000
==> Total training loss: 421.559153    Total training error rate: 13.216000
==> Testing Epoch: 48
0.000000/100.000000 ==> Testing loss: 1.388665    Testing error rate: 32.000000
==> Total testing loss: 131.418862    Total testing error rate: 32.110000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 49
0.000000/1000.000000 ==> Training loss: 0.286335    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.251153    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.521200    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.266391    Training error rate: 6.000000
400.000000/1000.000000 ==> Training loss: 0.248091    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.378449    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.392876    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.475514    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.322186    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.651202    Training error rate: 14.000000
==> Total training loss: 422.091850    Total training error rate: 13.306000
==> Testing Epoch: 49
0.000000/100.000000 ==> Testing loss: 1.391034    Testing error rate: 32.000000
==> Total testing loss: 130.939900    Total testing error rate: 32.190000
==> Set learning rate: 0.010000
==> Training Epoch: 50
0.000000/1000.000000 ==> Training loss: 0.354893    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.359127    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.255967    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.472540    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.508757    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.292315    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.482700    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.293540    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.668817    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.612447    Training error rate: 20.000000
==> Total training loss: 408.621795    Total training error rate: 12.822000
==> Testing Epoch: 50
0.000000/100.000000 ==> Testing loss: 1.467349    Testing error rate: 30.000000
==> Total testing loss: 139.476798    Total testing error rate: 33.800000
==> Set learning rate: 0.001000
==> Training Epoch: 51
0.000000/1000.000000 ==> Training loss: 0.263546    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.408361    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.286537    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.100893    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.127582    Training error rate: 4.000000
500.000000/1000.000000 ==> Training loss: 0.126276    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.051277    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.146176    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.200365    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.156050    Training error rate: 4.000000
==> Total training loss: 182.323293    Total training error rate: 5.124000
==> Testing Epoch: 51
0.000000/100.000000 ==> Testing loss: 1.039829    Testing error rate: 26.000000
==> Total testing loss: 98.009625    Total testing error rate: 25.130000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 52
0.000000/1000.000000 ==> Training loss: 0.161465    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.130484    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.146930    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.147184    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.123698    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.050270    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.127200    Training error rate: 4.000000
700.000000/1000.000000 ==> Training loss: 0.064714    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.063733    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.091286    Training error rate: 0.000000
==> Total training loss: 109.452524    Total training error rate: 2.802000
==> Testing Epoch: 52
0.000000/100.000000 ==> Testing loss: 0.967126    Testing error rate: 26.000000
==> Total testing loss: 97.232719    Total testing error rate: 24.780000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 53
0.000000/1000.000000 ==> Training loss: 0.061779    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.198622    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.134314    Training error rate: 4.000000
300.000000/1000.000000 ==> Training loss: 0.138555    Training error rate: 8.000000
400.000000/1000.000000 ==> Training loss: 0.066358    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.069821    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.035040    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.087638    Training error rate: 4.000000
800.000000/1000.000000 ==> Training loss: 0.049972    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.098665    Training error rate: 4.000000
==> Total training loss: 86.452801    Total training error rate: 2.056000
==> Testing Epoch: 53
0.000000/100.000000 ==> Testing loss: 0.996089    Testing error rate: 25.000000
==> Total testing loss: 96.356685    Total testing error rate: 24.270000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 54
0.000000/1000.000000 ==> Training loss: 0.122988    Training error rate: 4.000000
100.000000/1000.000000 ==> Training loss: 0.055885    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.102953    Training error rate: 4.000000
300.000000/1000.000000 ==> Training loss: 0.088379    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.120404    Training error rate: 4.000000
500.000000/1000.000000 ==> Training loss: 0.078403    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.094636    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.031027    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.090296    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.046363    Training error rate: 2.000000
==> Total training loss: 74.433151    Total training error rate: 1.678000
==> Testing Epoch: 54
0.000000/100.000000 ==> Testing loss: 0.984923    Testing error rate: 25.000000
==> Total testing loss: 96.415824    Total testing error rate: 24.400000
==> Set learning rate: 0.001000
==> Training Epoch: 55
0.000000/1000.000000 ==> Training loss: 0.105239    Training error rate: 4.000000
100.000000/1000.000000 ==> Training loss: 0.054095    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.047134    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.041330    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.103047    Training error rate: 4.000000
500.000000/1000.000000 ==> Training loss: 0.039750    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.049535    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.096431    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.026650    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.035381    Training error rate: 0.000000
==> Total training loss: 63.759317    Total training error rate: 1.400000
==> Testing Epoch: 55
0.000000/100.000000 ==> Testing loss: 1.041708    Testing error rate: 24.000000
==> Total testing loss: 96.984179    Total testing error rate: 24.400000
==> Set learning rate: 0.001000
==> Training Epoch: 56
0.000000/1000.000000 ==> Training loss: 0.045476    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.033687    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.033719    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.065044    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.084440    Training error rate: 4.000000
500.000000/1000.000000 ==> Training loss: 0.049117    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.066549    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.067497    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.056518    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.057972    Training error rate: 0.000000
==> Total training loss: 59.269418    Total training error rate: 1.262000
==> Testing Epoch: 56
0.000000/100.000000 ==> Testing loss: 0.986507    Testing error rate: 25.000000
==> Total testing loss: 96.765734    Total testing error rate: 24.000000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 57
0.000000/1000.000000 ==> Training loss: 0.057265    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.038553    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.046123    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.130672    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.028729    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.062284    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.035695    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.100861    Training error rate: 4.000000
800.000000/1000.000000 ==> Training loss: 0.070961    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.092287    Training error rate: 2.000000
==> Total training loss: 52.688600    Total training error rate: 1.060000
==> Testing Epoch: 57
0.000000/100.000000 ==> Testing loss: 1.010573    Testing error rate: 23.000000
==> Total testing loss: 97.622772    Total testing error rate: 24.450000
==> Set learning rate: 0.001000
==> Training Epoch: 58
0.000000/1000.000000 ==> Training loss: 0.058143    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.026749    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.072776    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.021605    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.076461    Training error rate: 4.000000
500.000000/1000.000000 ==> Training loss: 0.035274    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.064367    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.030197    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.032898    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.064271    Training error rate: 4.000000
==> Total training loss: 48.671166    Total training error rate: 0.930000
==> Testing Epoch: 58
0.000000/100.000000 ==> Testing loss: 0.987502    Testing error rate: 24.000000
==> Total testing loss: 97.174281    Total testing error rate: 24.140000
==> Set learning rate: 0.001000
==> Training Epoch: 59
0.000000/1000.000000 ==> Training loss: 0.018754    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.053960    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.026938    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.050569    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.026264    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.045504    Training error rate: 4.000000
600.000000/1000.000000 ==> Training loss: 0.034490    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.079313    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.102033    Training error rate: 4.000000
900.000000/1000.000000 ==> Training loss: 0.038733    Training error rate: 0.000000
==> Total training loss: 44.288960    Total training error rate: 0.826000
==> Testing Epoch: 59
0.000000/100.000000 ==> Testing loss: 1.020272    Testing error rate: 26.000000
==> Total testing loss: 97.612033    Total testing error rate: 24.210000
==> Set learning rate: 0.001000
==> Training Epoch: 60
0.000000/1000.000000 ==> Training loss: 0.045345    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.026764    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.054765    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.034407    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.071620    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.021509    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.047332    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.019836    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.060295    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.074723    Training error rate: 4.000000
==> Total training loss: 40.192181    Total training error rate: 0.724000
==> Testing Epoch: 60
0.000000/100.000000 ==> Testing loss: 1.042673    Testing error rate: 25.000000
==> Total testing loss: 97.905589    Total testing error rate: 24.330000
==> Set learning rate: 0.001000
==> Training Epoch: 61
0.000000/1000.000000 ==> Training loss: 0.047650    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.014162    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.020609    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.032066    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.018640    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.006379    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.010021    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.035478    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.042461    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.028441    Training error rate: 0.000000
==> Total training loss: 37.519680    Total training error rate: 0.606000
==> Testing Epoch: 61
0.000000/100.000000 ==> Testing loss: 1.047288    Testing error rate: 24.000000
==> Total testing loss: 98.411233    Total testing error rate: 23.910000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 62
0.000000/1000.000000 ==> Training loss: 0.015061    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.037644    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.016324    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.026211    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.024109    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.022685    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.055260    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.010746    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.031583    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.009761    Training error rate: 0.000000
==> Total training loss: 35.791826    Total training error rate: 0.586000
==> Testing Epoch: 62
0.000000/100.000000 ==> Testing loss: 1.033335    Testing error rate: 22.000000
==> Total testing loss: 98.880046    Total testing error rate: 24.290000
==> Set learning rate: 0.001000
==> Training Epoch: 63
0.000000/1000.000000 ==> Training loss: 0.024393    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.029098    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.035869    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.051960    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.069210    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.052000    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.019460    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.022447    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.027774    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.046546    Training error rate: 2.000000
==> Total training loss: 32.816744    Total training error rate: 0.498000
==> Testing Epoch: 63
0.000000/100.000000 ==> Testing loss: 1.059809    Testing error rate: 24.000000
==> Total testing loss: 98.205803    Total testing error rate: 23.780000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 64
0.000000/1000.000000 ==> Training loss: 0.020841    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.027446    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.020629    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.038475    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.024628    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.029180    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.034867    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.080750    Training error rate: 4.000000
800.000000/1000.000000 ==> Training loss: 0.010007    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.017818    Training error rate: 0.000000
==> Total training loss: 31.689611    Total training error rate: 0.506000
==> Testing Epoch: 64
0.000000/100.000000 ==> Testing loss: 1.042821    Testing error rate: 24.000000
==> Total testing loss: 98.592841    Total testing error rate: 24.050000
==> Set learning rate: 0.001000
==> Training Epoch: 65
0.000000/1000.000000 ==> Training loss: 0.042286    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.044864    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.010278    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.028638    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.025317    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.026353    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.028189    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.029049    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.013932    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.031995    Training error rate: 0.000000
==> Total training loss: 30.611662    Total training error rate: 0.474000
==> Testing Epoch: 65
0.000000/100.000000 ==> Testing loss: 1.039850    Testing error rate: 23.000000
==> Total testing loss: 99.134130    Total testing error rate: 23.950000
==> Set learning rate: 0.001000
==> Training Epoch: 66
0.000000/1000.000000 ==> Training loss: 0.009567    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.008290    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.010932    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.026862    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.015309    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.017819    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.024772    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.036196    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.020128    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.067518    Training error rate: 4.000000
==> Total training loss: 27.497363    Total training error rate: 0.362000
==> Testing Epoch: 66
0.000000/100.000000 ==> Testing loss: 1.006496    Testing error rate: 24.000000
==> Total testing loss: 99.526162    Total testing error rate: 24.140000
==> Set learning rate: 0.001000
==> Training Epoch: 67
0.000000/1000.000000 ==> Training loss: 0.014077    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.013234    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.015747    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.011970    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.017637    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.027533    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.027078    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.014143    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.007773    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.025191    Training error rate: 0.000000
==> Total training loss: 27.017562    Total training error rate: 0.378000
==> Testing Epoch: 67
0.000000/100.000000 ==> Testing loss: 1.053990    Testing error rate: 25.000000
==> Total testing loss: 99.752688    Total testing error rate: 23.800000
==> Set learning rate: 0.001000
==> Training Epoch: 68
0.000000/1000.000000 ==> Training loss: 0.022192    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.018275    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.015947    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.020842    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.013890    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.021325    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.110484    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.019057    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.019050    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.017379    Training error rate: 0.000000
==> Total training loss: 24.902503    Total training error rate: 0.298000
==> Testing Epoch: 68
0.000000/100.000000 ==> Testing loss: 1.035249    Testing error rate: 25.000000
==> Total testing loss: 99.334550    Total testing error rate: 23.840000
==> Set learning rate: 0.001000
==> Training Epoch: 69
0.000000/1000.000000 ==> Training loss: 0.020124    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.010233    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.014154    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.007740    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.012390    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.013605    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.012419    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.016951    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.017799    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.033621    Training error rate: 0.000000
==> Total training loss: 24.323949    Total training error rate: 0.304000
==> Testing Epoch: 69
0.000000/100.000000 ==> Testing loss: 1.105018    Testing error rate: 24.000000
==> Total testing loss: 99.698951    Total testing error rate: 23.930000
==> Set learning rate: 0.001000
==> Training Epoch: 70
0.000000/1000.000000 ==> Training loss: 0.036174    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.025089    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.028928    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.021605    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.009798    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.010119    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.014829    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.017072    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.029345    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.035849    Training error rate: 0.000000
==> Total training loss: 24.659296    Total training error rate: 0.356000
==> Testing Epoch: 70
0.000000/100.000000 ==> Testing loss: 1.051256    Testing error rate: 23.000000
==> Total testing loss: 100.283529    Total testing error rate: 23.860000
==> Set learning rate: 0.001000
==> Training Epoch: 71
0.000000/1000.000000 ==> Training loss: 0.017735    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.026938    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008749    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.020752    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.010536    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.022612    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013082    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.062563    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.014773    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.010013    Training error rate: 0.000000
==> Total training loss: 23.157343    Total training error rate: 0.284000
==> Testing Epoch: 71
0.000000/100.000000 ==> Testing loss: 1.035890    Testing error rate: 23.000000
==> Total testing loss: 100.024229    Total testing error rate: 23.780000
==> Set learning rate: 0.001000
==> Training Epoch: 72
0.000000/1000.000000 ==> Training loss: 0.008133    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.007058    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.044904    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.006675    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.022045    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.013662    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.014007    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.034005    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.011132    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.022788    Training error rate: 0.000000
==> Total training loss: 21.948692    Total training error rate: 0.264000
==> Testing Epoch: 72
0.000000/100.000000 ==> Testing loss: 1.064477    Testing error rate: 25.000000
==> Total testing loss: 99.770982    Total testing error rate: 23.770000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 73
0.000000/1000.000000 ==> Training loss: 0.027653    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.026428    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.021279    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.007465    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.015618    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.026780    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.011495    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.013569    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.018581    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.026307    Training error rate: 0.000000
==> Total training loss: 21.274040    Total training error rate: 0.250000
==> Testing Epoch: 73
0.000000/100.000000 ==> Testing loss: 1.046895    Testing error rate: 24.000000
==> Total testing loss: 99.786021    Total testing error rate: 23.970000
==> Set learning rate: 0.001000
==> Training Epoch: 74
0.000000/1000.000000 ==> Training loss: 0.012324    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.010984    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013283    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.015585    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.022890    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.018388    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.007926    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.072352    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.025526    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.048767    Training error rate: 0.000000
==> Total training loss: 20.035124    Total training error rate: 0.210000
==> Testing Epoch: 74
0.000000/100.000000 ==> Testing loss: 1.050299    Testing error rate: 22.000000
==> Total testing loss: 100.445818    Total testing error rate: 23.760000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 75
0.000000/1000.000000 ==> Training loss: 0.024622    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.006860    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.033674    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.012166    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.014650    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.006788    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.023731    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.011422    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.007303    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.008262    Training error rate: 0.000000
==> Total training loss: 20.461660    Total training error rate: 0.266000
==> Testing Epoch: 75
0.000000/100.000000 ==> Testing loss: 1.026296    Testing error rate: 24.000000
==> Total testing loss: 100.033897    Total testing error rate: 23.840000
==> Set learning rate: 0.001000
==> Training Epoch: 76
0.000000/1000.000000 ==> Training loss: 0.027345    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.010073    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.012370    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.021094    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.017485    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.040864    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.013726    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.013941    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.021397    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.042513    Training error rate: 2.000000
==> Total training loss: 19.450072    Total training error rate: 0.214000
==> Testing Epoch: 76
0.000000/100.000000 ==> Testing loss: 1.009680    Testing error rate: 24.000000
==> Total testing loss: 100.356053    Total testing error rate: 23.730000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 77
0.000000/1000.000000 ==> Training loss: 0.019137    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.011882    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.005162    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.023012    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.058511    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.036262    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.020997    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.005963    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.021323    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.007543    Training error rate: 0.000000
==> Total training loss: 19.957717    Total training error rate: 0.276000
==> Testing Epoch: 77
0.000000/100.000000 ==> Testing loss: 1.046468    Testing error rate: 24.000000
==> Total testing loss: 100.119251    Total testing error rate: 23.830000
==> Set learning rate: 0.001000
==> Training Epoch: 78
0.000000/1000.000000 ==> Training loss: 0.007315    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.012476    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.010802    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.030715    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.051534    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.024135    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.010390    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.010565    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010213    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.016343    Training error rate: 0.000000
==> Total training loss: 18.386011    Total training error rate: 0.208000
==> Testing Epoch: 78
0.000000/100.000000 ==> Testing loss: 1.042721    Testing error rate: 24.000000
==> Total testing loss: 99.761186    Total testing error rate: 23.820000
==> Set learning rate: 0.001000
==> Training Epoch: 79
0.000000/1000.000000 ==> Training loss: 0.015185    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.032900    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.017776    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.011649    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.013518    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.047341    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.006628    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.029865    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010315    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.018187    Training error rate: 0.000000
==> Total training loss: 17.703415    Total training error rate: 0.182000
==> Testing Epoch: 79
0.000000/100.000000 ==> Testing loss: 1.023255    Testing error rate: 23.000000
==> Total testing loss: 99.746387    Total testing error rate: 23.540000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 80
0.000000/1000.000000 ==> Training loss: 0.012933    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.024611    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.014616    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.010896    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.013617    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.025383    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013335    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.018200    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010515    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.008386    Training error rate: 0.000000
==> Total training loss: 17.807547    Total training error rate: 0.190000
==> Testing Epoch: 80
0.000000/100.000000 ==> Testing loss: 0.958680    Testing error rate: 23.000000
==> Total testing loss: 100.440076    Total testing error rate: 23.660000
==> Set learning rate: 0.001000
==> Training Epoch: 81
0.000000/1000.000000 ==> Training loss: 0.013806    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.011163    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.030604    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.007686    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.009172    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.017348    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.006149    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.015272    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.009659    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.015380    Training error rate: 0.000000
==> Total training loss: 16.452942    Total training error rate: 0.128000
==> Testing Epoch: 81
0.000000/100.000000 ==> Testing loss: 1.007911    Testing error rate: 23.000000
==> Total testing loss: 101.038582    Total testing error rate: 23.550000
==> Set learning rate: 0.001000
==> Training Epoch: 82
0.000000/1000.000000 ==> Training loss: 0.010154    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.017215    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.012395    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.012022    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.011993    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.009614    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.024676    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.009050    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.007528    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.016275    Training error rate: 0.000000
==> Total training loss: 17.042966    Total training error rate: 0.170000
==> Testing Epoch: 82
0.000000/100.000000 ==> Testing loss: 0.977618    Testing error rate: 22.000000
==> Total testing loss: 100.383634    Total testing error rate: 23.620000
==> Set learning rate: 0.001000
==> Training Epoch: 83
0.000000/1000.000000 ==> Training loss: 0.012814    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.034186    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.037880    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.019341    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.009920    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.015621    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.023000    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.027420    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.025224    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.010125    Training error rate: 0.000000
==> Total training loss: 16.556099    Total training error rate: 0.162000
==> Testing Epoch: 83
0.000000/100.000000 ==> Testing loss: 0.955289    Testing error rate: 22.000000
==> Total testing loss: 100.207672    Total testing error rate: 23.300000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 84
0.000000/1000.000000 ==> Training loss: 0.008200    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.008664    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013373    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.015640    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.006534    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.022497    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.007821    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.010915    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.007866    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.016626    Training error rate: 0.000000
==> Total training loss: 15.671150    Total training error rate: 0.150000
==> Testing Epoch: 84
0.000000/100.000000 ==> Testing loss: 0.974630    Testing error rate: 24.000000
==> Total testing loss: 99.629794    Total testing error rate: 23.440000
==> Set learning rate: 0.001000
==> Training Epoch: 85
0.000000/1000.000000 ==> Training loss: 0.003934    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.040347    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.021163    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.012843    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.005031    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.009664    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.012770    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.010754    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.057147    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.016707    Training error rate: 0.000000
==> Total training loss: 15.616838    Total training error rate: 0.158000
==> Testing Epoch: 85
0.000000/100.000000 ==> Testing loss: 1.005675    Testing error rate: 23.000000
==> Total testing loss: 100.451630    Total testing error rate: 23.590000
==> Set learning rate: 0.001000
==> Training Epoch: 86
0.000000/1000.000000 ==> Training loss: 0.007809    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.015708    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008042    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.008756    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.017232    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.006358    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.035164    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.008492    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.011282    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.003793    Training error rate: 0.000000
==> Total training loss: 15.664555    Total training error rate: 0.140000
==> Testing Epoch: 86
0.000000/100.000000 ==> Testing loss: 1.013200    Testing error rate: 22.000000
==> Total testing loss: 100.490995    Total testing error rate: 23.760000
==> Set learning rate: 0.001000
==> Training Epoch: 87
0.000000/1000.000000 ==> Training loss: 0.018423    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.016834    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.026309    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.036782    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.011406    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.003614    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.019605    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.027250    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.028364    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.013969    Training error rate: 0.000000
==> Total training loss: 15.094114    Total training error rate: 0.156000
==> Testing Epoch: 87
0.000000/100.000000 ==> Testing loss: 0.965572    Testing error rate: 22.000000
==> Total testing loss: 99.892866    Total testing error rate: 23.610000
==> Set learning rate: 0.001000
==> Training Epoch: 88
0.000000/1000.000000 ==> Training loss: 0.014173    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.013395    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.009739    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.005881    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.008698    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.009242    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.010745    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.010272    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.014904    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.012528    Training error rate: 0.000000
==> Total training loss: 14.986319    Total training error rate: 0.148000
==> Testing Epoch: 88
0.000000/100.000000 ==> Testing loss: 1.015531    Testing error rate: 23.000000
==> Total testing loss: 99.664817    Total testing error rate: 23.550000
==> Set learning rate: 0.001000
==> Training Epoch: 89
0.000000/1000.000000 ==> Training loss: 0.007089    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.021430    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.010535    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.011674    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.008574    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.010977    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.020610    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.008999    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010783    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.028148    Training error rate: 0.000000
==> Total training loss: 14.943875    Total training error rate: 0.166000
==> Testing Epoch: 89
0.000000/100.000000 ==> Testing loss: 1.005782    Testing error rate: 20.000000
==> Total testing loss: 100.049338    Total testing error rate: 23.540000
==> Set learning rate: 0.001000
==> Training Epoch: 90
0.000000/1000.000000 ==> Training loss: 0.016944    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.021438    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.036086    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.007775    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.012972    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.010435    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.011294    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.029869    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.006734    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.010524    Training error rate: 0.000000
==> Total training loss: 14.504620    Total training error rate: 0.130000
==> Testing Epoch: 90
0.000000/100.000000 ==> Testing loss: 1.068969    Testing error rate: 22.000000
==> Total testing loss: 100.731037    Total testing error rate: 23.550000
==> Set learning rate: 0.001000
==> Training Epoch: 91
0.000000/1000.000000 ==> Training loss: 0.007924    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.009229    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.014220    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.008222    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.009951    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.021456    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013319    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.030890    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.011541    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.010417    Training error rate: 0.000000
==> Total training loss: 14.206728    Total training error rate: 0.124000
==> Testing Epoch: 91
0.000000/100.000000 ==> Testing loss: 1.019653    Testing error rate: 23.000000
==> Total testing loss: 100.320974    Total testing error rate: 23.610000
==> Set learning rate: 0.001000
==> Training Epoch: 92
0.000000/1000.000000 ==> Training loss: 0.012232    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.009003    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013689    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.020064    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.034016    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.027087    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013476    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.008200    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.013698    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.007586    Training error rate: 0.000000
==> Total training loss: 14.220978    Total training error rate: 0.124000
==> Testing Epoch: 92
0.000000/100.000000 ==> Testing loss: 1.029152    Testing error rate: 23.000000
==> Total testing loss: 100.719087    Total testing error rate: 23.430000
==> Set learning rate: 0.001000
==> Training Epoch: 93
0.000000/1000.000000 ==> Training loss: 0.011388    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.015057    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.019376    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.007421    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.018245    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011757    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.006547    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.030401    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.014651    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.013834    Training error rate: 0.000000
==> Total training loss: 13.847267    Total training error rate: 0.130000
==> Testing Epoch: 93
0.000000/100.000000 ==> Testing loss: 1.091127    Testing error rate: 23.000000
==> Total testing loss: 100.025977    Total testing error rate: 23.630000
==> Set learning rate: 0.001000
==> Training Epoch: 94
0.000000/1000.000000 ==> Training loss: 0.010774    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.010846    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013410    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.010792    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.035614    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011838    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.011358    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.007120    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.011521    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.005104    Training error rate: 0.000000
==> Total training loss: 14.015298    Total training error rate: 0.134000
==> Testing Epoch: 94
0.000000/100.000000 ==> Testing loss: 1.007468    Testing error rate: 22.000000
==> Total testing loss: 100.111760    Total testing error rate: 23.300000
==> Set learning rate: 0.001000
==> Training Epoch: 95
0.000000/1000.000000 ==> Training loss: 0.009161    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.008605    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008020    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.007878    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.007863    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.007757    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.011066    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.013843    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.029544    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.009474    Training error rate: 0.000000
==> Total training loss: 13.409258    Total training error rate: 0.116000
==> Testing Epoch: 95
0.000000/100.000000 ==> Testing loss: 1.029495    Testing error rate: 22.000000
==> Total testing loss: 100.070947    Total testing error rate: 23.430000
==> Set learning rate: 0.001000
==> Training Epoch: 96
0.000000/1000.000000 ==> Training loss: 0.015506    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.008824    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.014376    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.015241    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.013631    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.009068    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.014041    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.025086    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.009304    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.005916    Training error rate: 0.000000
==> Total training loss: 13.236030    Total training error rate: 0.096000
==> Testing Epoch: 96
0.000000/100.000000 ==> Testing loss: 1.061783    Testing error rate: 23.000000
==> Total testing loss: 99.815789    Total testing error rate: 23.390000
==> Set learning rate: 0.001000
==> Training Epoch: 97
0.000000/1000.000000 ==> Training loss: 0.018058    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.030783    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.008268    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.007216    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.008828    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.009083    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.016893    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.016626    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.007357    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.007150    Training error rate: 0.000000
==> Total training loss: 13.741963    Total training error rate: 0.116000
==> Testing Epoch: 97
0.000000/100.000000 ==> Testing loss: 1.059374    Testing error rate: 23.000000
==> Total testing loss: 99.907359    Total testing error rate: 23.540000
==> Set learning rate: 0.001000
==> Training Epoch: 98
0.000000/1000.000000 ==> Training loss: 0.020471    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.012189    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.006113    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.005801    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.015784    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.010611    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.017923    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.030364    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.004993    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.011316    Training error rate: 0.000000
==> Total training loss: 13.273300    Total training error rate: 0.104000
==> Testing Epoch: 98
0.000000/100.000000 ==> Testing loss: 1.036243    Testing error rate: 20.000000
==> Total testing loss: 99.445050    Total testing error rate: 23.260000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 99
0.000000/1000.000000 ==> Training loss: 0.018604    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.006369    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.007121    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.021582    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.008197    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.005543    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.027723    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.029259    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.008381    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.006326    Training error rate: 0.000000
==> Total training loss: 13.099230    Total training error rate: 0.102000
==> Testing Epoch: 99
0.000000/100.000000 ==> Testing loss: 0.988696    Testing error rate: 22.000000
==> Total testing loss: 99.741964    Total testing error rate: 23.390000
==> Set learning rate: 0.001000
==> Training Epoch: 100
0.000000/1000.000000 ==> Training loss: 0.008411    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.009946    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013960    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.007995    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.012612    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.007911    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.011469    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.011583    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.004211    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.012364    Training error rate: 0.000000
==> Total training loss: 12.551073    Total training error rate: 0.112000
==> Testing Epoch: 100
0.000000/100.000000 ==> Testing loss: 1.026749    Testing error rate: 23.000000
==> Total testing loss: 100.200804    Total testing error rate: 23.540000
