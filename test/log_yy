==> Init variables..
==> Init seed..
==> Download data..
Files already downloaded and verified
==> Calculate mean and std..
==> Prepare training transform..
==> Prepare testing transform..
==> Init dataloader..
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
==> Set learning rate: 0.010000
==> Training Epoch: 1
0.000000/1000.000000 ==> Training loss: 5.099441    Training error rate: 100.000000
50.000000/1000.000000 ==> Training loss: 5.162900    Training error rate: 100.000000
100.000000/1000.000000 ==> Training loss: 4.416446    Training error rate: 92.000000
150.000000/1000.000000 ==> Training loss: 4.146763    Training error rate: 92.000000
200.000000/1000.000000 ==> Training loss: 4.196798    Training error rate: 94.000000
250.000000/1000.000000 ==> Training loss: 4.275916    Training error rate: 98.000000
300.000000/1000.000000 ==> Training loss: 4.085358    Training error rate: 96.000000
350.000000/1000.000000 ==> Training loss: 3.955751    Training error rate: 92.000000
400.000000/1000.000000 ==> Training loss: 4.038115    Training error rate: 90.000000
450.000000/1000.000000 ==> Training loss: 3.933294    Training error rate: 90.000000
500.000000/1000.000000 ==> Training loss: 3.824020    Training error rate: 84.000000
550.000000/1000.000000 ==> Training loss: 4.291460    Training error rate: 98.000000
600.000000/1000.000000 ==> Training loss: 3.678003    Training error rate: 86.000000
650.000000/1000.000000 ==> Training loss: 3.824186    Training error rate: 90.000000
700.000000/1000.000000 ==> Training loss: 4.028839    Training error rate: 94.000000
750.000000/1000.000000 ==> Training loss: 3.705155    Training error rate: 82.000000
800.000000/1000.000000 ==> Training loss: 3.787447    Training error rate: 88.000000
850.000000/1000.000000 ==> Training loss: 3.356160    Training error rate: 88.000000
900.000000/1000.000000 ==> Training loss: 3.684165    Training error rate: 86.000000
950.000000/1000.000000 ==> Training loss: 3.438225    Training error rate: 82.000000
==> Total training loss: 4026.317283    Total training error rate: 92.072000
==> Testing Epoch: 1
0.000000/100.000000 ==> Testing loss: 3.758418    Testing error rate: 86.000000
50.000000/100.000000 ==> Testing loss: 3.650816    Testing error rate: 81.000000
==> Total testing loss: 365.586010    Total testing error rate: 86.580000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 2
0.000000/1000.000000 ==> Training loss: 3.549226    Training error rate: 86.000000
50.000000/1000.000000 ==> Training loss: 3.510678    Training error rate: 88.000000
100.000000/1000.000000 ==> Training loss: 3.433121    Training error rate: 84.000000
150.000000/1000.000000 ==> Training loss: 3.509465    Training error rate: 90.000000
200.000000/1000.000000 ==> Training loss: 3.349941    Training error rate: 82.000000
250.000000/1000.000000 ==> Training loss: 3.458431    Training error rate: 92.000000
300.000000/1000.000000 ==> Training loss: 3.431476    Training error rate: 82.000000
350.000000/1000.000000 ==> Training loss: 3.303687    Training error rate: 78.000000
400.000000/1000.000000 ==> Training loss: 3.742269    Training error rate: 94.000000
450.000000/1000.000000 ==> Training loss: 3.477706    Training error rate: 78.000000
500.000000/1000.000000 ==> Training loss: 3.338876    Training error rate: 86.000000
550.000000/1000.000000 ==> Training loss: 3.078589    Training error rate: 72.000000
600.000000/1000.000000 ==> Training loss: 3.103625    Training error rate: 82.000000
650.000000/1000.000000 ==> Training loss: 3.437057    Training error rate: 82.000000
700.000000/1000.000000 ==> Training loss: 3.287236    Training error rate: 76.000000
750.000000/1000.000000 ==> Training loss: 2.865732    Training error rate: 78.000000
800.000000/1000.000000 ==> Training loss: 3.272567    Training error rate: 82.000000
850.000000/1000.000000 ==> Training loss: 3.279181    Training error rate: 80.000000
900.000000/1000.000000 ==> Training loss: 2.880152    Training error rate: 74.000000
950.000000/1000.000000 ==> Training loss: 3.383698    Training error rate: 72.000000
==> Total training loss: 3359.914930    Total training error rate: 81.912000
==> Testing Epoch: 2
0.000000/100.000000 ==> Testing loss: 3.374336    Testing error rate: 80.000000
50.000000/100.000000 ==> Testing loss: 3.117177    Testing error rate: 74.000000
==> Total testing loss: 315.195562    Total testing error rate: 77.330000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 3
0.000000/1000.000000 ==> Training loss: 3.145865    Training error rate: 74.000000
50.000000/1000.000000 ==> Training loss: 2.870741    Training error rate: 62.000000
100.000000/1000.000000 ==> Training loss: 3.297317    Training error rate: 82.000000
150.000000/1000.000000 ==> Training loss: 3.799235    Training error rate: 84.000000
200.000000/1000.000000 ==> Training loss: 2.678745    Training error rate: 82.000000
250.000000/1000.000000 ==> Training loss: 2.843819    Training error rate: 74.000000
300.000000/1000.000000 ==> Training loss: 3.275985    Training error rate: 78.000000
350.000000/1000.000000 ==> Training loss: 2.756059    Training error rate: 66.000000
400.000000/1000.000000 ==> Training loss: 2.847559    Training error rate: 78.000000
450.000000/1000.000000 ==> Training loss: 2.795875    Training error rate: 72.000000
500.000000/1000.000000 ==> Training loss: 2.741857    Training error rate: 68.000000
550.000000/1000.000000 ==> Training loss: 2.882214    Training error rate: 70.000000
600.000000/1000.000000 ==> Training loss: 2.903807    Training error rate: 74.000000
650.000000/1000.000000 ==> Training loss: 2.969321    Training error rate: 72.000000
700.000000/1000.000000 ==> Training loss: 2.800308    Training error rate: 70.000000
750.000000/1000.000000 ==> Training loss: 2.433994    Training error rate: 68.000000
800.000000/1000.000000 ==> Training loss: 2.547932    Training error rate: 66.000000
850.000000/1000.000000 ==> Training loss: 2.979413    Training error rate: 68.000000
900.000000/1000.000000 ==> Training loss: 2.698562    Training error rate: 64.000000
950.000000/1000.000000 ==> Training loss: 2.946457    Training error rate: 70.000000
==> Total training loss: 2873.056162    Total training error rate: 73.292000
==> Testing Epoch: 3
0.000000/100.000000 ==> Testing loss: 2.909447    Testing error rate: 70.000000
50.000000/100.000000 ==> Testing loss: 2.580161    Testing error rate: 69.000000
==> Total testing loss: 278.947637    Total testing error rate: 70.880000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 4
0.000000/1000.000000 ==> Training loss: 2.840415    Training error rate: 72.000000
50.000000/1000.000000 ==> Training loss: 2.582368    Training error rate: 70.000000
100.000000/1000.000000 ==> Training loss: 2.590998    Training error rate: 66.000000
150.000000/1000.000000 ==> Training loss: 2.331321    Training error rate: 66.000000
200.000000/1000.000000 ==> Training loss: 2.334332    Training error rate: 64.000000
250.000000/1000.000000 ==> Training loss: 2.528769    Training error rate: 68.000000
300.000000/1000.000000 ==> Training loss: 2.625975    Training error rate: 68.000000
350.000000/1000.000000 ==> Training loss: 2.551847    Training error rate: 64.000000
400.000000/1000.000000 ==> Training loss: 2.384376    Training error rate: 62.000000
450.000000/1000.000000 ==> Training loss: 2.799548    Training error rate: 76.000000
500.000000/1000.000000 ==> Training loss: 2.656728    Training error rate: 84.000000
550.000000/1000.000000 ==> Training loss: 2.758385    Training error rate: 68.000000
600.000000/1000.000000 ==> Training loss: 2.019773    Training error rate: 50.000000
650.000000/1000.000000 ==> Training loss: 2.257355    Training error rate: 64.000000
700.000000/1000.000000 ==> Training loss: 2.132658    Training error rate: 52.000000
750.000000/1000.000000 ==> Training loss: 2.271585    Training error rate: 52.000000
800.000000/1000.000000 ==> Training loss: 2.052769    Training error rate: 60.000000
850.000000/1000.000000 ==> Training loss: 2.125876    Training error rate: 54.000000
900.000000/1000.000000 ==> Training loss: 2.733238    Training error rate: 72.000000
950.000000/1000.000000 ==> Training loss: 2.218821    Training error rate: 58.000000
==> Total training loss: 2457.501722    Total training error rate: 64.844000
==> Testing Epoch: 4
0.000000/100.000000 ==> Testing loss: 2.381531    Testing error rate: 62.000000
50.000000/100.000000 ==> Testing loss: 2.129648    Testing error rate: 52.000000
==> Total testing loss: 228.611561    Total testing error rate: 60.640000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 5
0.000000/1000.000000 ==> Training loss: 2.190807    Training error rate: 58.000000
50.000000/1000.000000 ==> Training loss: 1.896788    Training error rate: 56.000000
100.000000/1000.000000 ==> Training loss: 2.148231    Training error rate: 58.000000
150.000000/1000.000000 ==> Training loss: 2.085775    Training error rate: 62.000000
200.000000/1000.000000 ==> Training loss: 2.178115    Training error rate: 58.000000
250.000000/1000.000000 ==> Training loss: 2.192413    Training error rate: 60.000000
300.000000/1000.000000 ==> Training loss: 2.155362    Training error rate: 54.000000
350.000000/1000.000000 ==> Training loss: 2.115980    Training error rate: 56.000000
400.000000/1000.000000 ==> Training loss: 2.430694    Training error rate: 66.000000
450.000000/1000.000000 ==> Training loss: 2.306495    Training error rate: 64.000000
500.000000/1000.000000 ==> Training loss: 2.201328    Training error rate: 62.000000
550.000000/1000.000000 ==> Training loss: 1.843375    Training error rate: 52.000000
600.000000/1000.000000 ==> Training loss: 2.112009    Training error rate: 58.000000
650.000000/1000.000000 ==> Training loss: 1.933476    Training error rate: 60.000000
700.000000/1000.000000 ==> Training loss: 2.056706    Training error rate: 44.000000
750.000000/1000.000000 ==> Training loss: 1.873052    Training error rate: 58.000000
800.000000/1000.000000 ==> Training loss: 2.080417    Training error rate: 64.000000
850.000000/1000.000000 ==> Training loss: 2.575918    Training error rate: 72.000000
900.000000/1000.000000 ==> Training loss: 2.106843    Training error rate: 58.000000
950.000000/1000.000000 ==> Training loss: 1.903697    Training error rate: 56.000000
==> Total training loss: 2151.165540    Total training error rate: 58.320000
==> Testing Epoch: 5
0.000000/100.000000 ==> Testing loss: 2.217825    Testing error rate: 56.000000
50.000000/100.000000 ==> Testing loss: 2.022952    Testing error rate: 55.000000
==> Total testing loss: 209.899406    Total testing error rate: 55.800000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 6
0.000000/1000.000000 ==> Training loss: 2.059193    Training error rate: 54.000000
50.000000/1000.000000 ==> Training loss: 2.071478    Training error rate: 54.000000
100.000000/1000.000000 ==> Training loss: 2.033415    Training error rate: 60.000000
150.000000/1000.000000 ==> Training loss: 1.547388    Training error rate: 44.000000
200.000000/1000.000000 ==> Training loss: 1.880664    Training error rate: 46.000000
250.000000/1000.000000 ==> Training loss: 2.022408    Training error rate: 50.000000
300.000000/1000.000000 ==> Training loss: 2.078007    Training error rate: 56.000000
350.000000/1000.000000 ==> Training loss: 1.965082    Training error rate: 54.000000
400.000000/1000.000000 ==> Training loss: 2.158001    Training error rate: 58.000000
450.000000/1000.000000 ==> Training loss: 1.924524    Training error rate: 44.000000
500.000000/1000.000000 ==> Training loss: 1.738860    Training error rate: 52.000000
550.000000/1000.000000 ==> Training loss: 1.860035    Training error rate: 54.000000
600.000000/1000.000000 ==> Training loss: 2.002609    Training error rate: 54.000000
650.000000/1000.000000 ==> Training loss: 1.760525    Training error rate: 50.000000
700.000000/1000.000000 ==> Training loss: 1.627587    Training error rate: 56.000000
750.000000/1000.000000 ==> Training loss: 2.063627    Training error rate: 60.000000
800.000000/1000.000000 ==> Training loss: 1.956282    Training error rate: 60.000000
850.000000/1000.000000 ==> Training loss: 1.778941    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.968885    Training error rate: 62.000000
950.000000/1000.000000 ==> Training loss: 1.872229    Training error rate: 58.000000
==> Total training loss: 1933.528420    Total training error rate: 53.426000
==> Testing Epoch: 6
0.000000/100.000000 ==> Testing loss: 1.913968    Testing error rate: 47.000000
50.000000/100.000000 ==> Testing loss: 1.744333    Testing error rate: 45.000000
==> Total testing loss: 188.599868    Total testing error rate: 50.630000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 7
0.000000/1000.000000 ==> Training loss: 1.655617    Training error rate: 46.000000
50.000000/1000.000000 ==> Training loss: 2.058225    Training error rate: 58.000000
100.000000/1000.000000 ==> Training loss: 1.641984    Training error rate: 44.000000
150.000000/1000.000000 ==> Training loss: 1.519437    Training error rate: 48.000000
200.000000/1000.000000 ==> Training loss: 1.335865    Training error rate: 36.000000
250.000000/1000.000000 ==> Training loss: 1.371627    Training error rate: 44.000000
300.000000/1000.000000 ==> Training loss: 1.876528    Training error rate: 54.000000
350.000000/1000.000000 ==> Training loss: 1.786777    Training error rate: 46.000000
400.000000/1000.000000 ==> Training loss: 1.516261    Training error rate: 48.000000
450.000000/1000.000000 ==> Training loss: 1.786289    Training error rate: 50.000000
500.000000/1000.000000 ==> Training loss: 1.638345    Training error rate: 40.000000
550.000000/1000.000000 ==> Training loss: 1.813026    Training error rate: 48.000000
600.000000/1000.000000 ==> Training loss: 1.816995    Training error rate: 62.000000
650.000000/1000.000000 ==> Training loss: 1.644472    Training error rate: 54.000000
700.000000/1000.000000 ==> Training loss: 1.601784    Training error rate: 42.000000
750.000000/1000.000000 ==> Training loss: 1.614946    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.718328    Training error rate: 54.000000
850.000000/1000.000000 ==> Training loss: 1.503470    Training error rate: 42.000000
900.000000/1000.000000 ==> Training loss: 1.878428    Training error rate: 54.000000
950.000000/1000.000000 ==> Training loss: 1.960265    Training error rate: 50.000000
==> Total training loss: 1747.794708    Total training error rate: 48.896000
==> Testing Epoch: 7
0.000000/100.000000 ==> Testing loss: 1.839888    Testing error rate: 52.000000
50.000000/100.000000 ==> Testing loss: 1.753475    Testing error rate: 45.000000
==> Total testing loss: 182.689208    Total testing error rate: 49.470000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 8
0.000000/1000.000000 ==> Training loss: 2.101798    Training error rate: 50.000000
50.000000/1000.000000 ==> Training loss: 1.694197    Training error rate: 52.000000
100.000000/1000.000000 ==> Training loss: 1.381680    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 1.221705    Training error rate: 40.000000
200.000000/1000.000000 ==> Training loss: 1.771448    Training error rate: 56.000000
250.000000/1000.000000 ==> Training loss: 1.331762    Training error rate: 44.000000
300.000000/1000.000000 ==> Training loss: 1.735250    Training error rate: 44.000000
350.000000/1000.000000 ==> Training loss: 1.564412    Training error rate: 48.000000
400.000000/1000.000000 ==> Training loss: 1.583546    Training error rate: 48.000000
450.000000/1000.000000 ==> Training loss: 1.592473    Training error rate: 50.000000
500.000000/1000.000000 ==> Training loss: 1.602143    Training error rate: 38.000000
550.000000/1000.000000 ==> Training loss: 1.400313    Training error rate: 38.000000
600.000000/1000.000000 ==> Training loss: 1.742413    Training error rate: 48.000000
650.000000/1000.000000 ==> Training loss: 1.781681    Training error rate: 56.000000
700.000000/1000.000000 ==> Training loss: 1.934592    Training error rate: 56.000000
750.000000/1000.000000 ==> Training loss: 1.280964    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 2.093398    Training error rate: 48.000000
850.000000/1000.000000 ==> Training loss: 1.373587    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.557337    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.478017    Training error rate: 42.000000
==> Total training loss: 1607.199916    Total training error rate: 45.666000
==> Testing Epoch: 8
0.000000/100.000000 ==> Testing loss: 1.741315    Testing error rate: 43.000000
50.000000/100.000000 ==> Testing loss: 1.704606    Testing error rate: 47.000000
==> Total testing loss: 176.907435    Total testing error rate: 47.950000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 9
0.000000/1000.000000 ==> Training loss: 1.712361    Training error rate: 48.000000
50.000000/1000.000000 ==> Training loss: 1.524778    Training error rate: 38.000000
100.000000/1000.000000 ==> Training loss: 1.393590    Training error rate: 38.000000
150.000000/1000.000000 ==> Training loss: 1.836840    Training error rate: 44.000000
200.000000/1000.000000 ==> Training loss: 1.573191    Training error rate: 44.000000
250.000000/1000.000000 ==> Training loss: 1.445380    Training error rate: 40.000000
300.000000/1000.000000 ==> Training loss: 1.783751    Training error rate: 52.000000
350.000000/1000.000000 ==> Training loss: 1.687281    Training error rate: 52.000000
400.000000/1000.000000 ==> Training loss: 1.481537    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.447264    Training error rate: 38.000000
500.000000/1000.000000 ==> Training loss: 1.425301    Training error rate: 42.000000
550.000000/1000.000000 ==> Training loss: 1.473886    Training error rate: 44.000000
600.000000/1000.000000 ==> Training loss: 1.815346    Training error rate: 46.000000
650.000000/1000.000000 ==> Training loss: 1.521102    Training error rate: 38.000000
700.000000/1000.000000 ==> Training loss: 1.798823    Training error rate: 46.000000
750.000000/1000.000000 ==> Training loss: 1.337845    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.619673    Training error rate: 42.000000
850.000000/1000.000000 ==> Training loss: 1.812687    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.627762    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.633768    Training error rate: 54.000000
==> Total training loss: 1496.333617    Total training error rate: 42.558000
==> Testing Epoch: 9
0.000000/100.000000 ==> Testing loss: 1.805897    Testing error rate: 46.000000
50.000000/100.000000 ==> Testing loss: 1.769310    Testing error rate: 45.000000
==> Total testing loss: 181.217760    Total testing error rate: 48.140000
==> Set learning rate: 0.010000
==> Training Epoch: 10
0.000000/1000.000000 ==> Training loss: 1.308221    Training error rate: 42.000000
50.000000/1000.000000 ==> Training loss: 1.410322    Training error rate: 42.000000
100.000000/1000.000000 ==> Training loss: 1.523581    Training error rate: 40.000000
150.000000/1000.000000 ==> Training loss: 1.351217    Training error rate: 46.000000
200.000000/1000.000000 ==> Training loss: 1.427944    Training error rate: 42.000000
250.000000/1000.000000 ==> Training loss: 1.510982    Training error rate: 46.000000
300.000000/1000.000000 ==> Training loss: 1.980562    Training error rate: 48.000000
350.000000/1000.000000 ==> Training loss: 1.344460    Training error rate: 34.000000
400.000000/1000.000000 ==> Training loss: 1.590798    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.264358    Training error rate: 42.000000
500.000000/1000.000000 ==> Training loss: 1.392556    Training error rate: 38.000000
550.000000/1000.000000 ==> Training loss: 1.337679    Training error rate: 44.000000
600.000000/1000.000000 ==> Training loss: 1.452666    Training error rate: 44.000000
650.000000/1000.000000 ==> Training loss: 1.705950    Training error rate: 48.000000
700.000000/1000.000000 ==> Training loss: 1.303423    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 1.479489    Training error rate: 44.000000
800.000000/1000.000000 ==> Training loss: 1.039966    Training error rate: 28.000000
850.000000/1000.000000 ==> Training loss: 1.525991    Training error rate: 44.000000
900.000000/1000.000000 ==> Training loss: 1.504688    Training error rate: 48.000000
950.000000/1000.000000 ==> Training loss: 1.546789    Training error rate: 38.000000
==> Total training loss: 1395.739790    Total training error rate: 40.480000
==> Testing Epoch: 10
0.000000/100.000000 ==> Testing loss: 1.877012    Testing error rate: 49.000000
50.000000/100.000000 ==> Testing loss: 1.507011    Testing error rate: 44.000000
==> Total testing loss: 173.279470    Total testing error rate: 45.880000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 11
0.000000/1000.000000 ==> Training loss: 1.311020    Training error rate: 36.000000
50.000000/1000.000000 ==> Training loss: 1.559515    Training error rate: 48.000000
100.000000/1000.000000 ==> Training loss: 1.627589    Training error rate: 48.000000
150.000000/1000.000000 ==> Training loss: 1.203147    Training error rate: 36.000000
200.000000/1000.000000 ==> Training loss: 1.323186    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 1.649683    Training error rate: 52.000000
300.000000/1000.000000 ==> Training loss: 1.096161    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.966470    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 1.298168    Training error rate: 46.000000
450.000000/1000.000000 ==> Training loss: 1.596766    Training error rate: 44.000000
500.000000/1000.000000 ==> Training loss: 1.203436    Training error rate: 28.000000
550.000000/1000.000000 ==> Training loss: 1.087285    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 1.256607    Training error rate: 40.000000
650.000000/1000.000000 ==> Training loss: 1.551264    Training error rate: 50.000000
700.000000/1000.000000 ==> Training loss: 1.185382    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 1.462989    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.505102    Training error rate: 48.000000
850.000000/1000.000000 ==> Training loss: 1.326134    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 1.340112    Training error rate: 46.000000
950.000000/1000.000000 ==> Training loss: 1.878640    Training error rate: 60.000000
==> Total training loss: 1301.646671    Total training error rate: 37.654000
==> Testing Epoch: 11
0.000000/100.000000 ==> Testing loss: 1.623534    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.388697    Testing error rate: 37.000000
==> Total testing loss: 160.190827    Total testing error rate: 43.100000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 12
0.000000/1000.000000 ==> Training loss: 1.038240    Training error rate: 30.000000
50.000000/1000.000000 ==> Training loss: 0.933917    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 1.024173    Training error rate: 30.000000
150.000000/1000.000000 ==> Training loss: 1.391266    Training error rate: 38.000000
200.000000/1000.000000 ==> Training loss: 1.417287    Training error rate: 36.000000
250.000000/1000.000000 ==> Training loss: 1.024571    Training error rate: 34.000000
300.000000/1000.000000 ==> Training loss: 1.357576    Training error rate: 44.000000
350.000000/1000.000000 ==> Training loss: 0.931764    Training error rate: 32.000000
400.000000/1000.000000 ==> Training loss: 1.281607    Training error rate: 42.000000
450.000000/1000.000000 ==> Training loss: 1.186679    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 1.235408    Training error rate: 32.000000
550.000000/1000.000000 ==> Training loss: 1.012309    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 1.035232    Training error rate: 26.000000
650.000000/1000.000000 ==> Training loss: 1.675502    Training error rate: 48.000000
700.000000/1000.000000 ==> Training loss: 1.349720    Training error rate: 40.000000
750.000000/1000.000000 ==> Training loss: 0.894750    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 1.016231    Training error rate: 32.000000
850.000000/1000.000000 ==> Training loss: 1.287959    Training error rate: 38.000000
900.000000/1000.000000 ==> Training loss: 1.356178    Training error rate: 44.000000
950.000000/1000.000000 ==> Training loss: 1.030539    Training error rate: 34.000000
==> Total training loss: 1227.321164    Total training error rate: 35.586000
==> Testing Epoch: 12
0.000000/100.000000 ==> Testing loss: 1.326007    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.435211    Testing error rate: 40.000000
==> Total testing loss: 160.022756    Total testing error rate: 43.350000
==> Set learning rate: 0.010000
==> Training Epoch: 13
0.000000/1000.000000 ==> Training loss: 1.018544    Training error rate: 32.000000
50.000000/1000.000000 ==> Training loss: 1.258538    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 1.665256    Training error rate: 46.000000
150.000000/1000.000000 ==> Training loss: 1.054802    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.118211    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 1.119600    Training error rate: 34.000000
300.000000/1000.000000 ==> Training loss: 1.448340    Training error rate: 42.000000
350.000000/1000.000000 ==> Training loss: 1.036237    Training error rate: 40.000000
400.000000/1000.000000 ==> Training loss: 1.305989    Training error rate: 44.000000
450.000000/1000.000000 ==> Training loss: 1.126205    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 1.507743    Training error rate: 36.000000
550.000000/1000.000000 ==> Training loss: 1.176796    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 1.171439    Training error rate: 32.000000
650.000000/1000.000000 ==> Training loss: 1.072597    Training error rate: 42.000000
700.000000/1000.000000 ==> Training loss: 1.142354    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 1.275478    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.319710    Training error rate: 40.000000
850.000000/1000.000000 ==> Training loss: 1.223216    Training error rate: 40.000000
900.000000/1000.000000 ==> Training loss: 0.938066    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.087273    Training error rate: 40.000000
==> Total training loss: 1159.671479    Total training error rate: 34.186000
==> Testing Epoch: 13
0.000000/100.000000 ==> Testing loss: 1.461160    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.298147    Testing error rate: 35.000000
==> Total testing loss: 145.261071    Total testing error rate: 39.920000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 14
0.000000/1000.000000 ==> Training loss: 1.118261    Training error rate: 40.000000
50.000000/1000.000000 ==> Training loss: 1.124757    Training error rate: 40.000000
100.000000/1000.000000 ==> Training loss: 1.239783    Training error rate: 32.000000
150.000000/1000.000000 ==> Training loss: 1.187918    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 1.061010    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 0.839640    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 1.011726    Training error rate: 38.000000
350.000000/1000.000000 ==> Training loss: 1.097311    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 1.190576    Training error rate: 30.000000
450.000000/1000.000000 ==> Training loss: 0.762178    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.708490    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 1.111733    Training error rate: 34.000000
600.000000/1000.000000 ==> Training loss: 1.046133    Training error rate: 36.000000
650.000000/1000.000000 ==> Training loss: 1.255941    Training error rate: 40.000000
700.000000/1000.000000 ==> Training loss: 0.938072    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 1.017438    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 1.218119    Training error rate: 34.000000
850.000000/1000.000000 ==> Training loss: 0.997944    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 1.173154    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.205728    Training error rate: 38.000000
==> Total training loss: 1099.191433    Total training error rate: 32.438000
==> Testing Epoch: 14
0.000000/100.000000 ==> Testing loss: 1.375886    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.271874    Testing error rate: 37.000000
==> Total testing loss: 139.792277    Total testing error rate: 39.170000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 15
0.000000/1000.000000 ==> Training loss: 1.221613    Training error rate: 38.000000
50.000000/1000.000000 ==> Training loss: 0.560222    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.772762    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.710524    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.721321    Training error rate: 28.000000
250.000000/1000.000000 ==> Training loss: 1.056720    Training error rate: 30.000000
300.000000/1000.000000 ==> Training loss: 0.965970    Training error rate: 30.000000
350.000000/1000.000000 ==> Training loss: 1.331462    Training error rate: 36.000000
400.000000/1000.000000 ==> Training loss: 1.356863    Training error rate: 38.000000
450.000000/1000.000000 ==> Training loss: 1.071047    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 1.132167    Training error rate: 34.000000
550.000000/1000.000000 ==> Training loss: 1.021435    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 1.139271    Training error rate: 36.000000
650.000000/1000.000000 ==> Training loss: 0.996715    Training error rate: 32.000000
700.000000/1000.000000 ==> Training loss: 1.033458    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.971920    Training error rate: 34.000000
800.000000/1000.000000 ==> Training loss: 1.060618    Training error rate: 38.000000
850.000000/1000.000000 ==> Training loss: 1.182297    Training error rate: 40.000000
900.000000/1000.000000 ==> Training loss: 1.509563    Training error rate: 52.000000
950.000000/1000.000000 ==> Training loss: 0.985599    Training error rate: 30.000000
==> Total training loss: 1047.713929    Total training error rate: 31.170000
==> Testing Epoch: 15
0.000000/100.000000 ==> Testing loss: 1.496948    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.263095    Testing error rate: 36.000000
==> Total testing loss: 144.440971    Total testing error rate: 40.150000
==> Set learning rate: 0.010000
==> Training Epoch: 16
0.000000/1000.000000 ==> Training loss: 1.254057    Training error rate: 34.000000
50.000000/1000.000000 ==> Training loss: 0.903395    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.769630    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.850839    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 1.040989    Training error rate: 28.000000
250.000000/1000.000000 ==> Training loss: 0.819836    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.714233    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.939802    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.864940    Training error rate: 26.000000
450.000000/1000.000000 ==> Training loss: 1.144696    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.938208    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 1.097088    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 1.327735    Training error rate: 38.000000
650.000000/1000.000000 ==> Training loss: 1.081001    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 1.002182    Training error rate: 24.000000
750.000000/1000.000000 ==> Training loss: 0.763017    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 1.112584    Training error rate: 36.000000
850.000000/1000.000000 ==> Training loss: 1.136391    Training error rate: 36.000000
900.000000/1000.000000 ==> Training loss: 1.290845    Training error rate: 34.000000
950.000000/1000.000000 ==> Training loss: 0.902988    Training error rate: 26.000000
==> Total training loss: 990.005387    Total training error rate: 29.318000
==> Testing Epoch: 16
0.000000/100.000000 ==> Testing loss: 1.367854    Testing error rate: 40.000000
50.000000/100.000000 ==> Testing loss: 1.418253    Testing error rate: 33.000000
==> Total testing loss: 140.218684    Total testing error rate: 38.570000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 17
0.000000/1000.000000 ==> Training loss: 1.055174    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.886549    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 1.048889    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.783463    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.717315    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.899912    Training error rate: 28.000000
300.000000/1000.000000 ==> Training loss: 1.212397    Training error rate: 40.000000
350.000000/1000.000000 ==> Training loss: 0.936780    Training error rate: 24.000000
400.000000/1000.000000 ==> Training loss: 1.002606    Training error rate: 30.000000
450.000000/1000.000000 ==> Training loss: 0.864311    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 0.774292    Training error rate: 18.000000
550.000000/1000.000000 ==> Training loss: 1.071334    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.527526    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.781021    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 1.149153    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.701995    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 1.130402    Training error rate: 40.000000
850.000000/1000.000000 ==> Training loss: 1.261313    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 0.936168    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.922013    Training error rate: 30.000000
==> Total training loss: 948.588511    Total training error rate: 28.398000
==> Testing Epoch: 17
0.000000/100.000000 ==> Testing loss: 1.285158    Testing error rate: 41.000000
50.000000/100.000000 ==> Testing loss: 1.460227    Testing error rate: 37.000000
==> Total testing loss: 134.745520    Total testing error rate: 37.230000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 18
0.000000/1000.000000 ==> Training loss: 0.926093    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.796084    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.979270    Training error rate: 28.000000
150.000000/1000.000000 ==> Training loss: 1.031440    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 0.853587    Training error rate: 34.000000
250.000000/1000.000000 ==> Training loss: 0.909536    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.769329    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.512024    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 1.016944    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.944859    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.733042    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.744720    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 1.094880    Training error rate: 28.000000
650.000000/1000.000000 ==> Training loss: 1.038406    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 1.081145    Training error rate: 28.000000
750.000000/1000.000000 ==> Training loss: 0.731375    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 1.187885    Training error rate: 36.000000
850.000000/1000.000000 ==> Training loss: 0.683541    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.874115    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 1.047111    Training error rate: 38.000000
==> Total training loss: 904.076424    Total training error rate: 27.182000
==> Testing Epoch: 18
0.000000/100.000000 ==> Testing loss: 1.441281    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.389001    Testing error rate: 38.000000
==> Total testing loss: 137.738122    Total testing error rate: 37.410000
==> Set learning rate: 0.010000
==> Training Epoch: 19
0.000000/1000.000000 ==> Training loss: 0.773415    Training error rate: 28.000000
50.000000/1000.000000 ==> Training loss: 0.827497    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 0.632235    Training error rate: 24.000000
150.000000/1000.000000 ==> Training loss: 0.971510    Training error rate: 28.000000
200.000000/1000.000000 ==> Training loss: 0.541616    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.711499    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 1.100118    Training error rate: 46.000000
350.000000/1000.000000 ==> Training loss: 0.869232    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 0.708827    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.699850    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 1.015908    Training error rate: 28.000000
550.000000/1000.000000 ==> Training loss: 0.772177    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.835308    Training error rate: 22.000000
650.000000/1000.000000 ==> Training loss: 1.300890    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 0.876947    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 0.829903    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 0.835801    Training error rate: 32.000000
850.000000/1000.000000 ==> Training loss: 1.057488    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 0.781430    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.949653    Training error rate: 24.000000
==> Total training loss: 876.082728    Total training error rate: 26.468000
==> Testing Epoch: 19
0.000000/100.000000 ==> Testing loss: 1.321077    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.412123    Testing error rate: 34.000000
==> Total testing loss: 136.702347    Total testing error rate: 37.090000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 20
0.000000/1000.000000 ==> Training loss: 0.767655    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.823184    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 1.076606    Training error rate: 34.000000
150.000000/1000.000000 ==> Training loss: 0.674958    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.990246    Training error rate: 30.000000
250.000000/1000.000000 ==> Training loss: 0.617291    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.708325    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.889147    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.828724    Training error rate: 28.000000
450.000000/1000.000000 ==> Training loss: 0.751438    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.649178    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.726906    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.753827    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.808747    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 1.222787    Training error rate: 38.000000
750.000000/1000.000000 ==> Training loss: 0.877584    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.855438    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.967661    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.755784    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.551310    Training error rate: 8.000000
==> Total training loss: 833.038798    Total training error rate: 25.192000
==> Testing Epoch: 20
0.000000/100.000000 ==> Testing loss: 1.455191    Testing error rate: 41.000000
50.000000/100.000000 ==> Testing loss: 1.462273    Testing error rate: 39.000000
==> Total testing loss: 140.230237    Total testing error rate: 37.290000
==> Set learning rate: 0.010000
==> Training Epoch: 21
0.000000/1000.000000 ==> Training loss: 0.811763    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.613706    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.926985    Training error rate: 30.000000
150.000000/1000.000000 ==> Training loss: 0.687967    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.678507    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.755387    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.803285    Training error rate: 32.000000
350.000000/1000.000000 ==> Training loss: 0.670693    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 1.011418    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.591748    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.780168    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 0.664683    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 1.012379    Training error rate: 30.000000
650.000000/1000.000000 ==> Training loss: 0.767051    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.710810    Training error rate: 24.000000
750.000000/1000.000000 ==> Training loss: 0.566991    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.751992    Training error rate: 20.000000
850.000000/1000.000000 ==> Training loss: 0.836663    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 1.178255    Training error rate: 30.000000
950.000000/1000.000000 ==> Training loss: 0.693405    Training error rate: 24.000000
==> Total training loss: 805.103362    Total training error rate: 24.656000
==> Testing Epoch: 21
0.000000/100.000000 ==> Testing loss: 1.480474    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.286268    Testing error rate: 31.000000
==> Total testing loss: 138.010314    Total testing error rate: 36.940000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 22
0.000000/1000.000000 ==> Training loss: 0.385826    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.302237    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.778906    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.641563    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.672638    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.530859    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.903176    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.642352    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.865655    Training error rate: 32.000000
450.000000/1000.000000 ==> Training loss: 0.853225    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.812954    Training error rate: 32.000000
550.000000/1000.000000 ==> Training loss: 1.030781    Training error rate: 30.000000
600.000000/1000.000000 ==> Training loss: 0.781286    Training error rate: 24.000000
650.000000/1000.000000 ==> Training loss: 0.531625    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.764217    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.714585    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 1.226999    Training error rate: 42.000000
850.000000/1000.000000 ==> Training loss: 0.694048    Training error rate: 26.000000
900.000000/1000.000000 ==> Training loss: 0.824467    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.659504    Training error rate: 18.000000
==> Total training loss: 773.233685    Total training error rate: 23.638000
==> Testing Epoch: 22
0.000000/100.000000 ==> Testing loss: 1.505965    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.510814    Testing error rate: 33.000000
==> Total testing loss: 135.249002    Total testing error rate: 35.820000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 23
0.000000/1000.000000 ==> Training loss: 0.941841    Training error rate: 28.000000
50.000000/1000.000000 ==> Training loss: 0.799043    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.523504    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.562729    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.922137    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.944953    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.811191    Training error rate: 20.000000
350.000000/1000.000000 ==> Training loss: 0.694753    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.621766    Training error rate: 22.000000
450.000000/1000.000000 ==> Training loss: 0.710219    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.408593    Training error rate: 12.000000
550.000000/1000.000000 ==> Training loss: 0.609686    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.504516    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.899537    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 0.951560    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.618123    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.662444    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.966857    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.793802    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 1.061447    Training error rate: 30.000000
==> Total training loss: 747.542327    Total training error rate: 22.684000
==> Testing Epoch: 23
0.000000/100.000000 ==> Testing loss: 1.350777    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.434296    Testing error rate: 32.000000
==> Total testing loss: 131.407366    Total testing error rate: 34.600000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 24
0.000000/1000.000000 ==> Training loss: 0.604323    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.856048    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.563504    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.588071    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.483137    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.747936    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.987395    Training error rate: 32.000000
350.000000/1000.000000 ==> Training loss: 0.644239    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.584988    Training error rate: 22.000000
450.000000/1000.000000 ==> Training loss: 0.668496    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.608880    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.682369    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.630751    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.753236    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.949480    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.907636    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.943212    Training error rate: 30.000000
850.000000/1000.000000 ==> Training loss: 1.020969    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.845161    Training error rate: 28.000000
950.000000/1000.000000 ==> Training loss: 0.587399    Training error rate: 18.000000
==> Total training loss: 727.030024    Total training error rate: 22.458000
==> Testing Epoch: 24
0.000000/100.000000 ==> Testing loss: 1.381206    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.042953    Testing error rate: 30.000000
==> Total testing loss: 130.848066    Total testing error rate: 34.620000
==> Set learning rate: 0.010000
==> Training Epoch: 25
0.000000/1000.000000 ==> Training loss: 0.655942    Training error rate: 32.000000
50.000000/1000.000000 ==> Training loss: 0.641364    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.539858    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.518427    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.552606    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.680367    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.731961    Training error rate: 22.000000
350.000000/1000.000000 ==> Training loss: 0.944749    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 0.571494    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.505482    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.504386    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.896310    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.705431    Training error rate: 26.000000
650.000000/1000.000000 ==> Training loss: 0.792656    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.789050    Training error rate: 26.000000
750.000000/1000.000000 ==> Training loss: 0.597255    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.866821    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.579395    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.794953    Training error rate: 28.000000
950.000000/1000.000000 ==> Training loss: 0.731680    Training error rate: 24.000000
==> Total training loss: 698.406479    Total training error rate: 21.556000
==> Testing Epoch: 25
0.000000/100.000000 ==> Testing loss: 1.321187    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.090315    Testing error rate: 29.000000
==> Total testing loss: 127.379310    Total testing error rate: 34.670000
==> Set learning rate: 0.010000
==> Training Epoch: 26
0.000000/1000.000000 ==> Training loss: 0.697295    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.644665    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.567076    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.405677    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.496837    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.559085    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.541182    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.547433    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.728428    Training error rate: 20.000000
450.000000/1000.000000 ==> Training loss: 0.808685    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.722217    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.726590    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.594902    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.914522    Training error rate: 30.000000
700.000000/1000.000000 ==> Training loss: 0.881229    Training error rate: 30.000000
750.000000/1000.000000 ==> Training loss: 0.794620    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.932929    Training error rate: 34.000000
850.000000/1000.000000 ==> Training loss: 0.725854    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.634211    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.810605    Training error rate: 30.000000
==> Total training loss: 676.413966    Total training error rate: 20.674000
==> Testing Epoch: 26
0.000000/100.000000 ==> Testing loss: 1.136712    Testing error rate: 30.000000
50.000000/100.000000 ==> Testing loss: 1.198634    Testing error rate: 29.000000
==> Total testing loss: 127.802508    Total testing error rate: 33.790000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 27
0.000000/1000.000000 ==> Training loss: 0.637743    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.551723    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 0.610010    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.607348    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.570131    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.583517    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.595628    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.651267    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.467685    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.707869    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.679716    Training error rate: 26.000000
550.000000/1000.000000 ==> Training loss: 0.610795    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.791844    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.653148    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.767559    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.812972    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.642997    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.416368    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.686387    Training error rate: 26.000000
950.000000/1000.000000 ==> Training loss: 0.812223    Training error rate: 24.000000
==> Total training loss: 650.128888    Total training error rate: 20.248000
==> Testing Epoch: 27
0.000000/100.000000 ==> Testing loss: 1.193094    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.400613    Testing error rate: 34.000000
==> Total testing loss: 135.267944    Total testing error rate: 35.520000
==> Set learning rate: 0.010000
==> Training Epoch: 28
0.000000/1000.000000 ==> Training loss: 0.640792    Training error rate: 20.000000
50.000000/1000.000000 ==> Training loss: 0.577937    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.480967    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.599682    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.601168    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.877462    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 0.402524    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.676769    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.633084    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.767712    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.781782    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.754575    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.848044    Training error rate: 28.000000
650.000000/1000.000000 ==> Training loss: 0.378646    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.576601    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.640841    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.409048    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.735445    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.834494    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.691417    Training error rate: 22.000000
==> Total training loss: 641.710395    Total training error rate: 19.792000
==> Testing Epoch: 28
0.000000/100.000000 ==> Testing loss: 1.379735    Testing error rate: 33.000000
50.000000/100.000000 ==> Testing loss: 1.126057    Testing error rate: 28.000000
==> Total testing loss: 129.511713    Total testing error rate: 34.020000
==> Set learning rate: 0.010000
==> Training Epoch: 29
0.000000/1000.000000 ==> Training loss: 0.575846    Training error rate: 20.000000
50.000000/1000.000000 ==> Training loss: 0.522687    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.612530    Training error rate: 20.000000
150.000000/1000.000000 ==> Training loss: 0.484939    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.615149    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.590311    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.375446    Training error rate: 8.000000
350.000000/1000.000000 ==> Training loss: 0.530687    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.564189    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.462727    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.557025    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.858578    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.750054    Training error rate: 22.000000
650.000000/1000.000000 ==> Training loss: 0.747648    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.704276    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.467301    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.748628    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.761602    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.534158    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.657249    Training error rate: 24.000000
==> Total training loss: 616.169580    Total training error rate: 19.128000
==> Testing Epoch: 29
0.000000/100.000000 ==> Testing loss: 1.219901    Testing error rate: 31.000000
50.000000/100.000000 ==> Testing loss: 1.380697    Testing error rate: 40.000000
==> Total testing loss: 127.717714    Total testing error rate: 33.740000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 30
0.000000/1000.000000 ==> Training loss: 0.300561    Training error rate: 4.000000
50.000000/1000.000000 ==> Training loss: 0.698130    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.506481    Training error rate: 22.000000
150.000000/1000.000000 ==> Training loss: 0.419953    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.843976    Training error rate: 22.000000
250.000000/1000.000000 ==> Training loss: 0.270207    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.737075    Training error rate: 18.000000
350.000000/1000.000000 ==> Training loss: 0.652305    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.602375    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.645290    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.829702    Training error rate: 30.000000
550.000000/1000.000000 ==> Training loss: 0.546342    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.506395    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.622368    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.788734    Training error rate: 34.000000
750.000000/1000.000000 ==> Training loss: 0.638477    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.632868    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.650862    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.548122    Training error rate: 22.000000
950.000000/1000.000000 ==> Training loss: 0.687617    Training error rate: 22.000000
==> Total training loss: 610.352457    Total training error rate: 18.886000
==> Testing Epoch: 30
0.000000/100.000000 ==> Testing loss: 1.182697    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.471372    Testing error rate: 40.000000
==> Total testing loss: 133.991861    Total testing error rate: 34.240000
==> Set learning rate: 0.010000
==> Training Epoch: 31
0.000000/1000.000000 ==> Training loss: 0.667576    Training error rate: 24.000000
50.000000/1000.000000 ==> Training loss: 0.514208    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.617630    Training error rate: 26.000000
150.000000/1000.000000 ==> Training loss: 0.480765    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.610526    Training error rate: 18.000000
250.000000/1000.000000 ==> Training loss: 0.468969    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.945008    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.581384    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.363959    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.758432    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.634695    Training error rate: 22.000000
550.000000/1000.000000 ==> Training loss: 0.553093    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.556681    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.646187    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.553672    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.533663    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.602935    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.576443    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.706883    Training error rate: 30.000000
950.000000/1000.000000 ==> Training loss: 0.795737    Training error rate: 24.000000
==> Total training loss: 592.895187    Total training error rate: 18.432000
==> Testing Epoch: 31
0.000000/100.000000 ==> Testing loss: 1.279816    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.536136    Testing error rate: 35.000000
==> Total testing loss: 135.527912    Total testing error rate: 34.780000
==> Set learning rate: 0.010000
==> Training Epoch: 32
0.000000/1000.000000 ==> Training loss: 0.615069    Training error rate: 22.000000
50.000000/1000.000000 ==> Training loss: 0.669840    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.425765    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.628283    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.446802    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.461664    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.659031    Training error rate: 18.000000
350.000000/1000.000000 ==> Training loss: 0.471534    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.554291    Training error rate: 24.000000
450.000000/1000.000000 ==> Training loss: 0.821254    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 0.500932    Training error rate: 10.000000
550.000000/1000.000000 ==> Training loss: 0.622824    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.479912    Training error rate: 18.000000
650.000000/1000.000000 ==> Training loss: 0.642036    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.644528    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.835607    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.779741    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.658465    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.549923    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.571006    Training error rate: 14.000000
==> Total training loss: 578.608277    Total training error rate: 17.968000
==> Testing Epoch: 32
0.000000/100.000000 ==> Testing loss: 1.331419    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.472029    Testing error rate: 38.000000
==> Total testing loss: 133.881347    Total testing error rate: 34.920000
==> Set learning rate: 0.010000
==> Training Epoch: 33
0.000000/1000.000000 ==> Training loss: 0.376245    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.474759    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.368760    Training error rate: 8.000000
150.000000/1000.000000 ==> Training loss: 0.609937    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.443883    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.509917    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.451320    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.758068    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.477627    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.830653    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.529440    Training error rate: 20.000000
550.000000/1000.000000 ==> Training loss: 0.831604    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 0.704390    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.571816    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.589993    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.776993    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 0.440612    Training error rate: 10.000000
850.000000/1000.000000 ==> Training loss: 0.709979    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.767564    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.486793    Training error rate: 12.000000
==> Total training loss: 568.747226    Total training error rate: 17.668000
==> Testing Epoch: 33
0.000000/100.000000 ==> Testing loss: 1.496711    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.248182    Testing error rate: 35.000000
==> Total testing loss: 137.825184    Total testing error rate: 34.550000
==> Set learning rate: 0.010000
==> Training Epoch: 34
0.000000/1000.000000 ==> Training loss: 0.599273    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.453589    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.559413    Training error rate: 24.000000
150.000000/1000.000000 ==> Training loss: 0.362409    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.635144    Training error rate: 24.000000
250.000000/1000.000000 ==> Training loss: 0.385949    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.546547    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.433191    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.601622    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.451471    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.572335    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.392155    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.567442    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.609029    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.528576    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.440651    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.465503    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.741505    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.808153    Training error rate: 32.000000
950.000000/1000.000000 ==> Training loss: 0.722620    Training error rate: 24.000000
==> Total training loss: 544.274256    Total training error rate: 16.796000
==> Testing Epoch: 34
0.000000/100.000000 ==> Testing loss: 1.091064    Testing error rate: 30.000000
50.000000/100.000000 ==> Testing loss: 1.441337    Testing error rate: 34.000000
==> Total testing loss: 130.364300    Total testing error rate: 33.100000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 35
0.000000/1000.000000 ==> Training loss: 0.493398    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.517093    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.353866    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.349815    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.522168    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.778699    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.500315    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.376967    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.460305    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.368589    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.364994    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.426591    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.520659    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.488706    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.514119    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.321539    Training error rate: 8.000000
800.000000/1000.000000 ==> Training loss: 0.352800    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.747712    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.543496    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.385076    Training error rate: 18.000000
==> Total training loss: 537.894123    Total training error rate: 16.742000
==> Testing Epoch: 35
0.000000/100.000000 ==> Testing loss: 1.416957    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.298468    Testing error rate: 35.000000
==> Total testing loss: 128.735879    Total testing error rate: 33.180000
==> Set learning rate: 0.010000
==> Training Epoch: 36
0.000000/1000.000000 ==> Training loss: 0.473164    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.433537    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.467526    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.443184    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.617312    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.696765    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.382543    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.443756    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.459088    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.480450    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.516917    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.404938    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.250042    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.868545    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.427695    Training error rate: 16.000000
750.000000/1000.000000 ==> Training loss: 0.760578    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.595996    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.389359    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.581635    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.332481    Training error rate: 12.000000
==> Total training loss: 530.333477    Total training error rate: 16.518000
==> Testing Epoch: 36
0.000000/100.000000 ==> Testing loss: 1.491593    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.439892    Testing error rate: 33.000000
==> Total testing loss: 132.689520    Total testing error rate: 33.590000
==> Set learning rate: 0.010000
==> Training Epoch: 37
0.000000/1000.000000 ==> Training loss: 0.522319    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.337910    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.548351    Training error rate: 20.000000
150.000000/1000.000000 ==> Training loss: 0.572943    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.291482    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.392987    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.857531    Training error rate: 26.000000
350.000000/1000.000000 ==> Training loss: 0.416996    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.345856    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.720703    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.667278    Training error rate: 18.000000
550.000000/1000.000000 ==> Training loss: 0.518329    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.513659    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.516430    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.886918    Training error rate: 32.000000
750.000000/1000.000000 ==> Training loss: 0.589640    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.722260    Training error rate: 26.000000
850.000000/1000.000000 ==> Training loss: 0.764057    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.438010    Training error rate: 10.000000
950.000000/1000.000000 ==> Training loss: 0.519963    Training error rate: 14.000000
==> Total training loss: 514.767130    Total training error rate: 16.014000
==> Testing Epoch: 37
0.000000/100.000000 ==> Testing loss: 1.362627    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.494166    Testing error rate: 37.000000
==> Total testing loss: 135.344767    Total testing error rate: 34.270000
==> Set learning rate: 0.010000
==> Training Epoch: 38
0.000000/1000.000000 ==> Training loss: 0.390156    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.210681    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.212854    Training error rate: 6.000000
150.000000/1000.000000 ==> Training loss: 0.453237    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.567344    Training error rate: 20.000000
250.000000/1000.000000 ==> Training loss: 0.397695    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.659693    Training error rate: 24.000000
350.000000/1000.000000 ==> Training loss: 0.455969    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.388112    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.499256    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.281768    Training error rate: 10.000000
550.000000/1000.000000 ==> Training loss: 0.940469    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.354611    Training error rate: 12.000000
650.000000/1000.000000 ==> Training loss: 0.586523    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.491159    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.653063    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.430160    Training error rate: 12.000000
850.000000/1000.000000 ==> Training loss: 0.585682    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.761453    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.488902    Training error rate: 14.000000
==> Total training loss: 508.658543    Total training error rate: 15.914000
==> Testing Epoch: 38
0.000000/100.000000 ==> Testing loss: 1.669469    Testing error rate: 37.000000
50.000000/100.000000 ==> Testing loss: 1.403951    Testing error rate: 35.000000
==> Total testing loss: 144.760002    Total testing error rate: 35.390000
==> Set learning rate: 0.010000
==> Training Epoch: 39
0.000000/1000.000000 ==> Training loss: 0.713053    Training error rate: 26.000000
50.000000/1000.000000 ==> Training loss: 0.360587    Training error rate: 8.000000
100.000000/1000.000000 ==> Training loss: 0.333455    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.469013    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.373242    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.424465    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.788312    Training error rate: 28.000000
350.000000/1000.000000 ==> Training loss: 0.402207    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.570696    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.537472    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.705001    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.544030    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.502664    Training error rate: 18.000000
650.000000/1000.000000 ==> Training loss: 0.349189    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.606753    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.480229    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.416004    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.641803    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.490630    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.656467    Training error rate: 22.000000
==> Total training loss: 511.452728    Total training error rate: 16.054000
==> Testing Epoch: 39
0.000000/100.000000 ==> Testing loss: 1.325889    Testing error rate: 36.000000
50.000000/100.000000 ==> Testing loss: 1.391128    Testing error rate: 33.000000
==> Total testing loss: 133.601643    Total testing error rate: 33.750000
==> Set learning rate: 0.010000
==> Training Epoch: 40
0.000000/1000.000000 ==> Training loss: 0.716802    Training error rate: 30.000000
50.000000/1000.000000 ==> Training loss: 0.375767    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.348400    Training error rate: 10.000000
150.000000/1000.000000 ==> Training loss: 0.386557    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.490948    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.408495    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.239014    Training error rate: 6.000000
350.000000/1000.000000 ==> Training loss: 0.360666    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.521634    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.311205    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.692995    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.503899    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.571210    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.730871    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.454530    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.437251    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.784645    Training error rate: 24.000000
850.000000/1000.000000 ==> Training loss: 0.749598    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.507238    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.742778    Training error rate: 22.000000
==> Total training loss: 490.604032    Total training error rate: 15.276000
==> Testing Epoch: 40
0.000000/100.000000 ==> Testing loss: 1.388763    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.460940    Testing error rate: 34.000000
==> Total testing loss: 133.309104    Total testing error rate: 33.510000
==> Set learning rate: 0.010000
==> Training Epoch: 41
0.000000/1000.000000 ==> Training loss: 0.400794    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.552497    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.690024    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.691139    Training error rate: 26.000000
200.000000/1000.000000 ==> Training loss: 0.403494    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.454619    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.611817    Training error rate: 22.000000
350.000000/1000.000000 ==> Training loss: 0.284257    Training error rate: 8.000000
400.000000/1000.000000 ==> Training loss: 0.304677    Training error rate: 8.000000
450.000000/1000.000000 ==> Training loss: 0.488424    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.425047    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.485073    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.492987    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.392459    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.420543    Training error rate: 20.000000
750.000000/1000.000000 ==> Training loss: 0.349525    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.531872    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.396246    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.469083    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.736115    Training error rate: 22.000000
==> Total training loss: 482.457945    Total training error rate: 14.984000
==> Testing Epoch: 41
0.000000/100.000000 ==> Testing loss: 1.286527    Testing error rate: 31.000000
50.000000/100.000000 ==> Testing loss: 1.535186    Testing error rate: 41.000000
==> Total testing loss: 139.527525    Total testing error rate: 33.960000
==> Set learning rate: 0.010000
==> Training Epoch: 42
0.000000/1000.000000 ==> Training loss: 0.588413    Training error rate: 22.000000
50.000000/1000.000000 ==> Training loss: 0.583433    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.254410    Training error rate: 8.000000
150.000000/1000.000000 ==> Training loss: 0.348387    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.198521    Training error rate: 4.000000
250.000000/1000.000000 ==> Training loss: 0.424969    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.421748    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.449636    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.564013    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.470953    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.439111    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.566951    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.869370    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.212606    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.795025    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.538101    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.508715    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.432299    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.575001    Training error rate: 14.000000
950.000000/1000.000000 ==> Training loss: 0.418013    Training error rate: 14.000000
==> Total training loss: 485.551058    Total training error rate: 15.158000
==> Testing Epoch: 42
0.000000/100.000000 ==> Testing loss: 1.276446    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.411205    Testing error rate: 36.000000
==> Total testing loss: 133.642553    Total testing error rate: 33.830000
==> Set learning rate: 0.010000
==> Training Epoch: 43
0.000000/1000.000000 ==> Training loss: 0.268995    Training error rate: 6.000000
50.000000/1000.000000 ==> Training loss: 0.483741    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.404081    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.323366    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.669584    Training error rate: 22.000000
250.000000/1000.000000 ==> Training loss: 0.517139    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.528442    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.401222    Training error rate: 16.000000
400.000000/1000.000000 ==> Training loss: 0.408127    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.422027    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.469808    Training error rate: 16.000000
550.000000/1000.000000 ==> Training loss: 0.435880    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.676620    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.382778    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.430895    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.272249    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.807660    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.329221    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.588858    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.484126    Training error rate: 12.000000
==> Total training loss: 461.249712    Total training error rate: 14.458000
==> Testing Epoch: 43
0.000000/100.000000 ==> Testing loss: 1.401996    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.405297    Testing error rate: 31.000000
==> Total testing loss: 142.204903    Total testing error rate: 34.570000
==> Set learning rate: 0.010000
==> Training Epoch: 44
0.000000/1000.000000 ==> Training loss: 0.521284    Training error rate: 14.000000
50.000000/1000.000000 ==> Training loss: 0.326952    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.353533    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.498530    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.427460    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.349093    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.311323    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.337610    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.929363    Training error rate: 28.000000
450.000000/1000.000000 ==> Training loss: 0.502359    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.474299    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.644784    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.544656    Training error rate: 14.000000
650.000000/1000.000000 ==> Training loss: 0.418992    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.544744    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.677972    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.468367    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.603698    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.460167    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.414193    Training error rate: 12.000000
==> Total training loss: 460.236724    Total training error rate: 14.322000
==> Testing Epoch: 44
0.000000/100.000000 ==> Testing loss: 1.217281    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.287425    Testing error rate: 30.000000
==> Total testing loss: 136.454186    Total testing error rate: 33.370000
==> Set learning rate: 0.010000
==> Training Epoch: 45
0.000000/1000.000000 ==> Training loss: 0.430786    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.305765    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.326511    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.445151    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.351924    Training error rate: 10.000000
250.000000/1000.000000 ==> Training loss: 0.487034    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.348738    Training error rate: 12.000000
350.000000/1000.000000 ==> Training loss: 0.280281    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.374279    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.526731    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.709168    Training error rate: 24.000000
550.000000/1000.000000 ==> Training loss: 0.641037    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.673947    Training error rate: 20.000000
650.000000/1000.000000 ==> Training loss: 0.493031    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.863913    Training error rate: 36.000000
750.000000/1000.000000 ==> Training loss: 0.446785    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.953513    Training error rate: 28.000000
850.000000/1000.000000 ==> Training loss: 0.608976    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.604473    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.494909    Training error rate: 14.000000
==> Total training loss: 454.403468    Total training error rate: 14.204000
==> Testing Epoch: 45
0.000000/100.000000 ==> Testing loss: 1.304280    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.244558    Testing error rate: 33.000000
==> Total testing loss: 137.683011    Total testing error rate: 33.340000
==> Set learning rate: 0.010000
==> Training Epoch: 46
0.000000/1000.000000 ==> Training loss: 0.410263    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.319745    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.592297    Training error rate: 18.000000
150.000000/1000.000000 ==> Training loss: 0.366854    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.469393    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.320772    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.202899    Training error rate: 6.000000
350.000000/1000.000000 ==> Training loss: 0.471216    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.471814    Training error rate: 12.000000
450.000000/1000.000000 ==> Training loss: 0.523177    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.368291    Training error rate: 12.000000
550.000000/1000.000000 ==> Training loss: 0.488721    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.388250    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.337835    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.474120    Training error rate: 14.000000
750.000000/1000.000000 ==> Training loss: 0.304200    Training error rate: 8.000000
800.000000/1000.000000 ==> Training loss: 0.445077    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.430269    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.644756    Training error rate: 18.000000
950.000000/1000.000000 ==> Training loss: 0.239597    Training error rate: 8.000000
==> Total training loss: 450.199352    Total training error rate: 14.184000
==> Testing Epoch: 46
0.000000/100.000000 ==> Testing loss: 1.836052    Testing error rate: 39.000000
50.000000/100.000000 ==> Testing loss: 1.678423    Testing error rate: 36.000000
==> Total testing loss: 145.251171    Total testing error rate: 34.780000
==> Set learning rate: 0.010000
==> Training Epoch: 47
0.000000/1000.000000 ==> Training loss: 0.490157    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.312375    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.246098    Training error rate: 6.000000
150.000000/1000.000000 ==> Training loss: 0.720378    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.359708    Training error rate: 12.000000
250.000000/1000.000000 ==> Training loss: 0.395107    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.394884    Training error rate: 10.000000
350.000000/1000.000000 ==> Training loss: 0.266999    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.276269    Training error rate: 8.000000
450.000000/1000.000000 ==> Training loss: 0.691242    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.646556    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.457709    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.435227    Training error rate: 12.000000
650.000000/1000.000000 ==> Training loss: 0.313825    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.354125    Training error rate: 10.000000
750.000000/1000.000000 ==> Training loss: 0.339714    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.515155    Training error rate: 18.000000
850.000000/1000.000000 ==> Training loss: 0.802391    Training error rate: 28.000000
900.000000/1000.000000 ==> Training loss: 0.374395    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.638388    Training error rate: 18.000000
==> Total training loss: 453.612544    Total training error rate: 14.124000
==> Testing Epoch: 47
0.000000/100.000000 ==> Testing loss: 1.587674    Testing error rate: 33.000000
50.000000/100.000000 ==> Testing loss: 1.214726    Testing error rate: 35.000000
==> Total testing loss: 136.184986    Total testing error rate: 33.200000
==> Set learning rate: 0.010000
==> Training Epoch: 48
0.000000/1000.000000 ==> Training loss: 0.291185    Training error rate: 12.000000
50.000000/1000.000000 ==> Training loss: 0.547348    Training error rate: 18.000000
100.000000/1000.000000 ==> Training loss: 0.419140    Training error rate: 14.000000
150.000000/1000.000000 ==> Training loss: 0.248258    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.602171    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.250053    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.498472    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.398649    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.379074    Training error rate: 16.000000
450.000000/1000.000000 ==> Training loss: 0.513914    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.417094    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.660054    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.266843    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.336535    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.667075    Training error rate: 20.000000
750.000000/1000.000000 ==> Training loss: 0.758635    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.339449    Training error rate: 14.000000
850.000000/1000.000000 ==> Training loss: 0.261778    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.472816    Training error rate: 12.000000
950.000000/1000.000000 ==> Training loss: 0.292143    Training error rate: 4.000000
==> Total training loss: 432.183565    Total training error rate: 13.526000
==> Testing Epoch: 48
0.000000/100.000000 ==> Testing loss: 1.483737    Testing error rate: 35.000000
50.000000/100.000000 ==> Testing loss: 1.292743    Testing error rate: 30.000000
==> Total testing loss: 133.245294    Total testing error rate: 33.460000
==> Set learning rate: 0.010000
==> Training Epoch: 49
0.000000/1000.000000 ==> Training loss: 0.274462    Training error rate: 8.000000
50.000000/1000.000000 ==> Training loss: 0.320562    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.192347    Training error rate: 4.000000
150.000000/1000.000000 ==> Training loss: 0.296792    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.370274    Training error rate: 14.000000
250.000000/1000.000000 ==> Training loss: 0.291290    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.395462    Training error rate: 14.000000
350.000000/1000.000000 ==> Training loss: 0.447141    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.694239    Training error rate: 18.000000
450.000000/1000.000000 ==> Training loss: 0.391202    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.314588    Training error rate: 8.000000
550.000000/1000.000000 ==> Training loss: 0.522208    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.350614    Training error rate: 10.000000
650.000000/1000.000000 ==> Training loss: 0.629715    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.748747    Training error rate: 22.000000
750.000000/1000.000000 ==> Training loss: 0.634868    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.484708    Training error rate: 16.000000
850.000000/1000.000000 ==> Training loss: 0.231160    Training error rate: 8.000000
900.000000/1000.000000 ==> Training loss: 0.678211    Training error rate: 20.000000
950.000000/1000.000000 ==> Training loss: 0.405243    Training error rate: 12.000000
==> Total training loss: 434.063528    Total training error rate: 13.640000
==> Testing Epoch: 49
0.000000/100.000000 ==> Testing loss: 1.341824    Testing error rate: 32.000000
50.000000/100.000000 ==> Testing loss: 1.651895    Testing error rate: 37.000000
==> Total testing loss: 132.950175    Total testing error rate: 32.860000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 50
0.000000/1000.000000 ==> Training loss: 0.271199    Training error rate: 10.000000
50.000000/1000.000000 ==> Training loss: 0.368749    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.334075    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.458919    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.540437    Training error rate: 16.000000
250.000000/1000.000000 ==> Training loss: 0.333041    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.357528    Training error rate: 16.000000
350.000000/1000.000000 ==> Training loss: 0.355388    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.452689    Training error rate: 14.000000
450.000000/1000.000000 ==> Training loss: 0.391300    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.344761    Training error rate: 14.000000
550.000000/1000.000000 ==> Training loss: 0.308382    Training error rate: 6.000000
600.000000/1000.000000 ==> Training loss: 0.434797    Training error rate: 16.000000
650.000000/1000.000000 ==> Training loss: 0.527235    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.473409    Training error rate: 18.000000
750.000000/1000.000000 ==> Training loss: 0.541115    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.221035    Training error rate: 8.000000
850.000000/1000.000000 ==> Training loss: 0.317308    Training error rate: 10.000000
900.000000/1000.000000 ==> Training loss: 0.460594    Training error rate: 16.000000
950.000000/1000.000000 ==> Training loss: 0.379441    Training error rate: 12.000000
==> Total training loss: 414.474264    Total training error rate: 13.054000
==> Testing Epoch: 50
0.000000/100.000000 ==> Testing loss: 1.411966    Testing error rate: 34.000000
50.000000/100.000000 ==> Testing loss: 1.248434    Testing error rate: 29.000000
==> Total testing loss: 130.023694    Total testing error rate: 32.080000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 51
0.000000/1000.000000 ==> Training loss: 0.548065    Training error rate: 18.000000
50.000000/1000.000000 ==> Training loss: 0.530744    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.343395    Training error rate: 12.000000
150.000000/1000.000000 ==> Training loss: 0.170699    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.308693    Training error rate: 8.000000
250.000000/1000.000000 ==> Training loss: 0.087190    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.324754    Training error rate: 14.000000
350.000000/1000.000000 ==> Training loss: 0.187241    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.168844    Training error rate: 10.000000
450.000000/1000.000000 ==> Training loss: 0.186813    Training error rate: 8.000000
500.000000/1000.000000 ==> Training loss: 0.231997    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.105977    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.128095    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.162911    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.187772    Training error rate: 6.000000
750.000000/1000.000000 ==> Training loss: 0.241723    Training error rate: 10.000000
800.000000/1000.000000 ==> Training loss: 0.275930    Training error rate: 8.000000
850.000000/1000.000000 ==> Training loss: 0.246420    Training error rate: 6.000000
900.000000/1000.000000 ==> Training loss: 0.086957    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.111015    Training error rate: 2.000000
==> Total training loss: 188.944883    Total training error rate: 5.376000
==> Testing Epoch: 51
0.000000/100.000000 ==> Testing loss: 1.104177    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.076446    Testing error rate: 26.000000
==> Total testing loss: 100.681273    Total testing error rate: 25.650000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 52
0.000000/1000.000000 ==> Training loss: 0.041489    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.087955    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.106856    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.262815    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.054615    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.087994    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.076306    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.082756    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.065131    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.164704    Training error rate: 6.000000
500.000000/1000.000000 ==> Training loss: 0.093612    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.063397    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.161989    Training error rate: 6.000000
650.000000/1000.000000 ==> Training loss: 0.206366    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.144986    Training error rate: 4.000000
750.000000/1000.000000 ==> Training loss: 0.074154    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.091502    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.078383    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.158312    Training error rate: 4.000000
950.000000/1000.000000 ==> Training loss: 0.094958    Training error rate: 4.000000
==> Total training loss: 114.190603    Total training error rate: 2.926000
==> Testing Epoch: 52
0.000000/100.000000 ==> Testing loss: 1.114215    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.081510    Testing error rate: 26.000000
==> Total testing loss: 99.513879    Total testing error rate: 25.300000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 53
0.000000/1000.000000 ==> Training loss: 0.056279    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.093921    Training error rate: 4.000000
100.000000/1000.000000 ==> Training loss: 0.105373    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.177438    Training error rate: 4.000000
200.000000/1000.000000 ==> Training loss: 0.115392    Training error rate: 6.000000
250.000000/1000.000000 ==> Training loss: 0.023996    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.073734    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.022851    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.107368    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.045182    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.087432    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.057751    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.350080    Training error rate: 8.000000
650.000000/1000.000000 ==> Training loss: 0.031174    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.052472    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.086697    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.082900    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.120121    Training error rate: 6.000000
900.000000/1000.000000 ==> Training loss: 0.126391    Training error rate: 4.000000
950.000000/1000.000000 ==> Training loss: 0.106905    Training error rate: 6.000000
==> Total training loss: 90.951876    Total training error rate: 2.206000
==> Testing Epoch: 53
0.000000/100.000000 ==> Testing loss: 1.136514    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.078532    Testing error rate: 26.000000
==> Total testing loss: 98.590839    Total testing error rate: 25.240000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 54
0.000000/1000.000000 ==> Training loss: 0.113570    Training error rate: 4.000000
50.000000/1000.000000 ==> Training loss: 0.077907    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.085364    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.093794    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.048368    Training error rate: 2.000000
250.000000/1000.000000 ==> Training loss: 0.096128    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.031369    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.106214    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.044870    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.076299    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.108279    Training error rate: 2.000000
550.000000/1000.000000 ==> Training loss: 0.040064    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.039673    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.194496    Training error rate: 6.000000
700.000000/1000.000000 ==> Training loss: 0.042963    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.054935    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.045241    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.061993    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.062955    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.082277    Training error rate: 2.000000
==> Total training loss: 75.623383    Total training error rate: 1.646000
==> Testing Epoch: 54
0.000000/100.000000 ==> Testing loss: 1.154672    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.126218    Testing error rate: 25.000000
==> Total testing loss: 98.961594    Total testing error rate: 25.030000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 55
0.000000/1000.000000 ==> Training loss: 0.025742    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.078817    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.066502    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.054367    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.063125    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.050767    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.110262    Training error rate: 4.000000
350.000000/1000.000000 ==> Training loss: 0.044629    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.098108    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.044445    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.106241    Training error rate: 4.000000
550.000000/1000.000000 ==> Training loss: 0.048584    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.034466    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.038486    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.067968    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.080149    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.065024    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.097756    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.033506    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.102009    Training error rate: 2.000000
==> Total training loss: 68.041148    Total training error rate: 1.518000
==> Testing Epoch: 55
0.000000/100.000000 ==> Testing loss: 1.144079    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.141368    Testing error rate: 24.000000
==> Total testing loss: 99.233717    Total testing error rate: 24.840000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 56
0.000000/1000.000000 ==> Training loss: 0.031017    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.023896    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.062789    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.029212    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.045258    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.030282    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.014496    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.026994    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.074402    Training error rate: 4.000000
450.000000/1000.000000 ==> Training loss: 0.040946    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.048940    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.068985    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.059540    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.040985    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.037657    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.043446    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.066146    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.036403    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.074663    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.058206    Training error rate: 0.000000
==> Total training loss: 61.289695    Total training error rate: 1.348000
==> Testing Epoch: 56
0.000000/100.000000 ==> Testing loss: 1.155417    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.100886    Testing error rate: 25.000000
==> Total testing loss: 99.680632    Total testing error rate: 25.210000
==> Set learning rate: 0.001000
==> Training Epoch: 57
0.000000/1000.000000 ==> Training loss: 0.045787    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.044946    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.030281    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.097151    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.074337    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.007269    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.067873    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.095476    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.054860    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.037578    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.056823    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.036744    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.051314    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.032107    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.065067    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.042080    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.030135    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.036866    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.034153    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.032587    Training error rate: 0.000000
==> Total training loss: 55.976676    Total training error rate: 1.154000
==> Testing Epoch: 57
0.000000/100.000000 ==> Testing loss: 1.182112    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.179492    Testing error rate: 26.000000
==> Total testing loss: 99.916698    Total testing error rate: 24.790000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 58
0.000000/1000.000000 ==> Training loss: 0.061238    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.038715    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.047444    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.043027    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.011927    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.034696    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.030611    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.040962    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.068296    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.073488    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.032725    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.081250    Training error rate: 4.000000
600.000000/1000.000000 ==> Training loss: 0.036159    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.026027    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.040845    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.063127    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.048407    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.085359    Training error rate: 4.000000
900.000000/1000.000000 ==> Training loss: 0.025505    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.084730    Training error rate: 4.000000
==> Total training loss: 49.025808    Total training error rate: 0.916000
==> Testing Epoch: 58
0.000000/100.000000 ==> Testing loss: 1.220926    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.160979    Testing error rate: 26.000000
==> Total testing loss: 100.286230    Total testing error rate: 24.660000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 59
0.000000/1000.000000 ==> Training loss: 0.122963    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.032938    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.046979    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.038040    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.086270    Training error rate: 2.000000
250.000000/1000.000000 ==> Training loss: 0.045283    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.050806    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.064947    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.059310    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.042756    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.041309    Training error rate: 2.000000
550.000000/1000.000000 ==> Training loss: 0.098157    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.063187    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.041597    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.025869    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.026402    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.043598    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.040436    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.032241    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.031281    Training error rate: 0.000000
==> Total training loss: 46.151425    Total training error rate: 0.844000
==> Testing Epoch: 59
0.000000/100.000000 ==> Testing loss: 1.189765    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.124161    Testing error rate: 25.000000
==> Total testing loss: 100.316914    Total testing error rate: 24.790000
==> Set learning rate: 0.001000
==> Training Epoch: 60
0.000000/1000.000000 ==> Training loss: 0.051563    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.033214    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.026651    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.045460    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.035215    Training error rate: 2.000000
250.000000/1000.000000 ==> Training loss: 0.029832    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.058198    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.053774    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.064322    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.032760    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.024754    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.139447    Training error rate: 4.000000
600.000000/1000.000000 ==> Training loss: 0.032917    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.038835    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.051032    Training error rate: 2.000000
750.000000/1000.000000 ==> Training loss: 0.030254    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.033588    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.060741    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.022385    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.050543    Training error rate: 0.000000
==> Total training loss: 42.256131    Total training error rate: 0.752000
==> Testing Epoch: 60
0.000000/100.000000 ==> Testing loss: 1.210415    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.165082    Testing error rate: 27.000000
==> Total testing loss: 100.654579    Total testing error rate: 24.790000
==> Set learning rate: 0.001000
==> Training Epoch: 61
0.000000/1000.000000 ==> Training loss: 0.152288    Training error rate: 4.000000
50.000000/1000.000000 ==> Training loss: 0.049870    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.020357    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.137516    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.020593    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.048411    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.049537    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.038599    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.020200    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.041078    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.018251    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.063580    Training error rate: 4.000000
600.000000/1000.000000 ==> Training loss: 0.039330    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.016329    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.080361    Training error rate: 2.000000
750.000000/1000.000000 ==> Training loss: 0.053916    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.013465    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.115441    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.033145    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.058360    Training error rate: 2.000000
==> Total training loss: 39.862183    Total training error rate: 0.700000
==> Testing Epoch: 61
0.000000/100.000000 ==> Testing loss: 1.235801    Testing error rate: 29.000000
50.000000/100.000000 ==> Testing loss: 1.160040    Testing error rate: 28.000000
==> Total testing loss: 101.098889    Total testing error rate: 24.800000
==> Set learning rate: 0.001000
==> Training Epoch: 62
0.000000/1000.000000 ==> Training loss: 0.033131    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.125032    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.092090    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.028848    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.059370    Training error rate: 2.000000
250.000000/1000.000000 ==> Training loss: 0.035428    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.025853    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.042134    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.020121    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.018871    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.018592    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.036472    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.024419    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.025193    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.009486    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.047173    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.053710    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.014683    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.033697    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.043740    Training error rate: 2.000000
==> Total training loss: 37.933803    Total training error rate: 0.636000
==> Testing Epoch: 62
0.000000/100.000000 ==> Testing loss: 1.164165    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.187014    Testing error rate: 26.000000
==> Total testing loss: 101.306421    Total testing error rate: 24.640000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 63
0.000000/1000.000000 ==> Training loss: 0.027026    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.017593    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.024263    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.042189    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.017886    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.072104    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.015999    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.038031    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.011893    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.027197    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.051932    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.053901    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.040546    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.022776    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.021514    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.033193    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.075043    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.041541    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.040064    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.029831    Training error rate: 0.000000
==> Total training loss: 35.192014    Total training error rate: 0.554000
==> Testing Epoch: 63
0.000000/100.000000 ==> Testing loss: 1.177693    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.197958    Testing error rate: 27.000000
==> Total testing loss: 101.758805    Total testing error rate: 24.480000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 64
0.000000/1000.000000 ==> Training loss: 0.016574    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.018380    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.022402    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.062796    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.025406    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.049083    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.030203    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.034675    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.024030    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.056932    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.018063    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.024734    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.028154    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.027666    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.034771    Training error rate: 2.000000
750.000000/1000.000000 ==> Training loss: 0.024104    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.028615    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.036135    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.033990    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.024974    Training error rate: 0.000000
==> Total training loss: 33.521032    Total training error rate: 0.542000
==> Testing Epoch: 64
0.000000/100.000000 ==> Testing loss: 1.205277    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.217986    Testing error rate: 28.000000
==> Total testing loss: 101.869199    Total testing error rate: 24.500000
==> Set learning rate: 0.001000
==> Training Epoch: 65
0.000000/1000.000000 ==> Training loss: 0.018437    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.019011    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.046481    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.033124    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.019964    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.025600    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.030341    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.051971    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.016350    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.052768    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.018177    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.021501    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.035338    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.013747    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.023487    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.024359    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.062339    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.028680    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.024780    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.012304    Training error rate: 0.000000
==> Total training loss: 30.609411    Total training error rate: 0.442000
==> Testing Epoch: 65
0.000000/100.000000 ==> Testing loss: 1.185688    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.223515    Testing error rate: 30.000000
==> Total testing loss: 101.873830    Total testing error rate: 24.320000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 66
0.000000/1000.000000 ==> Training loss: 0.055873    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.038418    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.033582    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.024877    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.035855    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.072295    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.018445    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.023646    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.013957    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.028181    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.026254    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.041387    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.051811    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.020535    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.050584    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.021863    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.023682    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.026357    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.014343    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.010915    Training error rate: 0.000000
==> Total training loss: 30.130347    Total training error rate: 0.452000
==> Testing Epoch: 66
0.000000/100.000000 ==> Testing loss: 1.205467    Testing error rate: 26.000000
50.000000/100.000000 ==> Testing loss: 1.251519    Testing error rate: 25.000000
==> Total testing loss: 101.703182    Total testing error rate: 24.350000
==> Set learning rate: 0.001000
==> Training Epoch: 67
0.000000/1000.000000 ==> Training loss: 0.049008    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.040264    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.042256    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.013854    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.033614    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.040690    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.044038    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.011748    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.021311    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.018763    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.021211    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.047230    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.028730    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.057314    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.011536    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.006783    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.030198    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.034848    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.036116    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.022514    Training error rate: 0.000000
==> Total training loss: 29.097517    Total training error rate: 0.406000
==> Testing Epoch: 67
0.000000/100.000000 ==> Testing loss: 1.210401    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.237430    Testing error rate: 29.000000
==> Total testing loss: 102.342330    Total testing error rate: 24.460000
==> Set learning rate: 0.001000
==> Training Epoch: 68
0.000000/1000.000000 ==> Training loss: 0.013354    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.068025    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.021037    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.010109    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.025825    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.015216    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.019874    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.047514    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.042322    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.007242    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.024989    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.040505    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.007077    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.022593    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.028042    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.013764    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.013706    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.020621    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.014608    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.079300    Training error rate: 2.000000
==> Total training loss: 26.101041    Total training error rate: 0.362000
==> Testing Epoch: 68
0.000000/100.000000 ==> Testing loss: 1.176586    Testing error rate: 26.000000
50.000000/100.000000 ==> Testing loss: 1.182142    Testing error rate: 26.000000
==> Total testing loss: 101.416560    Total testing error rate: 24.280000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 69
0.000000/1000.000000 ==> Training loss: 0.014678    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.014513    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.053124    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.018736    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008788    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.010875    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.023614    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.018233    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.014218    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.021040    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.022077    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.014672    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.009918    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.014787    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.017516    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.034932    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.017597    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.035185    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.038151    Training error rate: 2.000000
950.000000/1000.000000 ==> Training loss: 0.029474    Training error rate: 0.000000
==> Total training loss: 26.057450    Total training error rate: 0.342000
==> Testing Epoch: 69
0.000000/100.000000 ==> Testing loss: 1.181604    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.233222    Testing error rate: 27.000000
==> Total testing loss: 101.247501    Total testing error rate: 24.160000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 70
0.000000/1000.000000 ==> Training loss: 0.016828    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.033698    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.027403    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.017679    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.016905    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.009618    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.012281    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.019341    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.041281    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.063372    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.018729    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.010592    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.008092    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.010307    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.018570    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.007691    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.038023    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.015351    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.017745    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.030510    Training error rate: 2.000000
==> Total training loss: 25.736188    Total training error rate: 0.356000
==> Testing Epoch: 70
0.000000/100.000000 ==> Testing loss: 1.241227    Testing error rate: 26.000000
50.000000/100.000000 ==> Testing loss: 1.247756    Testing error rate: 27.000000
==> Total testing loss: 102.162730    Total testing error rate: 24.200000
==> Set learning rate: 0.001000
==> Training Epoch: 71
0.000000/1000.000000 ==> Training loss: 0.012173    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.030201    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.012935    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.016761    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.019812    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.008594    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.025701    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.012639    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.013519    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.024574    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.025991    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.021114    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.047619    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.016307    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.053168    Training error rate: 2.000000
750.000000/1000.000000 ==> Training loss: 0.022695    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.015159    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.019314    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.018070    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.026527    Training error rate: 2.000000
==> Total training loss: 23.851401    Total training error rate: 0.338000
==> Testing Epoch: 71
0.000000/100.000000 ==> Testing loss: 1.203603    Testing error rate: 26.000000
50.000000/100.000000 ==> Testing loss: 1.250160    Testing error rate: 29.000000
==> Total testing loss: 101.431585    Total testing error rate: 24.440000
==> Set learning rate: 0.001000
==> Training Epoch: 72
0.000000/1000.000000 ==> Training loss: 0.029284    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.017241    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.022912    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.018992    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.010160    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.017792    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.020883    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.054311    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.012192    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.013421    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.009440    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.014535    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.031545    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.016644    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.027681    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.114758    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.021065    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.015857    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.017526    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.015770    Training error rate: 0.000000
==> Total training loss: 23.062252    Total training error rate: 0.280000
==> Testing Epoch: 72
0.000000/100.000000 ==> Testing loss: 1.256639    Testing error rate: 26.000000
50.000000/100.000000 ==> Testing loss: 1.237049    Testing error rate: 28.000000
==> Total testing loss: 102.122860    Total testing error rate: 24.410000
==> Set learning rate: 0.001000
==> Training Epoch: 73
0.000000/1000.000000 ==> Training loss: 0.021960    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.028998    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.022879    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.061310    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.033629    Training error rate: 2.000000
250.000000/1000.000000 ==> Training loss: 0.016072    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.012368    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.035349    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.040909    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.016386    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.014103    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.017839    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013114    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.023263    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.050636    Training error rate: 2.000000
750.000000/1000.000000 ==> Training loss: 0.009300    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.036760    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.016795    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.016901    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.025788    Training error rate: 0.000000
==> Total training loss: 21.547242    Total training error rate: 0.248000
==> Testing Epoch: 73
0.000000/100.000000 ==> Testing loss: 1.198441    Testing error rate: 27.000000
50.000000/100.000000 ==> Testing loss: 1.247458    Testing error rate: 28.000000
==> Total testing loss: 102.064361    Total testing error rate: 24.170000
==> Set learning rate: 0.001000
==> Training Epoch: 74
0.000000/1000.000000 ==> Training loss: 0.008925    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.013846    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.009904    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.013893    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.009178    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.010295    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.007831    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.007763    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.007458    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.023795    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.015106    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.017395    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.031050    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.015059    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.050799    Training error rate: 2.000000
750.000000/1000.000000 ==> Training loss: 0.010559    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.005639    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.015161    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.019804    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.026784    Training error rate: 0.000000
==> Total training loss: 21.520054    Total training error rate: 0.280000
==> Testing Epoch: 74
0.000000/100.000000 ==> Testing loss: 1.207772    Testing error rate: 26.000000
50.000000/100.000000 ==> Testing loss: 1.253980    Testing error rate: 26.000000
==> Total testing loss: 101.675899    Total testing error rate: 24.090000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 75
0.000000/1000.000000 ==> Training loss: 0.017695    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.013449    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.041228    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.015800    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013388    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.012094    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.028145    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.015689    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.025414    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.012359    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.013992    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.019293    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.011600    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.020408    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.016277    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.008799    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.034270    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.019555    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.015719    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.026339    Training error rate: 0.000000
==> Total training loss: 20.650964    Total training error rate: 0.232000
==> Testing Epoch: 75
0.000000/100.000000 ==> Testing loss: 1.283736    Testing error rate: 28.000000
50.000000/100.000000 ==> Testing loss: 1.263651    Testing error rate: 28.000000
==> Total testing loss: 102.642572    Total testing error rate: 24.460000
==> Set learning rate: 0.001000
==> Training Epoch: 76
0.000000/1000.000000 ==> Training loss: 0.049088    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.010103    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.017129    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.015812    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.014173    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.022966    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.030555    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.007610    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.028604    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.017164    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.009222    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.027223    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.026461    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.016394    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.020666    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.081401    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.047018    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.020102    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.015903    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.051547    Training error rate: 2.000000
==> Total training loss: 20.621523    Total training error rate: 0.250000
==> Testing Epoch: 76
0.000000/100.000000 ==> Testing loss: 1.232529    Testing error rate: 26.000000
50.000000/100.000000 ==> Testing loss: 1.254336    Testing error rate: 28.000000
==> Total testing loss: 102.359755    Total testing error rate: 24.290000
==> Set learning rate: 0.001000
==> Training Epoch: 77
0.000000/1000.000000 ==> Training loss: 0.006460    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.010568    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.016784    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.034454    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013009    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.013758    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.021178    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.036387    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.014220    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.010109    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.014821    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.020919    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.019032    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.022805    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.004832    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.008787    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.004725    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.017695    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.009024    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.011445    Training error rate: 0.000000
==> Total training loss: 19.410701    Total training error rate: 0.198000
==> Testing Epoch: 77
0.000000/100.000000 ==> Testing loss: 1.215770    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.262897    Testing error rate: 28.000000
==> Total testing loss: 101.569332    Total testing error rate: 24.200000
==> Set learning rate: 0.001000
==> Training Epoch: 78
0.000000/1000.000000 ==> Training loss: 0.013421    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.007267    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.035716    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.007661    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.024312    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.014885    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.019195    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.060634    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.014179    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.008499    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.004152    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.007219    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.069008    Training error rate: 2.000000
650.000000/1000.000000 ==> Training loss: 0.016608    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.017559    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.016522    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.011114    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.035834    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.029192    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.044218    Training error rate: 2.000000
==> Total training loss: 19.097789    Total training error rate: 0.230000
==> Testing Epoch: 78
0.000000/100.000000 ==> Testing loss: 1.251593    Testing error rate: 25.000000
50.000000/100.000000 ==> Testing loss: 1.294089    Testing error rate: 28.000000
==> Total testing loss: 102.507854    Total testing error rate: 24.250000
==> Set learning rate: 0.001000
==> Training Epoch: 79
0.000000/1000.000000 ==> Training loss: 0.012681    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.017384    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.014281    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.070366    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.016631    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.012741    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.011277    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.008492    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.017568    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.010342    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.014663    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.010969    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.014524    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.010126    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.006262    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.060792    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.031335    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.021956    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.016905    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.009866    Training error rate: 0.000000
==> Total training loss: 18.595108    Total training error rate: 0.208000
==> Testing Epoch: 79
0.000000/100.000000 ==> Testing loss: 1.208625    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.254350    Testing error rate: 29.000000
==> Total testing loss: 102.255742    Total testing error rate: 24.130000
==> Set learning rate: 0.001000
==> Training Epoch: 80
0.000000/1000.000000 ==> Training loss: 0.005213    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.005444    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.011904    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.025671    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013528    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.011633    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.010590    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.022597    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.011867    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.007374    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.016862    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.020066    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013419    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.010249    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.016246    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.016576    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.029433    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.015473    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.025665    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.017238    Training error rate: 0.000000
==> Total training loss: 18.316771    Total training error rate: 0.182000
==> Testing Epoch: 80
0.000000/100.000000 ==> Testing loss: 1.179873    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.276125    Testing error rate: 29.000000
==> Total testing loss: 102.370256    Total testing error rate: 24.280000
==> Set learning rate: 0.001000
==> Training Epoch: 81
0.000000/1000.000000 ==> Training loss: 0.019771    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.014536    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.016870    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.014383    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.052900    Training error rate: 2.000000
250.000000/1000.000000 ==> Training loss: 0.011049    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.011657    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.010832    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.023573    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.017480    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.007068    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.011781    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.018165    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.038068    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.033159    Training error rate: 2.000000
750.000000/1000.000000 ==> Training loss: 0.010440    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010284    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.036586    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.004492    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.008703    Training error rate: 0.000000
==> Total training loss: 18.429360    Total training error rate: 0.208000
==> Testing Epoch: 81
0.000000/100.000000 ==> Testing loss: 1.208149    Testing error rate: 25.000000
50.000000/100.000000 ==> Testing loss: 1.212409    Testing error rate: 27.000000
==> Total testing loss: 102.303331    Total testing error rate: 24.080000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 82
0.000000/1000.000000 ==> Training loss: 0.016119    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.017095    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.021358    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.007182    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008308    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.005748    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.006212    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.031003    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.019162    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.011059    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.017076    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.014037    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.006672    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.025621    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.016062    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.009700    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.025479    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.016562    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.016701    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.012486    Training error rate: 0.000000
==> Total training loss: 17.016347    Total training error rate: 0.178000
==> Testing Epoch: 82
0.000000/100.000000 ==> Testing loss: 1.251171    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.248287    Testing error rate: 29.000000
==> Total testing loss: 102.684331    Total testing error rate: 24.200000
==> Set learning rate: 0.001000
==> Training Epoch: 83
0.000000/1000.000000 ==> Training loss: 0.023355    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.009119    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.016125    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.005254    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.009290    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.012769    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.010336    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.022994    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.014292    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.014459    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011132    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.016493    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.015067    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.024130    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.022601    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.008845    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.051994    Training error rate: 2.000000
850.000000/1000.000000 ==> Training loss: 0.008179    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.010614    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.040759    Training error rate: 2.000000
==> Total training loss: 17.616428    Total training error rate: 0.200000
==> Testing Epoch: 83
0.000000/100.000000 ==> Testing loss: 1.214740    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.242054    Testing error rate: 29.000000
==> Total testing loss: 101.993431    Total testing error rate: 23.880000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 84
0.000000/1000.000000 ==> Training loss: 0.012242    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.010153    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.014902    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.005494    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.009757    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.012031    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.019445    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.008830    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.017012    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.041249    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011131    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.017227    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.010835    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.044274    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.008603    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.010806    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.012822    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.007079    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.019454    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.014417    Training error rate: 0.000000
==> Total training loss: 17.250628    Total training error rate: 0.180000
==> Testing Epoch: 84
0.000000/100.000000 ==> Testing loss: 1.187362    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.229976    Testing error rate: 29.000000
==> Total testing loss: 102.343271    Total testing error rate: 23.850000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 85
0.000000/1000.000000 ==> Training loss: 0.005706    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.025251    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.009109    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.007041    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.025428    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.008597    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.021801    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.005775    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.012147    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.054405    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.027596    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.011108    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.015307    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.014827    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.019739    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.014480    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.013341    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.008105    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.010842    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.016157    Training error rate: 0.000000
==> Total training loss: 16.462936    Total training error rate: 0.154000
==> Testing Epoch: 85
0.000000/100.000000 ==> Testing loss: 1.191247    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.249005    Testing error rate: 28.000000
==> Total testing loss: 102.425579    Total testing error rate: 24.060000
==> Set learning rate: 0.001000
==> Training Epoch: 86
0.000000/1000.000000 ==> Training loss: 0.015426    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.015736    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.009145    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.015813    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.020582    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.013016    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.026391    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.010407    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.023571    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.008420    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.013849    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.033602    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.005437    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.005460    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.007692    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.033479    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.005955    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.023637    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.009100    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.024138    Training error rate: 0.000000
==> Total training loss: 15.435163    Total training error rate: 0.152000
==> Testing Epoch: 86
0.000000/100.000000 ==> Testing loss: 1.202342    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.222927    Testing error rate: 30.000000
==> Total testing loss: 101.783019    Total testing error rate: 23.940000
==> Set learning rate: 0.001000
==> Training Epoch: 87
0.000000/1000.000000 ==> Training loss: 0.011194    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.008874    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.007951    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.013055    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.006954    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.006364    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.005823    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.014277    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.008238    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.006681    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.022444    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.021570    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013796    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.034384    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.015326    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.022995    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.007184    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.013509    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.009611    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.011901    Training error rate: 0.000000
==> Total training loss: 15.609976    Total training error rate: 0.148000
==> Testing Epoch: 87
0.000000/100.000000 ==> Testing loss: 1.241102    Testing error rate: 25.000000
50.000000/100.000000 ==> Testing loss: 1.227072    Testing error rate: 29.000000
==> Total testing loss: 101.663796    Total testing error rate: 23.880000
==> Set learning rate: 0.001000
==> Training Epoch: 88
0.000000/1000.000000 ==> Training loss: 0.009125    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.036404    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.047376    Training error rate: 2.000000
150.000000/1000.000000 ==> Training loss: 0.016709    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.017413    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.007012    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.030084    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.015770    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.020565    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.019070    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.013073    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.024327    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.005497    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.020541    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.009529    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.035893    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.011971    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.005270    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.017050    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.014915    Training error rate: 0.000000
==> Total training loss: 16.294522    Total training error rate: 0.168000
==> Testing Epoch: 88
0.000000/100.000000 ==> Testing loss: 1.237201    Testing error rate: 25.000000
50.000000/100.000000 ==> Testing loss: 1.279938    Testing error rate: 30.000000
==> Total testing loss: 102.077657    Total testing error rate: 23.690000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 89
0.000000/1000.000000 ==> Training loss: 0.006199    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.014527    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.011997    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.017826    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008893    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.008748    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.053050    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.017971    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.004872    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.010025    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011873    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.023558    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.014197    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.038856    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.012407    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.008375    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.014755    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.015497    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.021832    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.006810    Training error rate: 0.000000
==> Total training loss: 14.629162    Total training error rate: 0.122000
==> Testing Epoch: 89
0.000000/100.000000 ==> Testing loss: 1.220368    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.266531    Testing error rate: 30.000000
==> Total testing loss: 102.284296    Total testing error rate: 23.970000
==> Set learning rate: 0.001000
==> Training Epoch: 90
0.000000/1000.000000 ==> Training loss: 0.018933    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.013191    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.012374    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.014217    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008396    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.022135    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.067312    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.015190    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.023977    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.011018    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.006734    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.011094    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.010978    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.013849    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.005633    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.019796    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.011534    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.005001    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.006208    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.015094    Training error rate: 0.000000
==> Total training loss: 15.074726    Total training error rate: 0.140000
==> Testing Epoch: 90
0.000000/100.000000 ==> Testing loss: 1.207675    Testing error rate: 26.000000
50.000000/100.000000 ==> Testing loss: 1.282072    Testing error rate: 30.000000
==> Total testing loss: 102.219547    Total testing error rate: 23.880000
==> Set learning rate: 0.001000
==> Training Epoch: 91
0.000000/1000.000000 ==> Training loss: 0.005924    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.015724    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.005850    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.008696    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013750    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.010656    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.014309    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.022030    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.008761    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.013762    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.014166    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.012798    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.015551    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.020382    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.015373    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.021790    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.007941    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.010721    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.011768    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.020735    Training error rate: 0.000000
==> Total training loss: 13.971862    Total training error rate: 0.126000
==> Testing Epoch: 91
0.000000/100.000000 ==> Testing loss: 1.218572    Testing error rate: 25.000000
50.000000/100.000000 ==> Testing loss: 1.314112    Testing error rate: 32.000000
==> Total testing loss: 101.956739    Total testing error rate: 23.760000
==> Set learning rate: 0.001000
==> Training Epoch: 92
0.000000/1000.000000 ==> Training loss: 0.011093    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.011805    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.017838    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.008001    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.017389    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.017007    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.035517    Training error rate: 2.000000
350.000000/1000.000000 ==> Training loss: 0.005503    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.010022    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.011556    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.009782    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.013359    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.014505    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.008023    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.009104    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.006315    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.019480    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.011772    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.016611    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.016282    Training error rate: 0.000000
==> Total training loss: 14.573593    Total training error rate: 0.138000
==> Testing Epoch: 92
0.000000/100.000000 ==> Testing loss: 1.265851    Testing error rate: 22.000000
50.000000/100.000000 ==> Testing loss: 1.316101    Testing error rate: 30.000000
==> Total testing loss: 101.190391    Total testing error rate: 23.690000
==> Set learning rate: 0.001000
==> Training Epoch: 93
0.000000/1000.000000 ==> Training loss: 0.008345    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.019590    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.012240    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.015086    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013289    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.012420    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.015144    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.023306    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.014070    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.008449    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.015273    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.013776    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.011203    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.010738    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.017536    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.005443    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.015302    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.022160    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.016223    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.006225    Training error rate: 0.000000
==> Total training loss: 14.217756    Total training error rate: 0.126000
==> Testing Epoch: 93
0.000000/100.000000 ==> Testing loss: 1.281945    Testing error rate: 25.000000
50.000000/100.000000 ==> Testing loss: 1.283820    Testing error rate: 29.000000
==> Total testing loss: 102.146576    Total testing error rate: 23.820000
==> Set learning rate: 0.001000
==> Training Epoch: 94
0.000000/1000.000000 ==> Training loss: 0.020809    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.013066    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.016590    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.005411    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.018853    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.021245    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.010933    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.008805    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.011394    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.009859    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.007844    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.006355    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.009476    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.017880    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.007136    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.007856    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.009820    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.005489    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.009424    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.017505    Training error rate: 0.000000
==> Total training loss: 14.399251    Total training error rate: 0.132000
==> Testing Epoch: 94
0.000000/100.000000 ==> Testing loss: 1.266931    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.268298    Testing error rate: 27.000000
==> Total testing loss: 101.956541    Total testing error rate: 23.710000
==> Set learning rate: 0.001000
==> Training Epoch: 95
0.000000/1000.000000 ==> Training loss: 0.006414    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.005253    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.020458    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.098202    Training error rate: 4.000000
200.000000/1000.000000 ==> Training loss: 0.015319    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.017829    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.038183    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.008612    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.012386    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.012500    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.010296    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.009645    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.006906    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.004956    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.017721    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.010763    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.018114    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.038393    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.016357    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.011082    Training error rate: 0.000000
==> Total training loss: 13.562856    Total training error rate: 0.110000
==> Testing Epoch: 95
0.000000/100.000000 ==> Testing loss: 1.251188    Testing error rate: 26.000000
50.000000/100.000000 ==> Testing loss: 1.305976    Testing error rate: 30.000000
==> Total testing loss: 101.242952    Total testing error rate: 23.950000
==> Set learning rate: 0.001000
==> Training Epoch: 96
0.000000/1000.000000 ==> Training loss: 0.014103    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.019303    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.004206    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.012084    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.019110    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.015044    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.012022    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.008231    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.016427    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.009925    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.003407    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.013165    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.009239    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.011010    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.010941    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.010264    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.033212    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.005693    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.012360    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.013938    Training error rate: 0.000000
==> Total training loss: 13.450479    Total training error rate: 0.092000
==> Testing Epoch: 96
0.000000/100.000000 ==> Testing loss: 1.203048    Testing error rate: 23.000000
50.000000/100.000000 ==> Testing loss: 1.256448    Testing error rate: 29.000000
==> Total testing loss: 102.424888    Total testing error rate: 24.030000
==> Set learning rate: 0.001000
==> Training Epoch: 97
0.000000/1000.000000 ==> Training loss: 0.021398    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.018139    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.004205    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.006140    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.003696    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.012957    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.016045    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.007853    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.014182    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.030143    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.018749    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.012439    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.011548    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.018102    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.019469    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.014751    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.019417    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.009547    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.018210    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.009881    Training error rate: 0.000000
==> Total training loss: 13.824074    Total training error rate: 0.120000
==> Testing Epoch: 97
0.000000/100.000000 ==> Testing loss: 1.221661    Testing error rate: 25.000000
50.000000/100.000000 ==> Testing loss: 1.275608    Testing error rate: 27.000000
==> Total testing loss: 101.764381    Total testing error rate: 23.660000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 98
0.000000/1000.000000 ==> Training loss: 0.120907    Training error rate: 2.000000
50.000000/1000.000000 ==> Training loss: 0.005251    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.006850    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.005447    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.009913    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.013541    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.009713    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.008343    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.018088    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.004267    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.014132    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.005672    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.004201    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.011032    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.012695    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.011253    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.008276    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.006075    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.008828    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.007288    Training error rate: 0.000000
==> Total training loss: 13.606645    Total training error rate: 0.100000
==> Testing Epoch: 98
0.000000/100.000000 ==> Testing loss: 1.198871    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.303437    Testing error rate: 29.000000
==> Total testing loss: 101.798342    Total testing error rate: 23.680000
==> Set learning rate: 0.001000
==> Training Epoch: 99
0.000000/1000.000000 ==> Training loss: 0.015185    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.007381    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.017053    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.023795    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008287    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.006451    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.023692    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.003040    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.043417    Training error rate: 2.000000
450.000000/1000.000000 ==> Training loss: 0.010392    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.004755    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.008744    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.003408    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.006321    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.006331    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.008789    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010490    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.006468    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.017269    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.013660    Training error rate: 0.000000
==> Total training loss: 13.323597    Total training error rate: 0.100000
==> Testing Epoch: 99
0.000000/100.000000 ==> Testing loss: 1.202179    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.314480    Testing error rate: 29.000000
==> Total testing loss: 101.584855    Total testing error rate: 23.470000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 100
0.000000/1000.000000 ==> Training loss: 0.011847    Training error rate: 0.000000
50.000000/1000.000000 ==> Training loss: 0.005271    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.007950    Training error rate: 0.000000
150.000000/1000.000000 ==> Training loss: 0.005751    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.004230    Training error rate: 0.000000
250.000000/1000.000000 ==> Training loss: 0.009400    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.017933    Training error rate: 0.000000
350.000000/1000.000000 ==> Training loss: 0.010661    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.008703    Training error rate: 0.000000
450.000000/1000.000000 ==> Training loss: 0.005960    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011134    Training error rate: 0.000000
550.000000/1000.000000 ==> Training loss: 0.015162    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.008405    Training error rate: 0.000000
650.000000/1000.000000 ==> Training loss: 0.004174    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.006747    Training error rate: 0.000000
750.000000/1000.000000 ==> Training loss: 0.009715    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.014399    Training error rate: 0.000000
850.000000/1000.000000 ==> Training loss: 0.009419    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.013686    Training error rate: 0.000000
950.000000/1000.000000 ==> Training loss: 0.022346    Training error rate: 0.000000
==> Total training loss: 13.121233    Total training error rate: 0.100000
==> Testing Epoch: 100
0.000000/100.000000 ==> Testing loss: 1.250135    Testing error rate: 24.000000
50.000000/100.000000 ==> Testing loss: 1.278355    Testing error rate: 31.000000
==> Total testing loss: 102.146450    Total testing error rate: 23.850000
