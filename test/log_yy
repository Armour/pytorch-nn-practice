==> Init variables..
==> Init seed..
==> Download data..
Files already downloaded and verified
==> Calculate mean and std..
==> Prepare training transform..
==> Prepare testing transform..
==> Init dataloader..
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
==> Set learning rate: 0.010000
==> Training Epoch: 1
0.000000/1000.000000 ==> Training loss: 4.813872    Training error rate: 100.000000
100.000000/1000.000000 ==> Training loss: 4.421768    Training error rate: 98.000000
200.000000/1000.000000 ==> Training loss: 4.235881    Training error rate: 96.000000
300.000000/1000.000000 ==> Training loss: 4.066106    Training error rate: 98.000000
400.000000/1000.000000 ==> Training loss: 3.913899    Training error rate: 94.000000
500.000000/1000.000000 ==> Training loss: 3.896514    Training error rate: 84.000000
600.000000/1000.000000 ==> Training loss: 3.955308    Training error rate: 84.000000
700.000000/1000.000000 ==> Training loss: 3.878551    Training error rate: 90.000000
800.000000/1000.000000 ==> Training loss: 3.761478    Training error rate: 80.000000
900.000000/1000.000000 ==> Training loss: 3.757973    Training error rate: 90.000000
==> Total training loss: 4053.957822    Total training error rate: 92.086000
==> Testing Epoch: 1
0.000000/100.000000 ==> Testing loss: 4.491995    Testing error rate: 94.000000
==> Total testing loss: 434.051548    Total testing error rate: 91.460000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 2
0.000000/1000.000000 ==> Training loss: 3.641360    Training error rate: 80.000000
100.000000/1000.000000 ==> Training loss: 3.302987    Training error rate: 78.000000
200.000000/1000.000000 ==> Training loss: 3.649400    Training error rate: 86.000000
300.000000/1000.000000 ==> Training loss: 3.611736    Training error rate: 82.000000
400.000000/1000.000000 ==> Training loss: 3.290858    Training error rate: 78.000000
500.000000/1000.000000 ==> Training loss: 3.503504    Training error rate: 86.000000
600.000000/1000.000000 ==> Training loss: 3.191789    Training error rate: 80.000000
700.000000/1000.000000 ==> Training loss: 3.228693    Training error rate: 88.000000
800.000000/1000.000000 ==> Training loss: 3.152416    Training error rate: 78.000000
900.000000/1000.000000 ==> Training loss: 2.910923    Training error rate: 70.000000
==> Total training loss: 3338.662097    Total training error rate: 81.596000
==> Testing Epoch: 2
0.000000/100.000000 ==> Testing loss: 3.929239    Testing error rate: 89.000000
==> Total testing loss: 391.850916    Total testing error rate: 85.740000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 3
0.000000/1000.000000 ==> Training loss: 2.948991    Training error rate: 76.000000
100.000000/1000.000000 ==> Training loss: 3.322435    Training error rate: 84.000000
200.000000/1000.000000 ==> Training loss: 3.300275    Training error rate: 76.000000
300.000000/1000.000000 ==> Training loss: 2.893930    Training error rate: 68.000000
400.000000/1000.000000 ==> Training loss: 3.358602    Training error rate: 78.000000
500.000000/1000.000000 ==> Training loss: 2.984750    Training error rate: 72.000000
600.000000/1000.000000 ==> Training loss: 3.068852    Training error rate: 80.000000
700.000000/1000.000000 ==> Training loss: 2.338546    Training error rate: 66.000000
800.000000/1000.000000 ==> Training loss: 2.430519    Training error rate: 72.000000
900.000000/1000.000000 ==> Training loss: 2.579612    Training error rate: 76.000000
==> Total training loss: 2795.437782    Total training error rate: 71.596000
==> Testing Epoch: 3
0.000000/100.000000 ==> Testing loss: 3.476290    Testing error rate: 79.000000
==> Total testing loss: 343.711299    Total testing error rate: 77.090000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 4
0.000000/1000.000000 ==> Training loss: 2.010599    Training error rate: 64.000000
100.000000/1000.000000 ==> Training loss: 2.337628    Training error rate: 62.000000
200.000000/1000.000000 ==> Training loss: 2.561248    Training error rate: 66.000000
300.000000/1000.000000 ==> Training loss: 2.320358    Training error rate: 66.000000
400.000000/1000.000000 ==> Training loss: 2.135101    Training error rate: 60.000000
500.000000/1000.000000 ==> Training loss: 2.564319    Training error rate: 66.000000
600.000000/1000.000000 ==> Training loss: 2.787740    Training error rate: 72.000000
700.000000/1000.000000 ==> Training loss: 2.311452    Training error rate: 60.000000
800.000000/1000.000000 ==> Training loss: 2.281356    Training error rate: 66.000000
900.000000/1000.000000 ==> Training loss: 2.423172    Training error rate: 70.000000
==> Total training loss: 2384.838604    Total training error rate: 63.028000
==> Testing Epoch: 4
0.000000/100.000000 ==> Testing loss: 3.071010    Testing error rate: 70.000000
==> Total testing loss: 296.645567    Total testing error rate: 70.310000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 5
0.000000/1000.000000 ==> Training loss: 2.314822    Training error rate: 64.000000
100.000000/1000.000000 ==> Training loss: 1.523326    Training error rate: 42.000000
200.000000/1000.000000 ==> Training loss: 2.338969    Training error rate: 64.000000
300.000000/1000.000000 ==> Training loss: 2.355378    Training error rate: 56.000000
400.000000/1000.000000 ==> Training loss: 2.024438    Training error rate: 52.000000
500.000000/1000.000000 ==> Training loss: 1.927671    Training error rate: 52.000000
600.000000/1000.000000 ==> Training loss: 1.788621    Training error rate: 48.000000
700.000000/1000.000000 ==> Training loss: 1.968306    Training error rate: 56.000000
800.000000/1000.000000 ==> Training loss: 2.425253    Training error rate: 62.000000
900.000000/1000.000000 ==> Training loss: 1.918532    Training error rate: 50.000000
==> Total training loss: 2090.654976    Total training error rate: 56.786000
==> Testing Epoch: 5
0.000000/100.000000 ==> Testing loss: 2.867070    Testing error rate: 61.000000
==> Total testing loss: 268.951139    Total testing error rate: 63.920000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 6
0.000000/1000.000000 ==> Training loss: 2.071005    Training error rate: 46.000000
100.000000/1000.000000 ==> Training loss: 1.667684    Training error rate: 48.000000
200.000000/1000.000000 ==> Training loss: 1.832415    Training error rate: 68.000000
300.000000/1000.000000 ==> Training loss: 2.441768    Training error rate: 62.000000
400.000000/1000.000000 ==> Training loss: 1.979749    Training error rate: 54.000000
500.000000/1000.000000 ==> Training loss: 1.699971    Training error rate: 48.000000
600.000000/1000.000000 ==> Training loss: 1.998232    Training error rate: 62.000000
700.000000/1000.000000 ==> Training loss: 1.911027    Training error rate: 52.000000
800.000000/1000.000000 ==> Training loss: 1.935393    Training error rate: 60.000000
900.000000/1000.000000 ==> Training loss: 1.682388    Training error rate: 46.000000
==> Total training loss: 1867.391383    Total training error rate: 51.668000
==> Testing Epoch: 6
0.000000/100.000000 ==> Testing loss: 2.886417    Testing error rate: 67.000000
==> Total testing loss: 270.531087    Total testing error rate: 63.660000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 7
0.000000/1000.000000 ==> Training loss: 1.563406    Training error rate: 54.000000
100.000000/1000.000000 ==> Training loss: 1.430551    Training error rate: 40.000000
200.000000/1000.000000 ==> Training loss: 1.931803    Training error rate: 56.000000
300.000000/1000.000000 ==> Training loss: 1.661745    Training error rate: 48.000000
400.000000/1000.000000 ==> Training loss: 1.382025    Training error rate: 38.000000
500.000000/1000.000000 ==> Training loss: 1.602836    Training error rate: 56.000000
600.000000/1000.000000 ==> Training loss: 1.621394    Training error rate: 42.000000
700.000000/1000.000000 ==> Training loss: 1.563127    Training error rate: 42.000000
800.000000/1000.000000 ==> Training loss: 1.226352    Training error rate: 38.000000
900.000000/1000.000000 ==> Training loss: 1.731452    Training error rate: 40.000000
==> Total training loss: 1705.587272    Total training error rate: 47.578000
==> Testing Epoch: 7
0.000000/100.000000 ==> Testing loss: 2.299769    Testing error rate: 56.000000
==> Total testing loss: 219.326081    Total testing error rate: 56.590000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 8
0.000000/1000.000000 ==> Training loss: 1.355269    Training error rate: 42.000000
100.000000/1000.000000 ==> Training loss: 1.878788    Training error rate: 48.000000
200.000000/1000.000000 ==> Training loss: 1.474158    Training error rate: 46.000000
300.000000/1000.000000 ==> Training loss: 1.503362    Training error rate: 44.000000
400.000000/1000.000000 ==> Training loss: 1.400327    Training error rate: 36.000000
500.000000/1000.000000 ==> Training loss: 1.795923    Training error rate: 52.000000
600.000000/1000.000000 ==> Training loss: 1.825514    Training error rate: 58.000000
700.000000/1000.000000 ==> Training loss: 1.676250    Training error rate: 46.000000
800.000000/1000.000000 ==> Training loss: 1.662952    Training error rate: 50.000000
900.000000/1000.000000 ==> Training loss: 1.520570    Training error rate: 44.000000
==> Total training loss: 1578.471054    Total training error rate: 44.666000
==> Testing Epoch: 8
0.000000/100.000000 ==> Testing loss: 2.389746    Testing error rate: 55.000000
==> Total testing loss: 223.094393    Total testing error rate: 56.370000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 9
0.000000/1000.000000 ==> Training loss: 1.091852    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 1.344022    Training error rate: 46.000000
200.000000/1000.000000 ==> Training loss: 1.223024    Training error rate: 42.000000
300.000000/1000.000000 ==> Training loss: 1.501955    Training error rate: 42.000000
400.000000/1000.000000 ==> Training loss: 1.222848    Training error rate: 36.000000
500.000000/1000.000000 ==> Training loss: 1.997183    Training error rate: 58.000000
600.000000/1000.000000 ==> Training loss: 1.473073    Training error rate: 38.000000
700.000000/1000.000000 ==> Training loss: 1.204914    Training error rate: 38.000000
800.000000/1000.000000 ==> Training loss: 1.561130    Training error rate: 44.000000
900.000000/1000.000000 ==> Training loss: 1.691139    Training error rate: 54.000000
==> Total training loss: 1459.065932    Total training error rate: 41.808000
==> Testing Epoch: 9
0.000000/100.000000 ==> Testing loss: 2.401368    Testing error rate: 62.000000
==> Total testing loss: 214.661457    Total testing error rate: 54.060000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 10
0.000000/1000.000000 ==> Training loss: 1.071183    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 1.448654    Training error rate: 44.000000
200.000000/1000.000000 ==> Training loss: 1.541184    Training error rate: 50.000000
300.000000/1000.000000 ==> Training loss: 1.290782    Training error rate: 40.000000
400.000000/1000.000000 ==> Training loss: 1.304506    Training error rate: 36.000000
500.000000/1000.000000 ==> Training loss: 1.508149    Training error rate: 46.000000
600.000000/1000.000000 ==> Training loss: 1.346881    Training error rate: 36.000000
700.000000/1000.000000 ==> Training loss: 1.584951    Training error rate: 44.000000
800.000000/1000.000000 ==> Training loss: 1.143670    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 1.458046    Training error rate: 34.000000
==> Total training loss: 1369.089479    Total training error rate: 39.458000
==> Testing Epoch: 10
0.000000/100.000000 ==> Testing loss: 2.254688    Testing error rate: 58.000000
==> Total testing loss: 233.226773    Total testing error rate: 55.330000
==> Set learning rate: 0.010000
==> Training Epoch: 11
0.000000/1000.000000 ==> Training loss: 1.337353    Training error rate: 36.000000
100.000000/1000.000000 ==> Training loss: 1.413534    Training error rate: 44.000000
200.000000/1000.000000 ==> Training loss: 1.179372    Training error rate: 36.000000
300.000000/1000.000000 ==> Training loss: 1.415022    Training error rate: 42.000000
400.000000/1000.000000 ==> Training loss: 1.093014    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 0.995920    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.979457    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.938977    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 1.245114    Training error rate: 36.000000
900.000000/1000.000000 ==> Training loss: 1.025807    Training error rate: 38.000000
==> Total training loss: 1295.399092    Total training error rate: 37.564000
==> Testing Epoch: 11
0.000000/100.000000 ==> Testing loss: 2.384130    Testing error rate: 51.000000
==> Total testing loss: 229.952577    Total testing error rate: 53.410000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 12
0.000000/1000.000000 ==> Training loss: 1.645934    Training error rate: 46.000000
100.000000/1000.000000 ==> Training loss: 1.072550    Training error rate: 30.000000
200.000000/1000.000000 ==> Training loss: 0.899796    Training error rate: 30.000000
300.000000/1000.000000 ==> Training loss: 1.564893    Training error rate: 44.000000
400.000000/1000.000000 ==> Training loss: 1.321002    Training error rate: 44.000000
500.000000/1000.000000 ==> Training loss: 1.149091    Training error rate: 32.000000
600.000000/1000.000000 ==> Training loss: 1.361670    Training error rate: 40.000000
700.000000/1000.000000 ==> Training loss: 1.011078    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 1.542817    Training error rate: 46.000000
900.000000/1000.000000 ==> Training loss: 1.502945    Training error rate: 40.000000
==> Total training loss: 1219.189189    Total training error rate: 35.744000
==> Testing Epoch: 12
0.000000/100.000000 ==> Testing loss: 1.972164    Testing error rate: 52.000000
==> Total testing loss: 215.671940    Total testing error rate: 52.150000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 13
0.000000/1000.000000 ==> Training loss: 0.959404    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 1.188228    Training error rate: 36.000000
200.000000/1000.000000 ==> Training loss: 1.397708    Training error rate: 42.000000
300.000000/1000.000000 ==> Training loss: 0.858842    Training error rate: 30.000000
400.000000/1000.000000 ==> Training loss: 1.054927    Training error rate: 36.000000
500.000000/1000.000000 ==> Training loss: 1.077719    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 1.300400    Training error rate: 34.000000
700.000000/1000.000000 ==> Training loss: 0.678299    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 1.146027    Training error rate: 38.000000
900.000000/1000.000000 ==> Training loss: 0.945617    Training error rate: 28.000000
==> Total training loss: 1151.066606    Total training error rate: 34.010000
==> Testing Epoch: 13
0.000000/100.000000 ==> Testing loss: 1.554674    Testing error rate: 42.000000
==> Total testing loss: 195.500296    Total testing error rate: 48.520000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 14
0.000000/1000.000000 ==> Training loss: 0.802394    Training error rate: 26.000000
100.000000/1000.000000 ==> Training loss: 0.953473    Training error rate: 30.000000
200.000000/1000.000000 ==> Training loss: 0.919809    Training error rate: 30.000000
300.000000/1000.000000 ==> Training loss: 1.169524    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 1.296283    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.882037    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.944323    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 1.091447    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 1.020110    Training error rate: 34.000000
900.000000/1000.000000 ==> Training loss: 1.073417    Training error rate: 36.000000
==> Total training loss: 1095.356083    Total training error rate: 32.372000
==> Testing Epoch: 14
0.000000/100.000000 ==> Testing loss: 1.954979    Testing error rate: 49.000000
==> Total testing loss: 183.702391    Total testing error rate: 47.320000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 15
0.000000/1000.000000 ==> Training loss: 0.954156    Training error rate: 34.000000
100.000000/1000.000000 ==> Training loss: 0.958515    Training error rate: 32.000000
200.000000/1000.000000 ==> Training loss: 0.987152    Training error rate: 30.000000
300.000000/1000.000000 ==> Training loss: 1.098017    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 1.285064    Training error rate: 38.000000
500.000000/1000.000000 ==> Training loss: 1.189380    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 1.293163    Training error rate: 38.000000
700.000000/1000.000000 ==> Training loss: 1.419076    Training error rate: 48.000000
800.000000/1000.000000 ==> Training loss: 1.278156    Training error rate: 42.000000
900.000000/1000.000000 ==> Training loss: 0.957181    Training error rate: 22.000000
==> Total training loss: 1045.934546    Total training error rate: 30.944000
==> Testing Epoch: 15
0.000000/100.000000 ==> Testing loss: 1.749119    Testing error rate: 49.000000
==> Total testing loss: 197.496816    Total testing error rate: 48.030000
==> Set learning rate: 0.010000
==> Training Epoch: 16
0.000000/1000.000000 ==> Training loss: 0.926130    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.666075    Training error rate: 24.000000
200.000000/1000.000000 ==> Training loss: 1.430104    Training error rate: 40.000000
300.000000/1000.000000 ==> Training loss: 0.965926    Training error rate: 36.000000
400.000000/1000.000000 ==> Training loss: 1.139295    Training error rate: 32.000000
500.000000/1000.000000 ==> Training loss: 0.975615    Training error rate: 32.000000
600.000000/1000.000000 ==> Training loss: 0.760650    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 1.439827    Training error rate: 40.000000
800.000000/1000.000000 ==> Training loss: 0.876727    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.872970    Training error rate: 20.000000
==> Total training loss: 990.867218    Total training error rate: 29.664000
==> Testing Epoch: 16
0.000000/100.000000 ==> Testing loss: 1.691551    Testing error rate: 42.000000
==> Total testing loss: 171.199939    Total testing error rate: 44.510000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 17
0.000000/1000.000000 ==> Training loss: 1.148486    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 0.841764    Training error rate: 22.000000
200.000000/1000.000000 ==> Training loss: 0.894710    Training error rate: 32.000000
300.000000/1000.000000 ==> Training loss: 0.901047    Training error rate: 32.000000
400.000000/1000.000000 ==> Training loss: 1.085627    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.988337    Training error rate: 36.000000
600.000000/1000.000000 ==> Training loss: 0.933445    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 1.011628    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.994807    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 0.809073    Training error rate: 26.000000
==> Total training loss: 948.811897    Total training error rate: 28.532000
==> Testing Epoch: 17
0.000000/100.000000 ==> Testing loss: 1.998513    Testing error rate: 42.000000
==> Total testing loss: 189.229087    Total testing error rate: 46.660000
==> Set learning rate: 0.010000
==> Training Epoch: 18
0.000000/1000.000000 ==> Training loss: 1.015939    Training error rate: 28.000000
100.000000/1000.000000 ==> Training loss: 1.016673    Training error rate: 34.000000
200.000000/1000.000000 ==> Training loss: 0.789321    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.840131    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 0.818597    Training error rate: 34.000000
500.000000/1000.000000 ==> Training loss: 0.927304    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 1.106625    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 0.978227    Training error rate: 34.000000
800.000000/1000.000000 ==> Training loss: 0.840871    Training error rate: 26.000000
900.000000/1000.000000 ==> Training loss: 0.977291    Training error rate: 28.000000
==> Total training loss: 903.705311    Total training error rate: 27.356000
==> Testing Epoch: 18
0.000000/100.000000 ==> Testing loss: 1.762295    Testing error rate: 42.000000
==> Total testing loss: 177.349198    Total testing error rate: 45.240000
==> Set learning rate: 0.010000
==> Training Epoch: 19
0.000000/1000.000000 ==> Training loss: 0.961759    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 0.798551    Training error rate: 26.000000
200.000000/1000.000000 ==> Training loss: 1.096774    Training error rate: 28.000000
300.000000/1000.000000 ==> Training loss: 0.875116    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 0.851221    Training error rate: 32.000000
500.000000/1000.000000 ==> Training loss: 1.110812    Training error rate: 28.000000
600.000000/1000.000000 ==> Training loss: 0.895784    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 1.049780    Training error rate: 30.000000
800.000000/1000.000000 ==> Training loss: 1.073827    Training error rate: 32.000000
900.000000/1000.000000 ==> Training loss: 1.243038    Training error rate: 38.000000
==> Total training loss: 870.135698    Total training error rate: 26.144000
==> Testing Epoch: 19
0.000000/100.000000 ==> Testing loss: 1.942177    Testing error rate: 54.000000
==> Total testing loss: 193.349457    Total testing error rate: 47.310000
==> Set learning rate: 0.010000
==> Training Epoch: 20
0.000000/1000.000000 ==> Training loss: 1.015124    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.649942    Training error rate: 22.000000
200.000000/1000.000000 ==> Training loss: 0.546638    Training error rate: 16.000000
300.000000/1000.000000 ==> Training loss: 0.865654    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.932651    Training error rate: 30.000000
500.000000/1000.000000 ==> Training loss: 1.091422    Training error rate: 38.000000
600.000000/1000.000000 ==> Training loss: 0.900803    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 1.010201    Training error rate: 36.000000
800.000000/1000.000000 ==> Training loss: 0.722129    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.938634    Training error rate: 30.000000
==> Total training loss: 831.649012    Total training error rate: 25.234000
==> Testing Epoch: 20
0.000000/100.000000 ==> Testing loss: 1.594887    Testing error rate: 43.000000
==> Total testing loss: 176.109656    Total testing error rate: 44.340000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 21
0.000000/1000.000000 ==> Training loss: 0.814459    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.671666    Training error rate: 22.000000
200.000000/1000.000000 ==> Training loss: 0.760440    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.771996    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.789225    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 0.673419    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.834301    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.760214    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.692888    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 1.019644    Training error rate: 30.000000
==> Total training loss: 797.710746    Total training error rate: 24.264000
==> Testing Epoch: 21
0.000000/100.000000 ==> Testing loss: 2.057073    Testing error rate: 47.000000
==> Total testing loss: 189.960852    Total testing error rate: 45.800000
==> Set learning rate: 0.010000
==> Training Epoch: 22
0.000000/1000.000000 ==> Training loss: 0.794362    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.528777    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.751217    Training error rate: 26.000000
300.000000/1000.000000 ==> Training loss: 0.715837    Training error rate: 22.000000
400.000000/1000.000000 ==> Training loss: 0.787650    Training error rate: 28.000000
500.000000/1000.000000 ==> Training loss: 0.664189    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.558804    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.942666    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.752603    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.819102    Training error rate: 24.000000
==> Total training loss: 771.484713    Total training error rate: 23.694000
==> Testing Epoch: 22
0.000000/100.000000 ==> Testing loss: 2.003158    Testing error rate: 52.000000
==> Total testing loss: 179.679873    Total testing error rate: 43.980000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 23
0.000000/1000.000000 ==> Training loss: 0.890198    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.491186    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.777000    Training error rate: 28.000000
300.000000/1000.000000 ==> Training loss: 0.802725    Training error rate: 28.000000
400.000000/1000.000000 ==> Training loss: 0.473655    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.622714    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.825145    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.874284    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.789815    Training error rate: 26.000000
900.000000/1000.000000 ==> Training loss: 0.878695    Training error rate: 22.000000
==> Total training loss: 745.050158    Total training error rate: 22.568000
==> Testing Epoch: 23
0.000000/100.000000 ==> Testing loss: 1.686199    Testing error rate: 47.000000
==> Total testing loss: 182.061520    Total testing error rate: 44.120000
==> Set learning rate: 0.010000
==> Training Epoch: 24
0.000000/1000.000000 ==> Training loss: 0.854631    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.776864    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.667158    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 1.134658    Training error rate: 36.000000
400.000000/1000.000000 ==> Training loss: 0.671161    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.563930    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.788218    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.767423    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.608784    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.847666    Training error rate: 26.000000
==> Total training loss: 716.414327    Total training error rate: 22.022000
==> Testing Epoch: 24
0.000000/100.000000 ==> Testing loss: 1.732280    Testing error rate: 41.000000
==> Total testing loss: 176.618351    Total testing error rate: 43.690000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 25
0.000000/1000.000000 ==> Training loss: 0.441675    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.713651    Training error rate: 22.000000
200.000000/1000.000000 ==> Training loss: 0.787225    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.564408    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.823469    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.816308    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.564132    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.852936    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 0.639583    Training error rate: 24.000000
900.000000/1000.000000 ==> Training loss: 0.490408    Training error rate: 12.000000
==> Total training loss: 693.989807    Total training error rate: 21.254000
==> Testing Epoch: 25
0.000000/100.000000 ==> Testing loss: 1.500215    Testing error rate: 37.000000
==> Total testing loss: 175.794650    Total testing error rate: 42.760000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 26
0.000000/1000.000000 ==> Training loss: 0.812693    Training error rate: 20.000000
100.000000/1000.000000 ==> Training loss: 0.339013    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.475203    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.618332    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.619184    Training error rate: 14.000000
500.000000/1000.000000 ==> Training loss: 0.608935    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.682288    Training error rate: 22.000000
700.000000/1000.000000 ==> Training loss: 0.570066    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.694981    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.817468    Training error rate: 18.000000
==> Total training loss: 680.408529    Total training error rate: 20.854000
==> Testing Epoch: 26
0.000000/100.000000 ==> Testing loss: 1.459890    Testing error rate: 36.000000
==> Total testing loss: 159.195637    Total testing error rate: 40.910000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 27
0.000000/1000.000000 ==> Training loss: 0.460475    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.612138    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.910815    Training error rate: 24.000000
300.000000/1000.000000 ==> Training loss: 0.438091    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.693664    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.416831    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.747211    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.685577    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.548537    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.838109    Training error rate: 26.000000
==> Total training loss: 660.128173    Total training error rate: 20.454000
==> Testing Epoch: 27
0.000000/100.000000 ==> Testing loss: 1.758072    Testing error rate: 44.000000
==> Total testing loss: 185.347957    Total testing error rate: 43.940000
==> Set learning rate: 0.010000
==> Training Epoch: 28
0.000000/1000.000000 ==> Training loss: 0.454344    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.312407    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.560889    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.487074    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.580582    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.407102    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.472208    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.577155    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.731925    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.440594    Training error rate: 14.000000
==> Total training loss: 631.711612    Total training error rate: 19.430000
==> Testing Epoch: 28
0.000000/100.000000 ==> Testing loss: 1.564677    Testing error rate: 42.000000
==> Total testing loss: 180.212621    Total testing error rate: 43.920000
==> Set learning rate: 0.010000
==> Training Epoch: 29
0.000000/1000.000000 ==> Training loss: 0.493917    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.385152    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.638107    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.505762    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.592048    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.720758    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.877318    Training error rate: 28.000000
700.000000/1000.000000 ==> Training loss: 1.101351    Training error rate: 32.000000
800.000000/1000.000000 ==> Training loss: 0.652723    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.776432    Training error rate: 26.000000
==> Total training loss: 623.614789    Total training error rate: 19.418000
==> Testing Epoch: 29
0.000000/100.000000 ==> Testing loss: 1.630931    Testing error rate: 38.000000
==> Total testing loss: 168.566996    Total testing error rate: 41.290000
==> Set learning rate: 0.010000
==> Training Epoch: 30
0.000000/1000.000000 ==> Training loss: 0.462402    Training error rate: 14.000000
100.000000/1000.000000 ==> Training loss: 0.695787    Training error rate: 22.000000
200.000000/1000.000000 ==> Training loss: 0.483839    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 1.095818    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 0.561890    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.600991    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.548579    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.663465    Training error rate: 20.000000
800.000000/1000.000000 ==> Training loss: 0.882852    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.622469    Training error rate: 22.000000
==> Total training loss: 606.469483    Total training error rate: 18.868000
==> Testing Epoch: 30
0.000000/100.000000 ==> Testing loss: 1.625851    Testing error rate: 34.000000
==> Total testing loss: 175.185879    Total testing error rate: 42.610000
==> Set learning rate: 0.010000
==> Training Epoch: 31
0.000000/1000.000000 ==> Training loss: 0.782567    Training error rate: 32.000000
100.000000/1000.000000 ==> Training loss: 0.547678    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.262194    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.930533    Training error rate: 26.000000
400.000000/1000.000000 ==> Training loss: 0.366207    Training error rate: 4.000000
500.000000/1000.000000 ==> Training loss: 0.623067    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.535950    Training error rate: 20.000000
700.000000/1000.000000 ==> Training loss: 0.532282    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.437446    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.727559    Training error rate: 20.000000
==> Total training loss: 600.858427    Total training error rate: 18.802000
==> Testing Epoch: 31
0.000000/100.000000 ==> Testing loss: 1.591991    Testing error rate: 42.000000
==> Total testing loss: 170.397745    Total testing error rate: 41.180000
==> Set learning rate: 0.010000
==> Training Epoch: 32
0.000000/1000.000000 ==> Training loss: 0.485122    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.480838    Training error rate: 16.000000
200.000000/1000.000000 ==> Training loss: 0.658293    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.528642    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.379574    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.637040    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.542195    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.733467    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.788818    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.417379    Training error rate: 10.000000
==> Total training loss: 568.408546    Total training error rate: 17.748000
==> Testing Epoch: 32
0.000000/100.000000 ==> Testing loss: 2.010921    Testing error rate: 45.000000
==> Total testing loss: 189.906169    Total testing error rate: 43.990000
==> Set learning rate: 0.010000
==> Training Epoch: 33
0.000000/1000.000000 ==> Training loss: 0.378603    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.379136    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.572520    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.771034    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.539149    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.584711    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.579541    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.674578    Training error rate: 26.000000
800.000000/1000.000000 ==> Training loss: 0.412109    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.471381    Training error rate: 20.000000
==> Total training loss: 565.526445    Total training error rate: 17.616000
==> Testing Epoch: 33
0.000000/100.000000 ==> Testing loss: 1.667126    Testing error rate: 39.000000
==> Total testing loss: 180.642826    Total testing error rate: 42.130000
==> Set learning rate: 0.010000
==> Training Epoch: 34
0.000000/1000.000000 ==> Training loss: 0.479793    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.364937    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.537265    Training error rate: 10.000000
300.000000/1000.000000 ==> Training loss: 0.320221    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.747057    Training error rate: 24.000000
500.000000/1000.000000 ==> Training loss: 0.752923    Training error rate: 22.000000
600.000000/1000.000000 ==> Training loss: 0.564998    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.699115    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.791660    Training error rate: 30.000000
900.000000/1000.000000 ==> Training loss: 1.282302    Training error rate: 36.000000
==> Total training loss: 550.647861    Total training error rate: 17.050000
==> Testing Epoch: 34
0.000000/100.000000 ==> Testing loss: 1.669310    Testing error rate: 39.000000
==> Total testing loss: 179.734707    Total testing error rate: 42.080000
==> Set learning rate: 0.010000
==> Training Epoch: 35
0.000000/1000.000000 ==> Training loss: 0.637802    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.460247    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.462617    Training error rate: 18.000000
300.000000/1000.000000 ==> Training loss: 0.667612    Training error rate: 20.000000
400.000000/1000.000000 ==> Training loss: 0.371223    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.496882    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.463973    Training error rate: 14.000000
700.000000/1000.000000 ==> Training loss: 0.758962    Training error rate: 22.000000
800.000000/1000.000000 ==> Training loss: 0.384491    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.801860    Training error rate: 26.000000
==> Total training loss: 540.979291    Total training error rate: 16.982000
==> Testing Epoch: 35
0.000000/100.000000 ==> Testing loss: 2.164583    Testing error rate: 53.000000
==> Total testing loss: 193.730852    Total testing error rate: 43.690000
==> Set learning rate: 0.010000
==> Training Epoch: 36
0.000000/1000.000000 ==> Training loss: 0.312723    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.564491    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.298811    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.493703    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.560088    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.646700    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.338296    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.684377    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.535118    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.516773    Training error rate: 16.000000
==> Total training loss: 535.470670    Total training error rate: 16.540000
==> Testing Epoch: 36
0.000000/100.000000 ==> Testing loss: 1.953950    Testing error rate: 37.000000
==> Total testing loss: 181.303533    Total testing error rate: 42.220000
==> Set learning rate: 0.010000
==> Training Epoch: 37
0.000000/1000.000000 ==> Training loss: 0.503060    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.410638    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.390518    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.325293    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.457998    Training error rate: 18.000000
500.000000/1000.000000 ==> Training loss: 0.286344    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.584893    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.589870    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.622337    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.701182    Training error rate: 28.000000
==> Total training loss: 513.202893    Total training error rate: 16.134000
==> Testing Epoch: 37
0.000000/100.000000 ==> Testing loss: 1.908954    Testing error rate: 40.000000
==> Total testing loss: 171.182696    Total testing error rate: 40.600000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 38
0.000000/1000.000000 ==> Training loss: 0.620511    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.674493    Training error rate: 18.000000
200.000000/1000.000000 ==> Training loss: 0.349065    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.450068    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.267317    Training error rate: 10.000000
500.000000/1000.000000 ==> Training loss: 0.493691    Training error rate: 18.000000
600.000000/1000.000000 ==> Training loss: 0.415788    Training error rate: 12.000000
700.000000/1000.000000 ==> Training loss: 0.561967    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.622924    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.351649    Training error rate: 12.000000
==> Total training loss: 508.807976    Total training error rate: 15.814000
==> Testing Epoch: 38
0.000000/100.000000 ==> Testing loss: 1.548828    Testing error rate: 37.000000
==> Total testing loss: 185.994283    Total testing error rate: 43.130000
==> Set learning rate: 0.010000
==> Training Epoch: 39
0.000000/1000.000000 ==> Training loss: 0.490647    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.682987    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.337803    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.514217    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.471195    Training error rate: 6.000000
500.000000/1000.000000 ==> Training loss: 0.443435    Training error rate: 16.000000
600.000000/1000.000000 ==> Training loss: 0.723701    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.287813    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.592667    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 1.001890    Training error rate: 30.000000
==> Total training loss: 496.469943    Total training error rate: 15.592000
==> Testing Epoch: 39
0.000000/100.000000 ==> Testing loss: 1.956181    Testing error rate: 46.000000
==> Total testing loss: 176.553693    Total testing error rate: 41.650000
==> Set learning rate: 0.010000
==> Training Epoch: 40
0.000000/1000.000000 ==> Training loss: 0.437562    Training error rate: 16.000000
100.000000/1000.000000 ==> Training loss: 0.492771    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.601554    Training error rate: 22.000000
300.000000/1000.000000 ==> Training loss: 0.422332    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.414690    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.811115    Training error rate: 32.000000
600.000000/1000.000000 ==> Training loss: 0.696367    Training error rate: 24.000000
700.000000/1000.000000 ==> Training loss: 0.769739    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.543900    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.853782    Training error rate: 26.000000
==> Total training loss: 492.967295    Total training error rate: 15.504000
==> Testing Epoch: 40
0.000000/100.000000 ==> Testing loss: 1.876196    Testing error rate: 43.000000
==> Total testing loss: 194.751362    Total testing error rate: 43.940000
==> Set learning rate: 0.010000
==> Training Epoch: 41
0.000000/1000.000000 ==> Training loss: 0.325444    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.270263    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.529899    Training error rate: 12.000000
300.000000/1000.000000 ==> Training loss: 0.260124    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.562397    Training error rate: 16.000000
500.000000/1000.000000 ==> Training loss: 0.553276    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.572013    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.539995    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.684391    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.457510    Training error rate: 18.000000
==> Total training loss: 493.516522    Total training error rate: 15.288000
==> Testing Epoch: 41
0.000000/100.000000 ==> Testing loss: 1.781908    Testing error rate: 42.000000
==> Total testing loss: 180.504383    Total testing error rate: 42.020000
==> Set learning rate: 0.010000
==> Training Epoch: 42
0.000000/1000.000000 ==> Training loss: 0.402514    Training error rate: 10.000000
100.000000/1000.000000 ==> Training loss: 0.340671    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.381178    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.288171    Training error rate: 6.000000
400.000000/1000.000000 ==> Training loss: 0.673862    Training error rate: 22.000000
500.000000/1000.000000 ==> Training loss: 0.434230    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.406081    Training error rate: 16.000000
700.000000/1000.000000 ==> Training loss: 0.793746    Training error rate: 28.000000
800.000000/1000.000000 ==> Training loss: 0.579647    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.570419    Training error rate: 18.000000
==> Total training loss: 471.419131    Total training error rate: 14.872000
==> Testing Epoch: 42
0.000000/100.000000 ==> Testing loss: 1.883098    Testing error rate: 38.000000
==> Total testing loss: 198.362015    Total testing error rate: 43.530000
==> Set learning rate: 0.010000
==> Training Epoch: 43
0.000000/1000.000000 ==> Training loss: 0.239171    Training error rate: 4.000000
100.000000/1000.000000 ==> Training loss: 0.368932    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.319172    Training error rate: 10.000000
300.000000/1000.000000 ==> Training loss: 0.402832    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.412586    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.479100    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.771230    Training error rate: 26.000000
700.000000/1000.000000 ==> Training loss: 0.599208    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.395240    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.425829    Training error rate: 10.000000
==> Total training loss: 469.008993    Total training error rate: 14.606000
==> Testing Epoch: 43
0.000000/100.000000 ==> Testing loss: 1.486133    Testing error rate: 39.000000
==> Total testing loss: 174.874867    Total testing error rate: 40.350000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 44
0.000000/1000.000000 ==> Training loss: 0.665832    Training error rate: 24.000000
100.000000/1000.000000 ==> Training loss: 0.443975    Training error rate: 20.000000
200.000000/1000.000000 ==> Training loss: 0.331824    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.493070    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.142533    Training error rate: 4.000000
500.000000/1000.000000 ==> Training loss: 0.929719    Training error rate: 26.000000
600.000000/1000.000000 ==> Training loss: 0.602986    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.568322    Training error rate: 16.000000
800.000000/1000.000000 ==> Training loss: 0.538399    Training error rate: 18.000000
900.000000/1000.000000 ==> Training loss: 0.450006    Training error rate: 18.000000
==> Total training loss: 449.019175    Total training error rate: 14.146000
==> Testing Epoch: 44
0.000000/100.000000 ==> Testing loss: 1.431592    Testing error rate: 35.000000
==> Total testing loss: 184.743575    Total testing error rate: 41.840000
==> Set learning rate: 0.010000
==> Training Epoch: 45
0.000000/1000.000000 ==> Training loss: 0.364674    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.369946    Training error rate: 10.000000
200.000000/1000.000000 ==> Training loss: 0.253580    Training error rate: 6.000000
300.000000/1000.000000 ==> Training loss: 0.333282    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.296576    Training error rate: 6.000000
500.000000/1000.000000 ==> Training loss: 0.464508    Training error rate: 14.000000
600.000000/1000.000000 ==> Training loss: 0.456315    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.424137    Training error rate: 14.000000
800.000000/1000.000000 ==> Training loss: 0.478714    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.516222    Training error rate: 18.000000
==> Total training loss: 454.818352    Total training error rate: 14.228000
==> Testing Epoch: 45
0.000000/100.000000 ==> Testing loss: 1.624008    Testing error rate: 38.000000
==> Total testing loss: 177.028372    Total testing error rate: 41.640000
==> Set learning rate: 0.010000
==> Training Epoch: 46
0.000000/1000.000000 ==> Training loss: 0.589590    Training error rate: 22.000000
100.000000/1000.000000 ==> Training loss: 0.333070    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.346780    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.475160    Training error rate: 12.000000
400.000000/1000.000000 ==> Training loss: 0.518806    Training error rate: 20.000000
500.000000/1000.000000 ==> Training loss: 0.468921    Training error rate: 20.000000
600.000000/1000.000000 ==> Training loss: 0.590812    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.538617    Training error rate: 18.000000
800.000000/1000.000000 ==> Training loss: 0.533155    Training error rate: 12.000000
900.000000/1000.000000 ==> Training loss: 0.445186    Training error rate: 14.000000
==> Total training loss: 443.780317    Total training error rate: 13.866000
==> Testing Epoch: 46
0.000000/100.000000 ==> Testing loss: 1.282090    Testing error rate: 34.000000
==> Total testing loss: 174.122086    Total testing error rate: 39.840000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 47
0.000000/1000.000000 ==> Training loss: 0.383865    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.329848    Training error rate: 12.000000
200.000000/1000.000000 ==> Training loss: 0.661150    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.636066    Training error rate: 18.000000
400.000000/1000.000000 ==> Training loss: 0.379962    Training error rate: 12.000000
500.000000/1000.000000 ==> Training loss: 0.637454    Training error rate: 24.000000
600.000000/1000.000000 ==> Training loss: 0.203078    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.596674    Training error rate: 24.000000
800.000000/1000.000000 ==> Training loss: 0.466511    Training error rate: 14.000000
900.000000/1000.000000 ==> Training loss: 0.421398    Training error rate: 12.000000
==> Total training loss: 445.161870    Total training error rate: 13.878000
==> Testing Epoch: 47
0.000000/100.000000 ==> Testing loss: 1.673970    Testing error rate: 35.000000
==> Total testing loss: 179.587851    Total testing error rate: 40.790000
==> Set learning rate: 0.010000
==> Training Epoch: 48
0.000000/1000.000000 ==> Training loss: 0.217058    Training error rate: 6.000000
100.000000/1000.000000 ==> Training loss: 0.453059    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.337857    Training error rate: 8.000000
300.000000/1000.000000 ==> Training loss: 0.556938    Training error rate: 24.000000
400.000000/1000.000000 ==> Training loss: 0.641875    Training error rate: 26.000000
500.000000/1000.000000 ==> Training loss: 0.292961    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.277700    Training error rate: 10.000000
700.000000/1000.000000 ==> Training loss: 0.295058    Training error rate: 4.000000
800.000000/1000.000000 ==> Training loss: 0.584822    Training error rate: 16.000000
900.000000/1000.000000 ==> Training loss: 0.266677    Training error rate: 12.000000
==> Total training loss: 432.259056    Total training error rate: 13.636000
==> Testing Epoch: 48
0.000000/100.000000 ==> Testing loss: 1.834350    Testing error rate: 44.000000
==> Total testing loss: 175.665729    Total testing error rate: 41.120000
==> Set learning rate: 0.010000
==> Training Epoch: 49
0.000000/1000.000000 ==> Training loss: 0.221587    Training error rate: 4.000000
100.000000/1000.000000 ==> Training loss: 0.333994    Training error rate: 6.000000
200.000000/1000.000000 ==> Training loss: 0.521416    Training error rate: 14.000000
300.000000/1000.000000 ==> Training loss: 0.287644    Training error rate: 14.000000
400.000000/1000.000000 ==> Training loss: 0.266816    Training error rate: 8.000000
500.000000/1000.000000 ==> Training loss: 0.404830    Training error rate: 12.000000
600.000000/1000.000000 ==> Training loss: 0.397837    Training error rate: 8.000000
700.000000/1000.000000 ==> Training loss: 0.331900    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.722452    Training error rate: 20.000000
900.000000/1000.000000 ==> Training loss: 0.300745    Training error rate: 8.000000
==> Total training loss: 423.275342    Total training error rate: 13.202000
==> Testing Epoch: 49
0.000000/100.000000 ==> Testing loss: 2.009725    Testing error rate: 42.000000
==> Total testing loss: 165.943739    Total testing error rate: 39.020000
==> Saving checkpoint..
==> Set learning rate: 0.010000
==> Training Epoch: 50
0.000000/1000.000000 ==> Training loss: 0.317618    Training error rate: 8.000000
100.000000/1000.000000 ==> Training loss: 0.418253    Training error rate: 14.000000
200.000000/1000.000000 ==> Training loss: 0.549589    Training error rate: 20.000000
300.000000/1000.000000 ==> Training loss: 0.361170    Training error rate: 10.000000
400.000000/1000.000000 ==> Training loss: 0.178025    Training error rate: 4.000000
500.000000/1000.000000 ==> Training loss: 0.260252    Training error rate: 10.000000
600.000000/1000.000000 ==> Training loss: 0.536732    Training error rate: 18.000000
700.000000/1000.000000 ==> Training loss: 0.379803    Training error rate: 12.000000
800.000000/1000.000000 ==> Training loss: 0.536088    Training error rate: 22.000000
900.000000/1000.000000 ==> Training loss: 0.366943    Training error rate: 10.000000
==> Total training loss: 424.431745    Total training error rate: 13.142000
==> Testing Epoch: 50
0.000000/100.000000 ==> Testing loss: 1.461529    Testing error rate: 36.000000
==> Total testing loss: 184.503069    Total testing error rate: 41.710000
==> Set learning rate: 0.001000
==> Training Epoch: 51
0.000000/1000.000000 ==> Training loss: 0.510844    Training error rate: 12.000000
100.000000/1000.000000 ==> Training loss: 0.280538    Training error rate: 8.000000
200.000000/1000.000000 ==> Training loss: 0.071167    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.154904    Training error rate: 6.000000
400.000000/1000.000000 ==> Training loss: 0.184766    Training error rate: 6.000000
500.000000/1000.000000 ==> Training loss: 0.099939    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.129913    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.120512    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.182890    Training error rate: 6.000000
900.000000/1000.000000 ==> Training loss: 0.213719    Training error rate: 8.000000
==> Total training loss: 191.487159    Total training error rate: 5.522000
==> Testing Epoch: 51
0.000000/100.000000 ==> Testing loss: 1.282471    Testing error rate: 30.000000
==> Total testing loss: 136.143745    Total testing error rate: 32.820000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 52
0.000000/1000.000000 ==> Training loss: 0.091273    Training error rate: 4.000000
100.000000/1000.000000 ==> Training loss: 0.111518    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.102986    Training error rate: 4.000000
300.000000/1000.000000 ==> Training loss: 0.095356    Training error rate: 4.000000
400.000000/1000.000000 ==> Training loss: 0.077878    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.088261    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.102892    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.069127    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.070846    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.106999    Training error rate: 2.000000
==> Total training loss: 114.241399    Total training error rate: 2.936000
==> Testing Epoch: 52
0.000000/100.000000 ==> Testing loss: 1.251179    Testing error rate: 32.000000
==> Total testing loss: 131.532194    Total testing error rate: 31.630000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 53
0.000000/1000.000000 ==> Training loss: 0.112718    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.100061    Training error rate: 4.000000
200.000000/1000.000000 ==> Training loss: 0.133804    Training error rate: 4.000000
300.000000/1000.000000 ==> Training loss: 0.051856    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.069039    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.083189    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.085168    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.150129    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.086657    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.053293    Training error rate: 0.000000
==> Total training loss: 92.000097    Total training error rate: 2.238000
==> Testing Epoch: 53
0.000000/100.000000 ==> Testing loss: 1.251142    Testing error rate: 28.000000
==> Total testing loss: 131.641292    Total testing error rate: 31.530000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 54
0.000000/1000.000000 ==> Training loss: 0.055392    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.099775    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.087744    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.049564    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.079615    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.064580    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.065244    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.117149    Training error rate: 4.000000
800.000000/1000.000000 ==> Training loss: 0.051088    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.150180    Training error rate: 2.000000
==> Total training loss: 78.001517    Total training error rate: 1.888000
==> Testing Epoch: 54
0.000000/100.000000 ==> Testing loss: 1.293722    Testing error rate: 33.000000
==> Total testing loss: 135.851583    Total testing error rate: 32.170000
==> Set learning rate: 0.001000
==> Training Epoch: 55
0.000000/1000.000000 ==> Training loss: 0.039651    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.050027    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.047469    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.023307    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.125545    Training error rate: 4.000000
500.000000/1000.000000 ==> Training loss: 0.146205    Training error rate: 4.000000
600.000000/1000.000000 ==> Training loss: 0.090292    Training error rate: 4.000000
700.000000/1000.000000 ==> Training loss: 0.039666    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.040064    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.040114    Training error rate: 2.000000
==> Total training loss: 68.152690    Total training error rate: 1.546000
==> Testing Epoch: 55
0.000000/100.000000 ==> Testing loss: 1.300384    Testing error rate: 30.000000
==> Total testing loss: 134.474907    Total testing error rate: 31.780000
==> Set learning rate: 0.001000
==> Training Epoch: 56
0.000000/1000.000000 ==> Training loss: 0.074139    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.018313    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.039473    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.038523    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.066244    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.053650    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.055396    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.042272    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.047488    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.045156    Training error rate: 0.000000
==> Total training loss: 61.058059    Total training error rate: 1.336000
==> Testing Epoch: 56
0.000000/100.000000 ==> Testing loss: 1.419674    Testing error rate: 32.000000
==> Total testing loss: 135.431575    Total testing error rate: 31.300000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 57
0.000000/1000.000000 ==> Training loss: 0.050515    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.072342    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.124135    Training error rate: 4.000000
300.000000/1000.000000 ==> Training loss: 0.043462    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.045973    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.033083    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.085441    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.020556    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.035233    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.049994    Training error rate: 2.000000
==> Total training loss: 54.060882    Total training error rate: 1.148000
==> Testing Epoch: 57
0.000000/100.000000 ==> Testing loss: 1.221637    Testing error rate: 33.000000
==> Total testing loss: 134.450449    Total testing error rate: 31.420000
==> Set learning rate: 0.001000
==> Training Epoch: 58
0.000000/1000.000000 ==> Training loss: 0.063865    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.026695    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.028522    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.031862    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.045871    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.056054    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.058537    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.092270    Training error rate: 4.000000
800.000000/1000.000000 ==> Training loss: 0.020753    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.036508    Training error rate: 0.000000
==> Total training loss: 48.641284    Total training error rate: 0.930000
==> Testing Epoch: 58
0.000000/100.000000 ==> Testing loss: 1.154315    Testing error rate: 28.000000
==> Total testing loss: 136.586688    Total testing error rate: 31.820000
==> Set learning rate: 0.001000
==> Training Epoch: 59
0.000000/1000.000000 ==> Training loss: 0.036099    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.024407    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.063797    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.061671    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.021512    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.012588    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.020978    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.100503    Training error rate: 6.000000
800.000000/1000.000000 ==> Training loss: 0.066815    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.043417    Training error rate: 2.000000
==> Total training loss: 47.053857    Total training error rate: 0.906000
==> Testing Epoch: 59
0.000000/100.000000 ==> Testing loss: 1.113588    Testing error rate: 30.000000
==> Total testing loss: 136.732808    Total testing error rate: 31.430000
==> Set learning rate: 0.001000
==> Training Epoch: 60
0.000000/1000.000000 ==> Training loss: 0.015573    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.096339    Training error rate: 4.000000
200.000000/1000.000000 ==> Training loss: 0.047165    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.039596    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.105706    Training error rate: 4.000000
500.000000/1000.000000 ==> Training loss: 0.033397    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.058274    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.037183    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.052082    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.052832    Training error rate: 0.000000
==> Total training loss: 41.900665    Total training error rate: 0.784000
==> Testing Epoch: 60
0.000000/100.000000 ==> Testing loss: 1.214751    Testing error rate: 29.000000
==> Total testing loss: 135.483624    Total testing error rate: 31.080000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 61
0.000000/1000.000000 ==> Training loss: 0.026398    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.043112    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.103447    Training error rate: 4.000000
300.000000/1000.000000 ==> Training loss: 0.030643    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.065834    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.040193    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.026498    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.013475    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.037517    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.034309    Training error rate: 0.000000
==> Total training loss: 38.298623    Total training error rate: 0.642000
==> Testing Epoch: 61
0.000000/100.000000 ==> Testing loss: 1.539640    Testing error rate: 30.000000
==> Total testing loss: 139.480581    Total testing error rate: 31.330000
==> Set learning rate: 0.001000
==> Training Epoch: 62
0.000000/1000.000000 ==> Training loss: 0.039481    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.035382    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013261    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.028654    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.052057    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.015190    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.032281    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.020434    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.015843    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.042517    Training error rate: 0.000000
==> Total training loss: 37.912599    Total training error rate: 0.690000
==> Testing Epoch: 62
0.000000/100.000000 ==> Testing loss: 1.317589    Testing error rate: 30.000000
==> Total testing loss: 135.655786    Total testing error rate: 31.060000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 63
0.000000/1000.000000 ==> Training loss: 0.024324    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.110181    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.015615    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.043166    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.048576    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.055654    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.032762    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.027538    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.023815    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.048094    Training error rate: 2.000000
==> Total training loss: 35.510042    Total training error rate: 0.624000
==> Testing Epoch: 63
0.000000/100.000000 ==> Testing loss: 1.363353    Testing error rate: 32.000000
==> Total testing loss: 134.471577    Total testing error rate: 30.800000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 64
0.000000/1000.000000 ==> Training loss: 0.020562    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.037855    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.089769    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.040281    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.026487    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.020020    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.027527    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.020761    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.026741    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.043986    Training error rate: 2.000000
==> Total training loss: 32.938208    Total training error rate: 0.546000
==> Testing Epoch: 64
0.000000/100.000000 ==> Testing loss: 1.299489    Testing error rate: 29.000000
==> Total testing loss: 136.320582    Total testing error rate: 31.320000
==> Set learning rate: 0.001000
==> Training Epoch: 65
0.000000/1000.000000 ==> Training loss: 0.033466    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.008799    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.022012    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.012571    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.022836    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.034714    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.033289    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.036633    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.070199    Training error rate: 4.000000
900.000000/1000.000000 ==> Training loss: 0.015869    Training error rate: 0.000000
==> Total training loss: 31.042217    Total training error rate: 0.480000
==> Testing Epoch: 65
0.000000/100.000000 ==> Testing loss: 1.476321    Testing error rate: 29.000000
==> Total testing loss: 136.150990    Total testing error rate: 31.010000
==> Set learning rate: 0.001000
==> Training Epoch: 66
0.000000/1000.000000 ==> Training loss: 0.056631    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.020944    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.041927    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.053322    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.027658    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.039577    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.046770    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.033630    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010366    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.007543    Training error rate: 0.000000
==> Total training loss: 30.177202    Total training error rate: 0.472000
==> Testing Epoch: 66
0.000000/100.000000 ==> Testing loss: 1.145872    Testing error rate: 28.000000
==> Total testing loss: 134.630661    Total testing error rate: 30.520000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 67
0.000000/1000.000000 ==> Training loss: 0.013165    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.064854    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.015795    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.049196    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.014894    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.013658    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.012393    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.030195    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.013173    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.028258    Training error rate: 0.000000
==> Total training loss: 27.634550    Total training error rate: 0.430000
==> Testing Epoch: 67
0.000000/100.000000 ==> Testing loss: 1.432440    Testing error rate: 32.000000
==> Total testing loss: 137.097075    Total testing error rate: 30.860000
==> Set learning rate: 0.001000
==> Training Epoch: 68
0.000000/1000.000000 ==> Training loss: 0.008817    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.038202    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.033093    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.032291    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.032575    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.032401    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.026093    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.043230    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.037227    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.035680    Training error rate: 2.000000
==> Total training loss: 26.902855    Total training error rate: 0.362000
==> Testing Epoch: 68
0.000000/100.000000 ==> Testing loss: 1.359789    Testing error rate: 31.000000
==> Total testing loss: 141.697651    Total testing error rate: 31.900000
==> Set learning rate: 0.001000
==> Training Epoch: 69
0.000000/1000.000000 ==> Training loss: 0.026171    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.025165    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.029322    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.016944    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.021055    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.018492    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.011765    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.027200    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.024189    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.024802    Training error rate: 0.000000
==> Total training loss: 26.053565    Total training error rate: 0.384000
==> Testing Epoch: 69
0.000000/100.000000 ==> Testing loss: 1.306058    Testing error rate: 34.000000
==> Total testing loss: 136.253311    Total testing error rate: 30.920000
==> Set learning rate: 0.001000
==> Training Epoch: 70
0.000000/1000.000000 ==> Training loss: 0.020331    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.035637    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.014319    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.042560    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.039132    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011030    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.027365    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.033986    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.016163    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.016309    Training error rate: 0.000000
==> Total training loss: 24.967511    Total training error rate: 0.394000
==> Testing Epoch: 70
0.000000/100.000000 ==> Testing loss: 1.168708    Testing error rate: 25.000000
==> Total testing loss: 144.890281    Total testing error rate: 32.300000
==> Set learning rate: 0.001000
==> Training Epoch: 71
0.000000/1000.000000 ==> Training loss: 0.015235    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.022237    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.025542    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.070094    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.028677    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.012902    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.020332    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.024397    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.025034    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.034452    Training error rate: 0.000000
==> Total training loss: 24.157467    Total training error rate: 0.352000
==> Testing Epoch: 71
0.000000/100.000000 ==> Testing loss: 1.487695    Testing error rate: 30.000000
==> Total testing loss: 134.769655    Total testing error rate: 30.600000
==> Set learning rate: 0.001000
==> Training Epoch: 72
0.000000/1000.000000 ==> Training loss: 0.023655    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.010282    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.011758    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.052829    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.024294    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.021521    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.009853    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.026724    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.015227    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.027116    Training error rate: 0.000000
==> Total training loss: 22.444478    Total training error rate: 0.280000
==> Testing Epoch: 72
0.000000/100.000000 ==> Testing loss: 1.066601    Testing error rate: 24.000000
==> Total testing loss: 139.143699    Total testing error rate: 30.840000
==> Set learning rate: 0.001000
==> Training Epoch: 73
0.000000/1000.000000 ==> Training loss: 0.010798    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.007966    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.025320    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.023532    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.014524    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.023380    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.008946    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.021347    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.029776    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.006669    Training error rate: 0.000000
==> Total training loss: 21.548197    Total training error rate: 0.270000
==> Testing Epoch: 73
0.000000/100.000000 ==> Testing loss: 1.243921    Testing error rate: 28.000000
==> Total testing loss: 138.240212    Total testing error rate: 31.160000
==> Set learning rate: 0.001000
==> Training Epoch: 74
0.000000/1000.000000 ==> Training loss: 0.054382    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.022568    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008788    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.010571    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.007673    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.009215    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.031788    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.048794    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.009835    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.008767    Training error rate: 0.000000
==> Total training loss: 21.256304    Total training error rate: 0.302000
==> Testing Epoch: 74
0.000000/100.000000 ==> Testing loss: 1.439783    Testing error rate: 30.000000
==> Total testing loss: 139.794034    Total testing error rate: 31.030000
==> Set learning rate: 0.001000
==> Training Epoch: 75
0.000000/1000.000000 ==> Training loss: 0.036307    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.012641    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.035751    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.030104    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.009576    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.043869    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.022091    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.019446    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.022718    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.007249    Training error rate: 0.000000
==> Total training loss: 21.132035    Total training error rate: 0.256000
==> Testing Epoch: 75
0.000000/100.000000 ==> Testing loss: 1.258858    Testing error rate: 22.000000
==> Total testing loss: 135.435602    Total testing error rate: 30.270000
==> Saving checkpoint..
==> Set learning rate: 0.001000
==> Training Epoch: 76
0.000000/1000.000000 ==> Training loss: 0.013812    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.036436    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.019784    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.013307    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.012294    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.015528    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.012833    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.017597    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.015282    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.042706    Training error rate: 2.000000
==> Total training loss: 19.637831    Total training error rate: 0.232000
==> Testing Epoch: 76
0.000000/100.000000 ==> Testing loss: 1.379287    Testing error rate: 31.000000
==> Total testing loss: 136.838082    Total testing error rate: 30.370000
==> Set learning rate: 0.001000
==> Training Epoch: 77
0.000000/1000.000000 ==> Training loss: 0.012111    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.010697    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.027466    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.014712    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.034571    Training error rate: 2.000000
500.000000/1000.000000 ==> Training loss: 0.024165    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.023183    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.021942    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.020673    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.021457    Training error rate: 0.000000
==> Total training loss: 19.436963    Total training error rate: 0.236000
==> Testing Epoch: 77
0.000000/100.000000 ==> Testing loss: 1.412430    Testing error rate: 30.000000
==> Total testing loss: 137.868217    Total testing error rate: 30.450000
==> Set learning rate: 0.001000
==> Training Epoch: 78
0.000000/1000.000000 ==> Training loss: 0.006614    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.062877    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.014875    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.007625    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.028312    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.014463    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.005859    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.025874    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.015631    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.011571    Training error rate: 0.000000
==> Total training loss: 18.751759    Total training error rate: 0.192000
==> Testing Epoch: 78
0.000000/100.000000 ==> Testing loss: 1.173188    Testing error rate: 27.000000
==> Total testing loss: 138.184397    Total testing error rate: 30.380000
==> Set learning rate: 0.001000
==> Training Epoch: 79
0.000000/1000.000000 ==> Training loss: 0.011794    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.012758    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.008945    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.013845    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.010845    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.008183    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.024955    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.018697    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.009629    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.021466    Training error rate: 0.000000
==> Total training loss: 17.851807    Total training error rate: 0.184000
==> Testing Epoch: 79
0.000000/100.000000 ==> Testing loss: 1.207165    Testing error rate: 28.000000
==> Total testing loss: 138.688115    Total testing error rate: 31.520000
==> Set learning rate: 0.001000
==> Training Epoch: 80
0.000000/1000.000000 ==> Training loss: 0.023124    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.018161    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.010891    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.007600    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.016258    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.024530    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.007190    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.022525    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.019499    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.027806    Training error rate: 0.000000
==> Total training loss: 18.019607    Total training error rate: 0.212000
==> Testing Epoch: 80
0.000000/100.000000 ==> Testing loss: 1.467971    Testing error rate: 30.000000
==> Total testing loss: 140.259773    Total testing error rate: 31.570000
==> Set learning rate: 0.001000
==> Training Epoch: 81
0.000000/1000.000000 ==> Training loss: 0.016413    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.023115    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.007771    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.038019    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.043394    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011273    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.012407    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.012424    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.013759    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.018492    Training error rate: 0.000000
==> Total training loss: 17.717888    Total training error rate: 0.204000
==> Testing Epoch: 81
0.000000/100.000000 ==> Testing loss: 1.077059    Testing error rate: 25.000000
==> Total testing loss: 139.980418    Total testing error rate: 31.600000
==> Set learning rate: 0.001000
==> Training Epoch: 82
0.000000/1000.000000 ==> Training loss: 0.038926    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.012359    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.007193    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.011366    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.005862    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.012811    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.024605    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.010028    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010887    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.010997    Training error rate: 0.000000
==> Total training loss: 16.967002    Total training error rate: 0.200000
==> Testing Epoch: 82
0.000000/100.000000 ==> Testing loss: 1.260672    Testing error rate: 25.000000
==> Total testing loss: 138.598203    Total testing error rate: 31.150000
==> Set learning rate: 0.001000
==> Training Epoch: 83
0.000000/1000.000000 ==> Training loss: 0.008850    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.027743    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.017814    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.009909    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.019734    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.010872    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.024696    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.008417    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.009725    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.013806    Training error rate: 0.000000
==> Total training loss: 16.723716    Total training error rate: 0.180000
==> Testing Epoch: 83
0.000000/100.000000 ==> Testing loss: 1.209488    Testing error rate: 27.000000
==> Total testing loss: 137.376376    Total testing error rate: 30.980000
==> Set learning rate: 0.001000
==> Training Epoch: 84
0.000000/1000.000000 ==> Training loss: 0.005926    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.012953    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.006549    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.013588    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.009071    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.015775    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.044376    Training error rate: 2.000000
700.000000/1000.000000 ==> Training loss: 0.009262    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.016597    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.012375    Training error rate: 0.000000
==> Total training loss: 16.339388    Total training error rate: 0.158000
==> Testing Epoch: 84
0.000000/100.000000 ==> Testing loss: 1.261348    Testing error rate: 27.000000
==> Total testing loss: 137.371372    Total testing error rate: 31.120000
==> Set learning rate: 0.001000
==> Training Epoch: 85
0.000000/1000.000000 ==> Training loss: 0.012827    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.007373    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.010874    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.008570    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.023576    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.019022    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.014617    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.023700    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.016220    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.038590    Training error rate: 2.000000
==> Total training loss: 15.758727    Total training error rate: 0.172000
==> Testing Epoch: 85
0.000000/100.000000 ==> Testing loss: 1.134812    Testing error rate: 26.000000
==> Total testing loss: 135.968068    Total testing error rate: 30.550000
==> Set learning rate: 0.001000
==> Training Epoch: 86
0.000000/1000.000000 ==> Training loss: 0.039497    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.074347    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.027032    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.008109    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.011247    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.036954    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.028117    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.022958    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.004124    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.013822    Training error rate: 0.000000
==> Total training loss: 16.098568    Total training error rate: 0.170000
==> Testing Epoch: 86
0.000000/100.000000 ==> Testing loss: 1.422047    Testing error rate: 28.000000
==> Total testing loss: 136.466506    Total testing error rate: 30.290000
==> Set learning rate: 0.001000
==> Training Epoch: 87
0.000000/1000.000000 ==> Training loss: 0.008718    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.012292    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.013804    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.007569    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.012003    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.008898    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.010715    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.031846    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.025197    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.015205    Training error rate: 0.000000
==> Total training loss: 15.367119    Total training error rate: 0.144000
==> Testing Epoch: 87
0.000000/100.000000 ==> Testing loss: 1.388350    Testing error rate: 27.000000
==> Total testing loss: 138.775586    Total testing error rate: 30.830000
==> Set learning rate: 0.001000
==> Training Epoch: 88
0.000000/1000.000000 ==> Training loss: 0.008322    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.010269    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.028475    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.007855    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.008484    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.007270    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.005308    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.010207    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.082475    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.009935    Training error rate: 0.000000
==> Total training loss: 15.283901    Total training error rate: 0.154000
==> Testing Epoch: 88
0.000000/100.000000 ==> Testing loss: 1.153708    Testing error rate: 28.000000
==> Total testing loss: 137.088875    Total testing error rate: 30.860000
==> Set learning rate: 0.001000
==> Training Epoch: 89
0.000000/1000.000000 ==> Training loss: 0.014786    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.018361    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.014046    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.012186    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.008134    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.018912    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.014167    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.022199    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.006261    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.007584    Training error rate: 0.000000
==> Total training loss: 15.449504    Total training error rate: 0.168000
==> Testing Epoch: 89
0.000000/100.000000 ==> Testing loss: 1.373492    Testing error rate: 28.000000
==> Total testing loss: 135.787054    Total testing error rate: 30.460000
==> Set learning rate: 0.001000
==> Training Epoch: 90
0.000000/1000.000000 ==> Training loss: 0.014396    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.014725    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.009660    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.013960    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.018984    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011445    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.017332    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.023079    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.014646    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.005666    Training error rate: 0.000000
==> Total training loss: 14.342711    Total training error rate: 0.156000
==> Testing Epoch: 90
0.000000/100.000000 ==> Testing loss: 1.232708    Testing error rate: 27.000000
==> Total testing loss: 139.338857    Total testing error rate: 30.740000
==> Set learning rate: 0.001000
==> Training Epoch: 91
0.000000/1000.000000 ==> Training loss: 0.013421    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.011167    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.005310    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.029217    Training error rate: 2.000000
400.000000/1000.000000 ==> Training loss: 0.005880    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.013814    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.074689    Training error rate: 4.000000
700.000000/1000.000000 ==> Training loss: 0.006169    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.045370    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.008154    Training error rate: 0.000000
==> Total training loss: 13.951833    Total training error rate: 0.146000
==> Testing Epoch: 91
0.000000/100.000000 ==> Testing loss: 1.225537    Testing error rate: 29.000000
==> Total testing loss: 138.113758    Total testing error rate: 30.580000
==> Set learning rate: 0.001000
==> Training Epoch: 92
0.000000/1000.000000 ==> Training loss: 0.028796    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.008776    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.012661    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.018972    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.022087    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.011675    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.006696    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.013435    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010932    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.007801    Training error rate: 0.000000
==> Total training loss: 14.454188    Total training error rate: 0.142000
==> Testing Epoch: 92
0.000000/100.000000 ==> Testing loss: 1.444203    Testing error rate: 35.000000
==> Total testing loss: 137.780375    Total testing error rate: 30.960000
==> Set learning rate: 0.001000
==> Training Epoch: 93
0.000000/1000.000000 ==> Training loss: 0.030707    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.010544    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.011671    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.013579    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.010221    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.015203    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013511    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.015990    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.069782    Training error rate: 2.000000
900.000000/1000.000000 ==> Training loss: 0.008192    Training error rate: 0.000000
==> Total training loss: 13.441439    Total training error rate: 0.112000
==> Testing Epoch: 93
0.000000/100.000000 ==> Testing loss: 1.557238    Testing error rate: 33.000000
==> Total testing loss: 138.143124    Total testing error rate: 30.670000
==> Set learning rate: 0.001000
==> Training Epoch: 94
0.000000/1000.000000 ==> Training loss: 0.009384    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.033785    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.017251    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.011490    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.013456    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.013088    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.011230    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.023053    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.010627    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.015307    Training error rate: 0.000000
==> Total training loss: 13.813973    Total training error rate: 0.150000
==> Testing Epoch: 94
0.000000/100.000000 ==> Testing loss: 1.429775    Testing error rate: 29.000000
==> Total testing loss: 138.418948    Total testing error rate: 30.870000
==> Set learning rate: 0.001000
==> Training Epoch: 95
0.000000/1000.000000 ==> Training loss: 0.011590    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.024444    Training error rate: 2.000000
200.000000/1000.000000 ==> Training loss: 0.046171    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.016571    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.004836    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.016448    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013732    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.006835    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.020556    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.025246    Training error rate: 0.000000
==> Total training loss: 13.527304    Total training error rate: 0.096000
==> Testing Epoch: 95
0.000000/100.000000 ==> Testing loss: 1.280640    Testing error rate: 28.000000
==> Total testing loss: 140.217373    Total testing error rate: 31.150000
==> Set learning rate: 0.001000
==> Training Epoch: 96
0.000000/1000.000000 ==> Training loss: 0.010778    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.016193    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.011597    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.025315    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.007205    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.010765    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.009234    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.006706    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.011356    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.011313    Training error rate: 0.000000
==> Total training loss: 13.509358    Total training error rate: 0.120000
==> Testing Epoch: 96
0.000000/100.000000 ==> Testing loss: 1.241278    Testing error rate: 24.000000
==> Total testing loss: 138.782477    Total testing error rate: 30.600000
==> Set learning rate: 0.001000
==> Training Epoch: 97
0.000000/1000.000000 ==> Training loss: 0.020005    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.007899    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.021931    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.012814    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.014397    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.003793    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013130    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.005030    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.017437    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.014614    Training error rate: 0.000000
==> Total training loss: 12.717156    Total training error rate: 0.094000
==> Testing Epoch: 97
0.000000/100.000000 ==> Testing loss: 1.266829    Testing error rate: 28.000000
==> Total testing loss: 139.586875    Total testing error rate: 31.270000
==> Set learning rate: 0.001000
==> Training Epoch: 98
0.000000/1000.000000 ==> Training loss: 0.009131    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.014125    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.014815    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.017496    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.006677    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.006801    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.024243    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.009263    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.016876    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.013729    Training error rate: 0.000000
==> Total training loss: 13.646961    Total training error rate: 0.148000
==> Testing Epoch: 98
0.000000/100.000000 ==> Testing loss: 1.305727    Testing error rate: 27.000000
==> Total testing loss: 140.019149    Total testing error rate: 30.740000
==> Set learning rate: 0.001000
==> Training Epoch: 99
0.000000/1000.000000 ==> Training loss: 0.022323    Training error rate: 2.000000
100.000000/1000.000000 ==> Training loss: 0.008773    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.007127    Training error rate: 0.000000
300.000000/1000.000000 ==> Training loss: 0.018205    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.016309    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.006928    Training error rate: 0.000000
600.000000/1000.000000 ==> Training loss: 0.013813    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.026985    Training error rate: 2.000000
800.000000/1000.000000 ==> Training loss: 0.006483    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.082300    Training error rate: 2.000000
==> Total training loss: 12.898377    Total training error rate: 0.106000
==> Testing Epoch: 99
0.000000/100.000000 ==> Testing loss: 1.551575    Testing error rate: 28.000000
==> Total testing loss: 139.483457    Total testing error rate: 30.900000
==> Set learning rate: 0.001000
==> Training Epoch: 100
0.000000/1000.000000 ==> Training loss: 0.006186    Training error rate: 0.000000
100.000000/1000.000000 ==> Training loss: 0.007350    Training error rate: 0.000000
200.000000/1000.000000 ==> Training loss: 0.048193    Training error rate: 2.000000
300.000000/1000.000000 ==> Training loss: 0.014897    Training error rate: 0.000000
400.000000/1000.000000 ==> Training loss: 0.015611    Training error rate: 0.000000
500.000000/1000.000000 ==> Training loss: 0.042900    Training error rate: 2.000000
600.000000/1000.000000 ==> Training loss: 0.009204    Training error rate: 0.000000
700.000000/1000.000000 ==> Training loss: 0.010618    Training error rate: 0.000000
800.000000/1000.000000 ==> Training loss: 0.017634    Training error rate: 0.000000
900.000000/1000.000000 ==> Training loss: 0.017284    Training error rate: 0.000000
==> Total training loss: 13.009120    Total training error rate: 0.116000
==> Testing Epoch: 100
0.000000/100.000000 ==> Testing loss: 1.270107    Testing error rate: 26.000000
==> Total testing loss: 134.847039    Total testing error rate: 30.350000
